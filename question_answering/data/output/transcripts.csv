course,topic,title,url,transcript
DSM100-2022-OCT,Module information,Module introduction video,https://learn.london.ac.uk/mod/page/view.php?id=96059&forceview=1,"-Welcome to AI Module. Artificial intelligence made enormous progress in recent years, entering all spheres of economy, technology, and our everyday life. AI is a driving force behind the growth of such top companies as Amazon, Facebook, Apple, Microsoft. AI is behind our daily activities, be it searching on Google, checking directions on Google Maps, talking to Siri, receiving targeted recommendations and advertisement, or going through security checks. AI has a more than seven decades history, with its ups and downs, but few doubt now that AI here is to stay. The term AI covers a wide range of techniques, approaches, methods. For some, AI is almost the same as machine learning, and for some, AI, it's almost the same as all of computing and robotics. In this module, we will be focusing on such foundational AI techniques as reasoning, decision making, knowledge representation, and robotics. Our program includes other AI-related modules, machine learning, natural language processing, neural networks. In this module, we intentionally will not be covering these topics. We will adopt an intelligent agent approach. We will follow in this to what is known by many as the best AI textbook, written by Peter Norvig and Stuart Russell. This module will include four theoretical topic sand six topics of more practical nature. We will focus on concrete AI systems and case studies. You will have an opportunity to learn how they're built and how they're working. After the end of this module, you should be able to critically evaluate key issues in agent-based system, knowledge system, robotics, automated reasoning, and problem-solving, represent tasks, environments, and outline strategies for intelligent agents, compare the adequacy and efficiency of different reasoning approaches. We form a deep researched analysis of a particular artificial intelligence method and their use, apply AI techniques within the context of a substantial research project. I wish you very best of luck with this module."
DSM100-2022-OCT,Module information,Meet the team,https://learn.london.ac.uk/mod/page/view.php?id=96060&forceview=1,"[music]-Welcome to the AI module. My name is Larisa Soldatova, and I am the module author, and here with me is Dr. Matthew Yee-King, who is co-author of this module. -Hello. -The module is divided in two parts. First part, first four topics are of more theoretical nature, and the last six topics are devoted to concrete case studies. I prepared one case study, and Matthew prepared two case studies. I will now ask Matthew to introduce himself and tell about his cases. -Okay. Thanks, Larisa. The two case studies I've worked on are the artificial creature sand the game playing AI, but before I talk about those, I should introduce myself and say that the reason I've chosen those two case studies is I've worked for many years as an AI scientist and a research engineer and various other thing son different research projects. I'm currently a member of faculty in the Goldsmiths Computing Department and also the course director for the online BSC computer science. To go back to the case studies, the first one, as I say, is evolving creatures, and what that involve sis essentially a deep dive where we recreate line by line in code, a very famous research paper from the mid-'90s, which demonstrated one of the first times really the power of automated design using artificial intelligence techniques. It's a great paper, and I think you'll really enjoy seeing that thing built up. I'm not going to give too much away, but you'll see some pretty interesting thing scrawling around on your screen when you work through that case study. Then, the second case study is another recreation of a famous research paper, this time from 2015. This paper was by Deep Mind, who you may well have heard of, and in this paper, what they did is they developed an AI agent which can play video games, and they demonstrated that it was able to play multiple different video gamesat the standard of human expert players, which was really a big hit at the time in the Nature magazine when it was published. Those are my two case studies, so I'm going to hand them back to Larisa now. -Thank you. In my case study, we will talk about the robot scientist system, how it generates new knowledge, how it can come up with new ideas, new hypotheses, and test them. We will look inside the system with components it has and how they interact. As a whole, this module covers a wide range of AI techniques, and we are considering them on these concrete case studies. I hope that by the end of this module, you will be able to master these technique sand apply them for your tasks. I wish you very best of luck with studying this module. -Good luck."
DSM100-2022-OCT,Topic 1: Introduction,Lecture: Introduction to Topic 1,https://learn.london.ac.uk/mod/page/view.php?id=96070&forceview=1,"Welcome to topic one, Introduction to AI. In this topic, we will attempt to define AI from different perspectives. We will look at history of AI. Its majors periods, milestones. We will consider early AI system sand we also will talk about people who developed them. We will talk about what AI can do now. What is the state of the art in AI development?We will spend some time discussing rational agents. What types of rational agents are, and how to construct them. We will conclude this topic by discussing not only benefits but also risks associated with AI technologies. We will discuss several important ethical issue sand what you as specialist in data science can do about it. Good luck with this topic."
DSM100-2022-OCT,Topic 1: Introduction,Lecture 1: Defining AI,https://learn.london.ac.uk/mod/page/view.php?id=96074&forceview=1,"[music]In this mini lecture, we will talk about what AI isand what makes an intelligent agent intelligent. We will discuss different approaches to defining AI. For example, we will talk about Turing test approach and if an AI system can pass the Turing test. To start with, it's important to stress that there is no an agreed definition of AI. One of the reasons of that is that there is no an agreed definition of what intelligence is. However, we all generally agree what intelligent behavior is. For example, playing chess, GO, driving a car, learning new concepts, making decisions, such behavior requires intelligence. There are two major types of AI. Narrow AI, AI applied to a specific domain like language translators, virtual assistants, self-driving cars, chat bots, recommendation engines, intelligent spam filters. Examples are Watson, Alexa, Siri, Cor tana, Google Assistant. Another type is general AI, AI that can perform a wide variety of unrelated tasks. It can learn to solve tasks for new problems. This level is not achieved yet. We will consider what AI is from four perspectives. Acting humanly- The Turing test approach. Thinking humanly- The cognitive modeling approach. Thinking rationally- The ""law of thought"" approach. Acting rationally- The rational agent approach. Turing test was proposed by Alan Turing in 1950, and it remains relevant 70 years later. It provides operational definition of intelligence. A computer passes the test if a human interrogator, after posing written questions, can't tell whether the answers came from a person or a computer. Can AI pass a Turing test?To pass a Turing test, AI needs the following capabilities, natural language processing to communicate with the interrogator, knowledge representation to store and process what it knows, automated reasoning to use as thought knowledge to answer question sand to draw conclusions, machine learning to detect and extrapolate patterns. The classical Turing test does not include physical context. The so-called total Turing test includes a video signal source that interrogator can test perceptual abilities, and also an interrogator can pass physical objects through the hatch. To pass the total Turing test, an AI system additionally to the listed about capabilities needs computer vision to perceive the objects and robotics to manipulate objects and to move about. These six disciplines are the core disciplines of AI. AI researchers do not focus on passing the Turing test, instead, they're focusing on underlying technologies. In this module, we will particularly focus on knowledge representation, automated reasoning, and robotics. There are other modules in our programs that are focusing on other core disciplines like machine learning, natural language processing, and data visualization. The cognitive modeling approach to AI is focusing on understanding how humans think and attempting to build AI that thinks like a human. The interdisciplinary field of cognitive science brings together computer models from AI and experimental techniques from psychology. There are interesting developments in this exciting area of research. For example, check open access research papers by my colleague from Goldsmiths, Max Garagnani. I provided some links for you. A subfield of cognitive science is human-like computing. Human-like computing research aims to endow machines with human-like perceptual reasoning and learning abilities, which support collaboration and communication with human beings. This approach is in contrast to more conventional approaches where AI systems rely on their ability to learn from huge numbers of examples. Like Tesla Autopilot program is built on 780 million months of travel data. Alpha Go probably played hundreds of millions of games prior to beating Go professional, Lee Se dol. Humans, on the other hand, can extrapolate from a small number of examples. Human-like computing is a new area of research and it is focusing on the development of AI systems that could produce explainable human-like reasoning. Such systems would be beneficial in many application areas. Thinking rationally approach. This approach dates back to Aristotle, the Greek philosopher who attempted to codify the laws of reasoning. His laws provided patterns for argumentative structures that always yield correct conclusions. For example, Socrates is a man. All man are mortal. Therefore, Socrates is mortal. Such laws are the foundation of the research into logic. An intelligent system needs the capability of ""thinking"" or logical reasoning. Acting rationally approach. A rational agent is an agent, be it a computer program, a robot, or a human, anyone or any sense that can act that acts to achieve the best outcome. All the capabilities required for the Turing testare also required for an agent to act rationally. Knowledge representation and reasoning enables the agent to accumulate knowledge and make decision based on what is known. NLP enables communication, and learning is critical for improving performance. In this module, we will be mainly focusing on the rational agent approach to AI. Because it is generic, we can apply it to robots and computer programs, to simple and complex systems, and also multi-agent systems. It is amenable to scientific developments. AI systems can be developed as rational agents, and their performance can be measured. It is practical. Agents are acting in concrete environments, like in car manufacturing, medical diagnosing, or personal intelligent assistants. In this mini lecture, we considered different approaches to defining what AI research is, approaches that are focusing on development of AI systems that act and thinks as humans, and also approaches that are focusing on development of AI system sand act and think as rational agents. All these approaches are important and extremely interesting. However, in this module, we will be mainly focusing on the rational agent approach to AI. We will consider AI systems as rational agent sand how to build them and how to work with them."
DSM100-2022-OCT,Topic 1: Introduction,Lecture 2:  Milestones in the history of AI,https://learn.london.ac.uk/mod/page/view.php?id=96080&forceview=1,"In this mini-lecture with a go through the history of AI. It has its ups and downs, over-optimistic expectations, and AI winter. By now, say AI research trends are from reset and AI industry is essential for the world's economy. The beginning of AI dates back more than seven decades. The first book that is now considered as AI was done in 1943, by Warren McCulloch and Walter Pitts. They propose a model of artificial neuron sin which each neuron can be switched on or off. The on/off switch depends on the simulation of sufficient number of neighboring neurons. They showed that any computable function can be computed by some network of connected neurons, including also logical connectors like and or not. McCulloch and Pitt's also suggested that suitably defined networks could learn. Two undergraduate students from Harvard, Marvin Minsky, and Dean Edmondsbuild the first neural network computer in 1950s, the [?] It was built from 3, 000 vacuum tube sand pilot mechanism from a B24 bomber to simulate a network of 40 neurons. The most influential work in AI was done by Alan Turing. In particular in his lecture in 1947at the London Mathematical Society and then his article in 1950, Computing Machinery and Intelligence, he introduced the Turing test, machine learning, genetic algorithms, and reinforcement learning. The term AI was coined in 1956. McCarthy convinced Minsky, Claude Shannon, and Nathaniel Rochester to bring together US researchers interested in automata theory, neuron maths, and the study of intelligence for a two months workshop in Dartmouth College. This is considered to be the official birthplace of the field of AI. The agenda of the workshop is two months, 10 man study of artificial intelligence. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described thata machine can be made to simulate it. AI became a separate field of research because none of the existing fields attempted to duplicate human creativity, self-improvement, and language use. AI combined computer science and also building machines. The era of great expectations and 15 and 16was referred to McCarthy as a period, ""Look Ma, no hands. ""At those times, computers were viewed as just a little bit more than arithmetic devices. Any book showing that they can do more and be more intelligent was astonishing. The examples are Logic Theorist by Newell and Simon presented at the Dartmouth workshopis a reasoning program capable of proving many theorems. Then they developed general problem solver. It was designed to imitate human problem-solving protocols. It could solve puzzles. Arthur Samuel developed programs for checkers that learned to play at a strong armature level. McCarthy developed AI programming language Lisp. It was the dominant programming language, particularly in US for 30 years. The AI programming language Prologue became more popular in Europe. In 1958, McCarthy describes the program Advice Taker. It is considered to be first complete AI system. The program was designed to use knowledge to search for solutions to problems. It included not only domain-specific knowledge but also general knowledge of the world. For example, axioms could be used to generate a plan to drive to the airport. The program could accept new axioms, allowing it to achieve competence in new areas without being reprogrammed. Thus Advice Taker embodied the principles of knowledge representation and reasoning. That it is useful to have a formal explicit representation of support and its working sand to be able to reason over it. Cordell Green developed question answering and planning systems based on the general purpose methos for logical reasoning. Shakey robotic project at Stanford Research Institute was the first to demonstrate complete integration of logical reasoning and physical activity. There are researchers who will not shyin making predictions about the success of AI. When your search predictions were over optimistic without acknowledging many of the difficulties in developing AI systems. Early AI systems didn't include knowledge models. A typical example is machine translation. US National Research Council funded machine translation of scientific papers from Russian to English after the Sputnik launch in 1957. Initially, it was thought that basic grammar rule sand a dictionary would be enough to capture the meaning of the text. The famous example is the translation of the spirit is willing but the flesh is weakas the vodka is good but the meat is rotten. Natural languages are ambiguous and translation requires background knowledge. Soon after that, all US government funding of translation projects was canceled. Another difficulty is computational complexity and combinatorial explosion. Early AI systems were not easily scalable. Computational power also was limited. For example, genetic algorithms could not be implemented by simple random mutations of the code. More than genetic algorithms use better representations. That was the main criticism of AI in the [?] reported in 1973after which the UK Government stopped funding AI research. The period of expert systems or knowledge-based system is the end of '60sup to '90s. Problem-solving approach during the early years of AI was to include general purpose reasoning to find a complete solution. Such an approach is considered as a weak method because also generally doesn't scale up to large or difficult problems. The alternative approach is to use more powerful domain-specific knowledge. Examples of such systems are Dendral by Buchanan et al. The problem was to infer a chemical structure from the information provided by a mass spectrometer. The naïve version of the program generated all possible structures consistent with the input. However, this is intractable even for a moderate-sized molecule. The Dendral researchers consulted with chemist sand found that they worked with well-known patterns corresponding to typical sub-structures like [?] subgroups. This reduces the search space enormously. Dendral was the first successful knowledge-intensive system. Its expertise derived from a large number of special-purpose rules. Mycin by Feigenbaum was designed to diagnose blood infections. It contained about 450 rule sand performed as well as some expert sand considerably better than junior doctors. AI became an industry in 1980s. R1 is the first commercial expert system that began operation in the Digita Equipment Corporation in 1982. The program helped configure orders for new computer systems. By 1986, it was saving the company an estimated $40 million a year. By 1988, DuPont had 100 expert systems in use, saving an estimated $10 million a year. Overall, the AI industry boomed from a few million dollars in 1980 to billions in 1988. Hundreds of companies were building expert systems, vision systems, and robots. Soon after that, came a period of the AI winterin which many failed to deliver the extravagant promises. Nowadays, AI industry is flourishing. According to McKinsey Global Institute's research, AI could deliver an additional output of $13 trillion to support the economy by 2030. The brittleness of expert system sled to a new more scientific approach incorporating probability rather than Boollean logic and machine learning rather than hand-coding. In 1980s, Perl's development of Bayesian networks for a rigorous and efficient formalism for representing uncertain knowledge, and practical algorithms for probabilistic reasoning. As achievement was shared benchmark problem sets. UC Irvine repository for machine learning datasets. The International Planning Competition. The LibriSpeech corpus for speech recognition. The MNIST data set for handwritten digital recognition, and so on. The next stage was the beginning of the Big data era. It is characterized by remarkable advances in computing power, creation of World Wide Web, and creation of very large datasets. Trillions of words of text, billions of images, billions of hours of speech and video, and genomic data, vehicle tracking data, social network data, and so on. This led to development of algorithms to work with big un-labeled data sets. The availability of big data and shift towards machine learning helped AI to recover commercial attractiveness. The next phase is deep learning. Deep Learning refers to neural networks with multiple layers. Experiments with such networks were carried out as far back as 1970s. Conventional neural networks found some successin handwritten digit recognition in 1990s. It was not until 2011however, that the DL message really took off. This first decoded speech recognition and the visual object recognition in 2012 competition DL system by Geoffrey Hinton from University of Toronto demonstrated a dramatic improvement over other systems. Since then, DL systems exceeded human performance on some vision tasks. Similar advances were made in speech recognition, machine translation, medical diagnosis, and game playing. The use of DL contributed to winning Alpha Go over leading human GO players. In this mini-lecture, we talked about the history of AI research, its milestones, examples systems, and also about researchers working in AI."
DSM100-2022-OCT,Topic 1: Introduction,Lecture 3: Rational agents,https://learn.london.ac.uk/mod/page/view.php?id=96085&forceview=1,"-In this mini lecture, we will talk about artificial agent sand what makes an agent rational. An agent is anything that perceives its environment through its sensor sand acts upon that environment through actuators. In this sense, we can view a human as an agent. A human has sensors like eyes, ear sand actuators like arms, legs, voice cords to act in their surrounded environment. A robotic agent may have cameras, voice recognition, there is a lot of work on smile recognition and electronic tongue or haptic inputs, outputs. Robots can go beyond human perceptions. For example, they can have infrared camera sand they can see what humans can't see. An agent can be a software agent or a bot. It can receive keystrokes or a set of data, files, a sensory input and act on the environment by, for example, displaying on the screen, writing to files or it can send messages to other agents, software, or robotic agents. In this case, we are talking about multi-agent communication. Let us consider a simple example, a vacuum cleaner agent world. If I operate a vacuum cleaner, then the vacuum cleaner is not an agent, the human is. If a vacuum cleaner can act on its own, then we can consider it as an agent. In this example, the world is very simple and restricted. There are only two locations, A and Band there can be dirt or no dirt. The environment is locations A and B. Sensors, the cleaner agent perceives which square it is in and if there is dirt or not. Actions, move left, move right, suck up the dirt or do nothing. Other important concepts are percept sequence is the complete history what agent ever perceived. Agent's behavior is a mapping of any percept sequence to an action. An agent can make a decision about the next action based on the current environment stateor based on any part or a whole history of percept sequences. In theory, we can construct a table that maps all possible percept sequences to some action. In practice, it is rarely possible because the world is not complicated than the vacuum cleaner world with only two locations. It is virtually impossible to list all percept sequences. All searching through them would take too long to decide what to do in what situation. What is a rational agent?One that behaves rationally. How to evaluate that?We need to define a performance measure. A criterion of success. Then, there can be many performance measures. For example, amount of dirt sucked in eight hours, then an agent can maximize it by sucking the dirt up, then disposing it and sucking it up again. Or a performance measure can be to minimize electricity consumption while maximizing use. Number of squares are cleaned at each time point. It can be a complex measure. We can also add minimize noise. Rationality also implies knowledge of the environment. In our example, the cleaner agent knows about the locations. Otherwise, it is hard to expect rational behavior. For each possible percept sequence, a rational agent should select an action that maximizes its performance measure. In many situations, an agent will have only partial knowledge of the environ mentor none at all. Then, information gathering through exploration is critical. Another important aspect of rational behavior is learningas much as possible from what it perceives. It can be a complex process. For example, learning from sensor data often involves machine learning. A rational agent should be autonomous. A vacuum cleaner should be able to decide what to do and when without control from humans. Autonomy and independence may not be required from the start. An agent first may need to learn about the environment actions and the effect sand only then, act autonomously. In this mini lecture, we talked about rational agents, rational behavior and how to measure it. We considered a very simple example. In reality, of course, agents and their environments are far more complex but they still can act rationally and we can measure its performance."
DSM100-2022-OCT,Topic 1: Introduction,Lecture 4: Properties of task environments,https://learn.london.ac.uk/mod/page/view.php?id=96087&forceview=1,"[music]In the previous mini-lecture, we talked about rational agent sand their environment. In this mini-lecture, we will discuss further how to specify the task environment for an intelligent agent. To specify the task environment for an agent, one has to describe its environment, sensors, actuators, and the performance measure. All together it is called the PEAS description. The table in the slide shows several examples of such description. Task environments can have many different properties. We will briefly consider some of the most important ones. We will come back to some of these properties later in this module. The environment is fully observable if agent's sensors give access to the complete state of the environmentat each point of time. The environment is partially observable if agent's sensors give access to a partial state of the environment, for example, due to noise or inaccurate sensors. Or some of the environment can be observed by the agent's sensors. This concept is connected to the performance measure. For example, for the English tutor agent, students' scores may not be observable by the agent, because they may need to be first approved by a human teacher, and it may take too long. However, students' answers are observable. If the performance measure includes scores, then the environment is partially observable. If it includes only answers, then it is fully observable. Unobservable, this sounds likean extremely difficult situation, but it can be solvable. It all depends on the exploration and learning stages. Single agent versus multi-agent environment. For example, playing chessis a two competitive agents environment. Multi-agent environment also can be cooperative. For example, robots working on the same task. Agents can share tasks or have their own tasks. They can share prior knowledge or have their own. Sometimes contradictory to others. Communication and interaction between agents is a hot area of research. I listed a couple of recommended articles about multi-agent systems. Please have a look if you're interested in this topic. A newly emerging area of researchis communication and interaction between artificial agents and humans. There is a dedicated journal of human-robot interaction. Interestingly, for example, in chess, teams of human sand software agents are superior to teams of only computer agents or to teams of humans. Perhaps, it is true for other areas of activities, and in the future, we will see more of human-computer robotics cooperation and co-creation. An environment is deterministic if the next state of the environmentis completely determined by the current state and the agent's actions. If not, then it is stochastic environment. Some environments are considered stochastic for practicality. For example, if it is a complex, partially observable environment. A related concept is uncertain environment. It is a nondeterministic environment where actions are characterized by possible outcomes, or if the environment is not fully observable. An environment is static if the state of the environment doesn't change while the agent is deciding what to do. If not, then it is a dynamic environment, continuously asking an agent what it wants to do. For example, a self-driving car. If an agent doesn't do anything, then it is also a decision, and the environment will keep changing. Semi dynamic environment. The environment doesn't change with time, but it is changed by agent's performance score. Discrete versus continuous environment. The state of the environment can be discrete or continuous. The passage of time can be discrete or continuous. Perception actions also can be discrete or continuous. Our example, the vacuum cleaner worldis a very simple world. It is fully observable, single agent, deterministic, static, and discrete. It can be modified to a more complex agent. An environment can be partially observable if it is unknown what is in different locations. It can be multi-agent with many cleaners. Dynamic, if dirt appears randomly. Continuous, if dirt appears continuously. As you can see, artificial agents can be simple or complex. They are characterized by their sensor sand actuators. Their behavior depends on the environment and also on performance measures."
DSM100-2022-OCT,Topic 1: Introduction,Lecture 5: Reflex agents,https://learn.london.ac.uk/mod/page/view.php?id=96090&forceview=1,"[music]In the previous mini lectures, we talked about behavior of rational agents and their environments. In this mini lecture, we will look inside an intelligent agent. If before, we talked about how an agent can be specified by a mapping of possible percept sequences to actions, today, we will discuss how such a mapping can be implemented as an agent program, a program that takes current percept as an input and outputs and action. The agent program can store also all the necessary knowledge about the environment, available actions, and percept history. There are different kinds of agent programs, dependent on how it generates actions, simple reflex agents, model-based reflex agents, utility agents, goal-based agents, and learning agents. We will now briefly discuss these kinds of agent programs. Simple reflex agents are called simple because they are taken as an input only the current percept, and they do not store the percept history. For example, the vacuum agent is a simple reflex agent if it makes decisions only based on the current location and presence or absence of dirt. The program for the vacuum agent might look like the one show non this slide written in a pseudo-code, function reflex vacuum agent with parameters, location, status, returns, and action. If status is dirty, then it returns action suck. If location is A, then it returns action right. If location is B, then it returns action left. The action program contains condition action rules. For example, if car in front is breaking, then initiate break. You can think about many more examples of such rules. Humans also are using such rules. They may be given, for example, for the current pandemic situation, if entering inside, then put a face mask on, or they can be learned from experience. If calling your friend and then ask how her kid is doing, such rules are also called situation action rules, productions, or simply if-then rules. Obviously, simple reflex agents have limitations. They can work only if a decision can be made based on the current state or the percept and environment. They're not suitable for partially observable or unobservable environments. For example, if in the vacuum agent world, there are many occasions and some are not observable, then the vacuum agent will never clean the whole space. It is not able to decide to move or suck if it is in unknown location. See the rules, it needs a location to initiate the move. Other types of agents are designed to overcome such limitations."
DSM100-2022-OCT,Topic 1: Introduction,"Lecture 6: Goal-based, model-based, utility-based, learning agents",https://learn.london.ac.uk/mod/page/view.php?id=96092&forceview=1,"[MUSIC]In the previous mini-lecture, we talked about simple reflex agents and their limitations. In this mini-lecture, we will consider other types of agents, model-based, goal-based, utility-based, and learning agents. Model-based reflex agents store knowledge about their environment called the knowledge model or a domain modelor just a model. The more an agent knows about the world it is in, the more intelligent behavior it can exhibit. For example, if the vacuum cleaner has a model of the floor where obstacles are and also where dirt usually is, then it can make better decisions. A model-based vacuum cleaner can operate by different rules. If it does know where dirt is, then it can find a path to get there and to avoid obstacles. This slide shows a simplified program in the pseudocodefor a model-based agent. The model may include a path between each pair of locations. There are algorithms for finding such a path. Having a model of the world may not be enough to decide what to do. For example, for a self-driving car agent, it is not enough to have a map of road sand rules of movements. It still needs to decide if to turn left or right, and it is hard to decide without knowing the destination. In other words, the car agent needs to know a goal. Agents that make decisions based on a given goal are called goal-based agents. Knowing goals and models makes agents even more intelligent. Sometimes, goals can be achieved by a couple of actions, but sometimes, it may require complicated and not obvious sequences of actions. Such agents need to search through the space of possible action sand also to plan their actions. Search and planning are subfields of AI devoted to finding solutions to achieve agents' goals. Utility-based agents. Knowing a goal is good, but it still may not be enough to generate high-quality behavior. In complex environments, the same goal can be achieved by different ways. For example, a self-driving car agent can reach a destination by following the best route, fastest route, or the most ecology-friendly one. Previously, we discussed the performance measures for rational agents. Such a measure can be implemented in the agent programas a utility function. A rational utility-based agent chooses actions that maximize the expected utility of the action outcomes. Learning agents. Modeling the world and writing production or situation action rule sis a time-consuming task. Can an agent learn it?The answer is yes. There are different architectures for learning. We will consider an architecture that includes an Oracle or a critic, someone or something outside the agent that can provide a feedback on agent's performance. A learning agent has to explore possible goals, states, actions, and their outcomes. It is usually done using a problem generator. Let us continue with the vacuum cleaner agent example. Imagine a cleaner that knows nothing about the complex world around. The world is a floor that consists of many square sand some have dirt and also some large objects, obstacles. The agent has learning capabilities, having an Oracle, a problem generator, and performance measures. It also knows what it can do, move left, right, straight, back, and suck in dirt. It has sensors to detect in what location it is inand if there is dirt in that location. For the exploration stage, the problem generator will randomly generate 100 three-action sequences or some other sequences. The goal will be to execute them and evaluate the agent's performance. The performance measure remains the same, clean the floor, but for the learning stage, it can be fragmented. The Oracle can assign three points for each clean square, one point for a move to adjust to dirt square, and it can penalize the agent if it was to move back after hitting an obstacle. The cleaner may start from selecting a three-action sequence randomly, executing it, and learning from the experience. It will learn sequences leading to a reward. For example, move from location A1 straight, then again straight, then suck in. Also, it will learn sequences leading to a penalty. For example, move from location A1 right, then straight, but it can't, so it has to go back. After executing 100 three-action sequences, the cleaner agent is likely to learn the whole map of the floor, where the dirt is and where the obstacles are. It will construct the model of the world or maybe it will construct the model only partially and then more and maybe longer sequences of actions will be generated. The agent will learn more and more with executing new sequences of actions. It will learn about the world and also what actions lead to a reward. Its behavior will become more and more rational and intelligent."
DSM100-2022-OCT,Topic 1: Introduction,Lecture 7: State-of-the-art in AI,https://learn.london.ac.uk/mod/page/view.php?id=96097&forceview=1,"[music]-In the previous mini-lecture, we went through the history of AI and wealso discussed where AI is now in the era of big data deployment, robotic sand substantial industry uptake. In this mini-lecture, we will talk about the state-of-the-artin AI, and we will consider some facts about AI, and also examples. AI100 is the Stanford University 100-year study on AI report. It concludes substantial increases in the future uses of AI applications, including more self-driving cost, healthcare diagnostics and targeted treatment, and physical assistance for eldercare can be expected. The year 2021 report reports that drugs, cancer, molecular drug discovery received the greatest amount of private AI investment in 2020, with more than $13. 8 billion, 4. 5 times higher than 2019. AI systems can now compose text, audio and images to a sufficiently high standard that humans have a hard time telling the difference between synthetic and non-synthetic outputs for some constrained applications of the technology. Rapid progress in natural language processing has yielded AI systems with significantly improved language capabilities that have started to have a meaningful economic impact on the world. Google and Microsoft have both deployed the BERT language model into their search engines, while other large language model shave been developed by companies ranging from Microsoft to OpenAI. Machine learning is changing the game in healthcare and biology. Deep Mind's Alpha Fold applied deep learning techniques tomake a significant breakthrough in the decades-long biology challenge of protein folding. The total global investment in AI, including private investment and public offerings, increased by 40% in 2020 relative to 2019, for a total of nearly $70 billion. Despite the pandemic, 2020 saw a 9. 3% increase in the amount of private AI investment from 2019. Brazil, India, Canada, Singapore, and South Africa are the countries with the highest growth in AI hiring from 2016 to 2020. This is consistent with the result of McKinsey Global Surveys. Despite the economic downturn caused by the pandemic, half the respondent sin the McKinsey survey said that the corona virus had no effect on their investment in AI, while 27% actually reported increasing their investment. Across the 14 countries analyzed, the AI hiring rate in 2020 was 2. 2 times higher on average than that in 2016. Demand for AI labor in six countries covered by Burning Glass data, the United States, the United Kingdom, Canada, Australia, New Zealand, and Singapore has grown significantly in the last seven years. On average, the share of AI job postings among all job postings in 2020 is more than five times larger than in 2013. Overall, half of respondents say their organizations have adoptedAI in at least one function. The business functions in which organizations adopt AI remain largely unchanged from the 2019 survey with service operations, product or service development, and marketing and sales, again, taking the top spot. What can AI do today?In robotics, the first demonstration of autonomous road driving without guides occurred in 1980s. DARPA Grand Challenge in 2005 successfully demonstrated autonomous driving on dirt roads for 132 miles. Urban Challenge in 2007 successfully demonstrated autonomous driving on streets with traffic. In 2018, Way mo test vehicles passed a landmark of 10 million miles driven on public roads without a serious accident with the human traffic stepping in to take control only every 6, 000 miles. Autonomous drones have been providing cross-country blood deliveries in Rwanda since 2016. Autonomous planning and schedule examples. In 1990s, during the Persian Gulf Crisis, US forces deployed DART, Dynamic Analysis and Planning Tool for automated logistics planning and scheduling for transportation. That involved over 50. 000 vehicles, cargo and people. That had to account for transport capacities, port and airfield capacities, and conflict resolution and more parameters. In 2000, NASA Remote Agent was the first to use autonomous planning program to control the scheduling of operations for a spacecraft. It generated plans from a high-level goal specified from the ground monitor's execution of those plans, detecting, diagnosing and recovering from the problems as they occurred. In 2012, EUROPA planning toolkit is used for daily operations by NASA's Mars rover. In 2017, SEXTANT system allows autonomous navigation in deep space, beyond the global GPS system. Every day, ride-hailing companies like Uber and mapping services as Google Maps provide driving instructions for hundreds of millions of users, quickly plotting an optimal route, taking into account current and predicted future traffic conditions. Online machine translation systems are now reading documents in over100 languages, including the native languages of 99% population for hundreds of millions of users. Speech recognition systems reached a word error rate of 5. 10%, matching human performance. About third of computer interactions are now done by voice rather than keyboard. Skype provides real-time speech-to-speech translation in 10 languages. Alexa, Siri, Cor tana offer assistants that can answer questions and carry out task for the user. In this mini-lecture, we talked about the state-of-the-art in AI. AI became a part of our everyday lives, be it using AI assistants like Alexa and Siri, using goods produced with the help of a program, or following driving instructions. Investments to AI are huge and steadily increasing and the demand for AI specialists will remain high."
DSM100-2022-OCT,Topic 1: Introduction,Lecture 8: Ethical and legal issues,https://learn.london.ac.uk/mod/page/view.php?id=96103&forceview=1,"[music]AI is a dual-use technology. It can be used for peaceful applications such as flight control, visual tracking and navigation, multi-agent planning, but it also can be easily applied to military purposes. The benefits of AI are great, but so are the risks. AI and robotics have the potential to free us from repetitive work, and dramatically increase production of goods and services. As Demis Hassabis, CEO of Google Deep Mind has suggested, ""First solve AI, then use AI to solve everything else. ""Unfortunately, AI and robotics can be used as lethal autonomous weapons for civilians, for making unfair decisions, and taking our jobs. There are numerous ethical issues associated with AI technologies. For example, how do we protect ourselves against unintended consequences?How does AI affect our behavior and interaction?How to fairly distribute the wealth created by AI?Can AI make mistakes?Can we stay in control of a complex AI system?How should we treat AI, and many other concerns. You, as a specialist in data science, have to be aware of the potential risks of AI technologies and associated ethical issues, and do whatever possible to mitigate those risks and resolve issues. In this mini-lecture, we will discuss some of issues associated with AI technologies. Autonomous weapons have been called the third revolution warfare after gunpower and nuclear weapons. Their military potential is obvious. Few experts doubt that autonomous fighter aircraft would defeat any human pilot. Autonomous aircraft, tanks, submarines, can be cheaper, faster, more maneuverable, and have longer range. Examples include Israel's HAROP Missile with a 10-foot wingspan and a 50-pound warhead. It searches for up to six hours in a given geographical region for any targets that meets given criteria and destroys it. Since 2014, the United Nations under the auspices of the conventionon Certain Conventional Weapons are discussing if to ban lethal autonomous weapons. The ethical side of this discussions is that many find it morally unacceptable to delegate the decision to kill humans to a machine. Antonio Guterres, the Head of United Nations, stated in 2019, ""Machines with the power and discretion to take lives without human involvement are politically unacceptable, morally repugnant, and should be prohibited by international law. ""Personally, I think it is a blueprint for solving ethical issues. Even if AI technologies have the capability for causing harm, the legislation must prevent such possibilities. Unfortunately, when it concerns AI, the legislation often lags behind. It is obvious that more efforts in this area are required. As of 2018, there was 350 million surveillance cameras in China and 70 million in US. China and other countries are exporting the surveillance equipment to other countries. Some has reputation for mistreating their citizen. More and more data about our daily activities are being collectedby governments and corporations. They want to use such data to prevent crime, to stop terrorists, to have better services, and we want to cure diseases, but we don't want to compromise our privacy, our human rights. We don't want to live in a police state. This quote from Virginia Eu banks illustrates how much data is collected from us in our everyday lives. ""Digital security guards collect information about us, make inferences about our behavior and control access to resources. Some are obvious and visible: closed-circuit cameras bristle on our street corners, our cell phones' global positioning devices record our movements, police drones fly over political protests, but many of devices that collect our information and monitor our actions are inscrutable, invisible pieces of code. They are embedded in social media interactions, flow through applications for government services, envelop every product we buy or try. They are so deeply woven into the fabric of social lives that, most of the time, we don't even notice we are being watched and analyzed. We all inhabit this new regime of digital data. ""The legislation must enforce the right way of collecting and using data. Data collectors have a moral and legal responsibility to be good stewards of the data they hold. For example, in Europe, GD PR, General Data Protection Regulation, mandates that companies design their systems with protection of data in mind, and requires that they obtain consent for any collection or processing of data. One of the primary concern arising from the advance of AI is that human labor can become obsolete. Oxford Economics predicted that 20 million manufacturing jobs could be lost due to automation by 2030. In 2017, Frey and Osborne surveyed 702 different occupation sand estimated that 47% of them are at risk of being automated. For example, 3% of all the workforce in US are vehicle drivers. The task of driving is likely to be eliminated by driver less cars, taxis, and trucks. McKinsey estimates that only 5% of occupations are fully automat able, but that 60% of occupations can have about 30% of their tasks automated. The fact is, AI mainly taking over tasks, not jobs. Historically, there were several periods of technological unemployment. For example, in the beginning of the 19th century, weavers were replaced by automated looms. However, the employment levels eventually recovered. There is a compensation effect that: Greater productivity leads to increase in overall wealth. It leads to greater demand for goods and services. It leads to increase in employment. An example, ATMs replaced humans in the job of counting out cash for withdrawals, that made it cheaper to operate the bank branch. The number of branches increased, more bank employees overall. The effect of AI may be similar. For example, one of the common jobs of the future can be labeling datasets. The most realistic scenario is that humans labor will not be obsolete, but the tasks humans are working on will change. In 2019, IBM predicted that 120 million workers would need retraining due to automation by 2022. Another important issue is that AI systems are often unable to explain their decisions and recommendations. Explainable and interpretable AI is a hot topic. Explain ability and interpret ability matters because it is giving users confidence in the system. People seek explanations for a variety of purposes. To support learning, to manage social interactions, to persuade and to assign responsibility. Safeguarding against bias: In order to check or confirms that an AI system is not using data in a way that results in bias or discriminatory outcomes. Assessing risks, robustness, and vulnerability. This can be particularly important if a system is deployed in a new environment, where the user cannot be sure of its effectiveness. Interpret ability can help developers understand how a system might be vulnerable to so-called adversarial attacks. The next issue we will consider is AI safety. It would be unethical to build and/ or distribute an unsafe AI agent. AI system has to be safe, should be able to avoid accidents, be resistant to adversarial attacks and malicious abuse, and in general, to cause benefits, not harm. Unfortunately, at present, there is no means to guarantee that. Almost things technology has the potential to cause harmin the wrong hands. The concern about AI and robotics in the hands may be operating on their own. Many are raising serious concerns that AI systems can posea threat to humanity because of their potential to self-improve at a rapid rate. It is up to us to develop AI systems that benefit humanity, not harm it. There is a long history of safety engineering in traditional engineering fields, and we should employ their methods for making AI safe. For example, in the failure models and effects analysis, analysts can see that each component of the system and imagine every possible way the component could go wrong. The field of software engineering is mainly focusing on the correctness of the programs, not their safety. This is shifted now towards safety. For example, what if a tire of a self-driving car is punctuate dat the highest speed?A safe system would be tested for this, and would have software to correct for the resulting loss of control. Engineering and Physical Sciences Research Committee UK and many other government organizations set principles of AI and robotics. The most common principles are insurance safety, ensure fairness, respect privacy, promote collaboration, establish accountability, uphold human right sand values, respect diversity and inclusion, avoid concentration of power. In this mini-lecture, we discuss only some of ethical issues associated with AI technologies. AI has a great potential to improve the quality of our lives, but there are also serious risks. Unfortunately, there is not enough recognition of the pressing need to address ethical issues. According to a McKinsey survey, despite growing calls to address ethical concerns associated with using AI, efforts to address these concerns in the industry are limited. For example, issues such as equity and fairness in AI continue to receive comparatively little attention from companies. Moreover, fewer companies in 2020 view personal or individual privacy risks as relevant compared with in 2019. That puts even more emphasis on having in place regulations, guidance, and laws to govern the development, in a moral and ethical manner. exploitation of AI technologies"
DSM100-2022-OCT,Topic 1: Introduction,Lecture 9: Biases in data,https://learn.london.ac.uk/mod/page/view.php?id=96105&forceview=1,"[music]Data scientists are working with data The results of their work largely depends on the quality of data The well-known principle, garbage in garbage out, tells that the quality of output depends on the quality of input There are many issues associated with data quality One of them is bias in datasets but it's not only a quality problem, it is also an ethical issue You as a data scientist need to be aware of it and remedy the situation whenever it's possible Medical data may be biased Females are often greatly underrepresented in many clinical studiesLarrazabal et al, note in their article that little attention is paid to the way databases are collected and how this might influence the performance of AI systems They also carried out a large-scale study using three deep neural network architectures and two well-known publicly available datasets The results show a consistent decrease in performance for underrepresented genders When female patients are excluded, or significantly underrepresented in the training data used to develop a machine learning model, then the model performs worse in diagnosing them For example, it has been found in some datasets only 6% of the total sample were womenFeldman et al, analyzed women's participation in clinical research over 25 years, from 1993 to 2018They show substantial female under representation in 7 out of 11 disease categories such as HIV, kidney diseases, and cardiovascular diseases Health care data may be biased by socio-economic status of patients According to Gianfrancesco et al, biases and deficiencies in the data used for the machine learning algorithm may contribute to socioeconomic disparities in health care They give reasons of biases in electronic health records which include patients with low health literacy may not be able to access online patient portals and document patients-reported outcomes Certain subgroups of patients may not exist in sufficient numbers for a predictive analytic algorithm Patients of low socioeconomic status may be more likely to be seen in teaching clinics where data input or clinical reasoning may be less accurate or systematically different than that from patients who have higher socioeconomic status Implicit bias by healthcare practitioners leads to disparities in care Social care data may be biased Virginia Eu banks studied the subject for over a decade Her conclusion is, ""Most people are targeted for digital scrutiny as members of social groups not as individuals People of color, migrants, and popular religious groups, sexual minorities, the poor, and other oppressed and exploited populations bear a much higher burden of monitoring and tracking than advantaged groups Marginalized groups face higher level of data collection when they access public benefits, walks through highly police neighborhoods, and there's a health care system across national borders That data acts to reinforce their marginality when it is used to target them for suspicion and extra scrutiny These groups are seen as undeserving and singled out for punitive public policy and more intensive scrutiny and the cycle begins again""Data science techniques, if not used with due care, may introduce bias For example, the US health care system uses commercial algorithms to guide health decisionsObermeyer et al found evidence of racial bias in widely used algorithm which means that Black patients assigned the same level of risk by the algorithm are sicker than white patients It's also estimated that this ratio bias reduces the number of Black patients identified for extra care by more than half What you as a data scientist can do to reduce or eliminate biases in datasets and algorithms Firstly, you need to understand your data For example, a postcode can be a proxy for socio-economical status Secondly, you can stratify your data For example, you can include more data on females in your training sets to better represent the real ratio between males and females or you can impute missing values You also can ask someone external to evaluate your datasets and algorithms for biases including for your unconscious bias"
DSM100-2022-OCT,Topic 1: Introduction,Lecture 10: Explainable AI,https://learn.london.ac.uk/mod/page/view.php?id=96107&forceview=1,"[music]-In this mini lecture, we will focus on the issue of interpretabilityand expandability of AI systems. AI programs are used to make important decisions affecting our everyday life. If to give you a bank loan or not, if to subscribe you to a social care program or not, if you're likely to commit a crime or not. Virginia Eu banks in her book Automating Inequality describes her personal painful experience when her house insurance has been suspended. It took her one year significant financial resources to correct what she believed was an automated decision. ""We spent our savings. Then we stopped paying our mortgage. Finally, we took out a new credit card and racked upan additional 5, 000 in debt. Not everyone fares so well when targeted by digital decision-making systems. Some families do not have the material resources and community support we enjoyed. ""There are many ethical issues associated with automated decisions. To complicate the metaphors, it can be hard to understand underlying models and explain how decisions are made. The main reason is that machine learning algorithms were not designed to be interpretable, unexplainable. They were designed and optimized for producing as accurate as possible predictions, generalizations, and classifications. The workings of some algorithms are easier to understand than the others. Decision trees are considered to be easily interpretable. However, if a tree is very complex and notes are not meaningful, then it may be hard to understand how decisions are made. Random forests is less interpretable and neural networks may be extremely hard to interpret. The whole concept of interpret ability and explainabilityis still not well defined. Miller defines explain ability as the level to which a system can provide the cause of its decision. Tom sett et al defines interpret ability as the degree to which a human can understand the cause of a decision. The questions are, for whom explanation should be provided. There can be different groups of explainees. For example, data scientists need to understand how recommendations about treatments of particular patients are made in order to evaluate the robustness of underlying algorithms. Doctors and patients also need to understand how such recommendations are made and if there are alternatives. General public may wish to see if there are any ethical implications in the way decisions are made, and so on. In what form explanations should be produced?In the form of natural language?Different groups of explainees would require different explanations with adifferent focus and level of detail, perhaps using different vocabularies. Moreover, software and robotic agents may require explanations for their workings. How much of explanations is enough and for what purpose?These all are open questions, and there is not enough done to answer them. Perhaps the most debated area is interpret ability and explainabilityof deep learning models. For example, Su et al showed that it is often enough to change only one pixel to fool deep neural network classification models. The researches showed that change in one pixel in about 74% of the analyzed test images made the neural nets wrongly label what they say. This doesn't boost public trust and recommendations based on machine learning. It is not good to label a stillborn as a dog. It can be life-threatening to mislabel an image as no cancer instead of cancer. There are legal rights to explanation. In US, under the Equal Credit Opportunity Act, creditors are required to notify applicants of action taken and provide specific reasons. In European Union, according to GD PR or General Data Protection Regulation, the data subject should have the right not to be subject to a decision, which may include a measure, evaluating personal aspects related to him or her which is based solely on automated processing and which produces legal effects concerning him or her or similarly significantly affects him or her, such as automatic refusal of an online credit application or e-recruiting practices without any human intervention. Unfortunately, even if the legislation requires explanations of automated decisions, it may be not technologically possible. From the report on XAI by the Royal Society, ""Some of today's AI tools are able to produce highly-accurate results, but are also highly complex. These so-called 'black box' models can be too complicated for even expert users to fully understand. ""I would like to give an example of where an automated decision could have led to World War III. On September 26 in 1983, a missile officer, Stanislav Petrov sawon his computer screen an alert about an upcoming missile attack. According to the protocol, Petrov should have initiated anuclear counterattack, but he suspected the alert was a bug. He was right and World War III was narrowly averted. This example shows the importance of having a human in the loop for making decisions. Nowadays, XAI [eXplainable AI] is a hot area of research. In 2019, Hall et al proposed a conceptual model of XAI anda systematic method for the development of XAI systems. The model consists of the following elements; an AI system, groups of explainees, and explanator module. The outcome of an AI system is a model or some decision output, which is evaluated in terms of AI system performance. An explanator forms part of the AI system which generates explanation artifacts, which in turn are evaluated in terms ofexplainability effectiveness. An explainee agent, human or machine, will consume explanation artifact sand their ability to understand. This is evaluated in terms of explanation interpret ability. The method for the development of XAI system includes the following steps. Determine the relevant explainee roles. Determine explanation characteristics. Capture explanation requirements. Assess the ability of appropriate explainable methods to meet these requirements. Map existing explainable techniques to XAI system requirements. Explain ability can be assessed by the following measures. Transparency, the level to which a system provides information about its internal workings or structure, and the data it has been trained with. Complexity, the size of an explanation. For example, rule length or decision tree depth and relationships between features presented in an explanation, for example, correlation or conjunction. Explanatory power, the scope of questions that the explanation can answer. Generalizability, the range of model to which explanation method can be applied. Interpret ability can be assessed by the following measures. Timeliness, the time required to usefully produce and interpret an explanation. The presentation of explanation, the form in which the explanation is presented. Explainee's satisfaction in explanations. Interactivity, the degree to which an explanation may be interacted with. Personalization, can explanations be given to category of individuals or a specific individual?In this mini lecture, we discussed the issue of interpretabilityand explain ability of AI systems. In some situations, explanations have low priorities than in other situations. For example, if we urgently need a new drug, we would accept the automatic prediction of what chemicals are likely to be the drugas long as it works, even if there is no explanation of why it works. However, in most of situation, explanations are essential and interpret ability is a vital feature of AI or future"
DSM100-2022-OCT,Topic 1: Introduction,Lecture: Topic 1 summary,https://learn.london.ac.uk/mod/page/view.php?id=96112&forceview=1,"[music]In topic one, we considered several approaches to defining AI. In this module, we adopted the rational agent approach to defining AI. This approach enables us to consider any agent, be it the software, be it a robot, or multi-agent system, or even the hybrid systems that includes human in the loop. We talked about what is required for defining a rational agent and how in principle such agents can be built. With talked about history of AI, and we discussed if AI can pass the Turing test. We talked about knowledge systems, and we also spent time discussing what AI system capable of doing nowadays, the state-of-the-art in AI. We talked about important ethical issue sand we also discussed that there is still not enough done about addressing these issues, and that is why it's so important that we all together do whatever possible to mitigate the risk and contribute to development of safe AI system that act in ethical and moral manner."
DSM100-2022-OCT,Topic 2: Problem solving agents,Lecture: Introduction to Topic 2,https://learn.london.ac.uk/mod/page/view.php?id=96116&forceview=1,"[music]Welcome to topic 2, Problem-Solving. In this topic, we will look at how artificial agent scan find solutions and make decisions in various, sometimes complex environments, like humans can make decisions even if our environment is not completely known to us, or if the situation can change suddenly. We are still capable of making acceptable solutions. The state-of-the-art AI technologies enable artificial agents be its software or robotic agents to find solutions and act rationally in partially observable or unobservable, stochastic and dynamic environments. The branch of AI that is dealing with such technique sis called problem-solving. I have to admit that, the search algorithms that we will be considering in this topic, may not look like particularly cool AI techniques. However, they're used for solving really intelligent tasks, and considering themis important for understanding how AI systems work. One of popular application area of problem-solving is adversarial search or games. AI systems beaten best human masters in chess and Go captured everyone's attention."
DSM100-2022-OCT,Topic 2: Problem solving agents,Lecture 1: Problem solving agents,https://learn.london.ac.uk/mod/page/view.php?id=96119&forceview=1,"[music]In this mini-lecture, we will consider problem-solving agents. An intelligent agent supposed to be able to solve problems, and they can do it in many different ways. Before going any further with the discussion of problem-solving agents, we will make several assumptions about their environment. We will assume that the environment is observable, so an agent knows the current state of the world. Discrete. In any state, there is a finite set of actions to choose from. Deterministic. Each action has one outcome. Under such assumptions, the solution of any problemis a sequence of actions. The process of finding such a solution is called a search. A search algorithm takes as input, a problem, and returns a sequence of actions. In a complex environment, it can be a difficult task. There are different approaches to finding a solution. It is also important if a solution is optimal or just a suitable one. We can continue with the vacuum cleaner agent examples. The cleaner has to find the sequence of actions to getto the locations with dirt. There are several suitable sequences. In this case, based on the performance measure, clean the floor. It doesn't matter which one it chooses. A well-defined problem is determined by the following components. First, the initial state the agent starts in. Second, a description of possible actions available to the agent. The third, transition model, a description of what each action does, successors of actions, and the state space, the set of all states reachable from the initial state. The fourth, paths and steps costs. A cost function reflects a performance measure for a problem-solving agent. Last, the goal. For the already familiar example of the vacuum cleaner agent, the problem is defined as following. The initial state is the agent's initial location. Possible actions available to the agents are move left, right, or suck in dirt. The transition model is defined by a description of what each action does. For example, suck leads to clean space, left leads to a change of the location, or nothing happens if the agent is already in the leftmost square. States are determined by combination of the agent and dirt location. Paths and steps costs. Each step can be assigned the cost one, and then the cost of a pathis the sum of steps. The goal state is all squares are clean. As you can see, this toy problem has discrete locations, reliable cleaning, and it never gets dirty. A search problem is an important class of problems. Route finding agents are used in a variety of applications, providing driving directions, video streams in computer networks, airline travel planning. A good example is automatic assembly sequencing. The aim is to find an order in which to assemble the parts of some object. If a wrong order is chosen, then there is no way to add some parts later. A possible sequence of actions starting at the initial state forms a search tree. The branches of the search tree are actions, and nodes are states from the state space. The initial state for our vacuum cleaner agent is A1. From that state, two actions are possible, moving straight to A2or right to B1. There are different search strategies. We can first explore the A2 branch going deeper and deeper, or we can gradually grow the tree by adding child nodes to all node sin the current layer. The search algorithm will explore the space until it finds all locations with dirt, B6, C4, F3, and E5. A cost function can be simply one point for each move. The goal is to minimize the cost function while cleaning all the dirt. How to measure problem-solving performance. A search algorithm has to take care about many things. Selecting a good search strategy, avoiding loops, and achieving the goal state. The performance of a search algorithm can be evaluated by completeness. Does the algorithm guarantee to find a solution if there is one?Optimality. Does the algorithm find an optimal solution?Time complexity. How long does it take to find the solution?Space complexity. How much memory is needed to perform the search?In following mini-lectures, we will consider different types of search algorithms. Let us consider another example, the 8-puzzle. It belongs to the family of sliding block puzzles, which are often use din AI as the test problem for new search algorithms. The 8-puzzle consists of a three-by-three board with eight number tiles and a blank space. A tile adjacent to a blank space can slide into the space. The problem definition is, states are defined as the location of each of the eight tiles and the blank space. Any state can be an initial state. Actions are movements of the blank space left, right, up, down. Transitions are defined by combinations of state and action leading to a new state. Each step costs one point, and the goal state is shown in the previous slide. The problem belongs to the class of NP-complete problems. NP stands for non-deterministic polynomial time. Such problems require polynomial time to find the solution, and are considered to be easy to solve. 8-puzzle has 181. 440 possible states. 15-puzzle has about 1. 3 trillions of possible states, and so on, but it is still easy to find the solution. NP-hard problems are problems with no polynomial-time solutions"
DSM100-2022-OCT,Topic 2: Problem solving agents,Lecture 2: Uninformed search,https://learn.london.ac.uk/mod/page/view.php?id=96124&forceview=1,"[MUSIC]In this mini-lecture, we will talk about several search strategies that are called blind or uninformed. This means that an agent doesn't have any additional information beyond the problem definition itself. Such an agent decides on the next step and then evaluates if the goal state was achieved or not. Blind search strategies differ from each other by the order they select the next nodes for expansion. We will consider breadth-first, uniform-cost, and depth-first searches. Breadth-first search is a strategy in which the root node is expanded first, then, all the successors of the root node are expanded, then their successors, and so on. The figure on this slide shows an example search tree. The root node A1 is first expanded to nodes A2 and B1. Then at the next step, A2 is expanded to A3, and B1 is expanded to C1. At the next step, A3 is expanded to A4 and B4and so on, level by level. Often the maximum depth of expansion is fixed. Now, we will assess the breadth-first search by the criteria we previously introduced. Completeness, the algorithm guarantees to find the solution if there is one at the finite depth. Optimality, generally it doesn't guarantee to find an optimal solution, but it will find the first goal state and then stop. However, for uniform-cost search where the cost of each step is equal, the solution will be optimal. Time and space complexity. This type of search is not economical, it requires exponential time and space to complete the search. For example, if 1 million nodes can be generated per second, one node requires 1000 bytes of storage. The search tree with branching factor b10and the depth of solution d10will require 3 hours and 10 terabytes. If the depth is 14, then it will require three and a half year sand 99 petabytes, and so on. It is not a very practical strategy. Depth-first search always expands the deepest node firs tin the current frontier of the search tree. For example, search tree, the root node A1is first expanded to nodes A2, A3, and A4. If solution is not there, then the algorithm will backtraceto the node A3 and expand it to B4. If no solution yet, then it will go back to A2, expand to B3, C4. If still no solution, then go back to B1 and expand to C1. Let us assume that C1 is the goal state then the breadth-first search will actually find C1 faster. As with breadth-first search, depth-first search is complete. It guarantees to find the solution if there is one and also if it avoids loops, but it doesn't guarantee to find an optimal one, only for uniform-cost search. Time complexity is exponential but depth-first search is more economical with space. There are many variants of these blind search strategies to overcome their inefficiency in using space and time. Depth-limited search. In many situations, a depth of search is limited and this can be used to improve the search strategy. For example, if a self-driving car agent is planning an optimal route from city X to city Y, then the search depth may be limitedby number of cities in the considered region. It is known as the diameter of the space. There are various combinations of depth-first and breadth-first searches. For example, iterative deepening depth-first search tries to find the best depth limit. Bidirectional search runs two searches in different directions. In this mini-lecture, we discussed uninformed search strategies, how to find the goal blindly. It is usually possible to dobut that requires time and a lot of space exploration."
DSM100-2022-OCT,Topic 2: Problem solving agents,Lecture 3: Informed search,https://learn.london.ac.uk/mod/page/view.php?id=96129&forceview=1,"[music]In the previous mini-lecture, we talked about limitations of blind search strategies. They require exponential time and considerable space to complete the search. In this mini-lecture, we will talk about how problem-specific knowledge, knowledge beyond the problem definition itself can help in finding solution more efficiently. Such search strategies are called informed or heuristic searches. Problem-specific knowledge can be expressed in different ways. It can be expressed as an evaluation function or it can be expressed as a heuristic function, as a component of the evaluation function. The main strategy for informed searchis the best-first search strategy where a node is selected for expansion based on an evaluation functionas it is likely quickly lead to a solution. The figure on this slide shows a search tree and the path to the goal state is highlighted in red. A good evaluation or heuristic function would direct this search to this path instead of systematically checking all possible options following breadths first or depths first strategies. The 8 puzzle is a classic example of heuristic search problem. The figure on the left shows an initial state, and on the right, the target state. The average solution has 22 steps. A blind search will systematically check all possible movements until it finds a solution. An informed search can be directed by heuristic. We can introduce various heuristic functions. For example, h1 is the number of misplaced tiles. All of the 8 tiles are in the wrong place, so the start state has h1=8. We can introduce a different heuristics. h2 is the sum of the distances of the tiles from their goal positions. For example, to get the tile 1 to the goal position, three moves are required. To put tile 2 to the goal position, two moves are required, and so on. The table on this slide shows a comparison of the search cost for the blind iterative deepening search and two heuristic searches using h1 and h2. The data are averaged over 100 instances of rounds of respective algorithms. As you can see, the use of heuristics substantially reduced the number of generated node sin the search trees. For example for the search depth of six, 680 nodes were generated by iterative deepening search, while using h1 reduced it to 20, and using h2 to 18. The deeper search, the more beneficial heuristics are. You also can see that heuristic h2 is better than h1. How one can find heuristic functions?Heuristic functions can be generated based on simplified, or so-called relaxed problems. They also can be learned from experience. If an agent solved many 8-puzzles and remember solutions, each example would include the initial state and the cost of the solution. [?] an agent can learn to predict the cost of solution for a new initial state and use it as a heuristic function. Features of the search space can be used as heuristics too. In the considered examples, the number of misplaced tiles is helpful in predicting the solution cost. For example, an agent may learn that if the number of misplaced tiles is five, then the cost of solution is usually 14, and so on. Thus, knowledge about the number of misplaced tiles is helpful. One of the best-known heuristics search is the A* search. It evaluates nodes by the evaluation function that includes the cost of reaching a node and heuristic. An evaluation function is the estimate of the cheapest solution through a node. A* search has nice properties and it can be complete and optimal under certain conditions. One of disadvantages of this approach is high demand for memory. Memory-bound heuristic searches are designed to overcome the limitations of the A* search. Iterative-deepening A* algorithms uses an evaluation function to determine a cut-off when to consider that the node should not be expanded further. Recursive best-first search is similar to recursive depth-first search but uses an evaluation function to limit the depth of search. Memory-bounded A* and its simplified version, SMA*, are improved versions of A* search which are used in memory more efficiently. To save the memory based on the evaluation function, they can drop worst leaf nodes and keep only the best ones. In this mini-lecture, we discussed informed search strategies, how to use problem-specific knowledge to find a goal quicker. Heuristic search algorithms are widely used in many applications, but their performance depends on the quality of the heuristic function. Heuristic function can belong from experience or generated from relaxed problems."
DSM100-2022-OCT,Topic 2: Problem solving agents,Lecture 4: Genetic algorithms,https://learn.london.ac.uk/mod/page/view.php?id=96134&forceview=1,"[MUSIC]In the previous mini-lectures, we can see how an intelligent agent can find a solution in a non-observable and deterministic environment. Now, we will look at what happened in more complex environments. In this mini-lecture, we will look at the local search. The search of algorithms that we have seen farare designed to explore the search space systematically. The past to goNconstituted a solution to a problem. In many situations, however, the pass to the go is irrelevant. The flow is clean, the puzzles are solved, an object is assembled and it doesn't matter how. We can consider a different class of algorithms, a local search. Local search algorithms operate using a single current node[?] and multiple node path sand generally move only to neighbors of that node. Typically, the path forward by the search are not retained. Local search algorithms are not systematic but they have key advantages. They use very little memory, usually, a constant amount and they can often find reasonable solution sin large or infinity continuous states basis for which more systematic algorithms are unsuitable. Local search algorithms are useful for finding optimal solutions for optimization problems. One of the best-known local search algorithm sis a genetic algorithm. It is inspired by Charles Darwin's theory of natural evolution. This algorithm reflects the process of natural selection, where the fittest individuals are selected for further reproduction. The offspring inherit characteristics of their parents. Individuals in the new generation are assessed, the fittest are selected for further reproduction, and so on until the process converges if there are no improvements in fitness over the next generation. The idea is that natural selection will find a solution for the problem and often it does, but not obvious. A genetic algorithm will increase the population of chromosomes, a set of potential solutions for the problem. For example, A1, A2, A3, and A4as it is shown on this slide. A chromosome is represented as a sequence with digits, usually zeros and ones. Each digit represents a gene. A genetic algorithm includes three evolutionary processes, selection, gene crossover, and mutation. Consequently, there are five components to a genetic algorithm. Initial population and we just have seen an example of the initial population on the previous slide. The fitness function, selection, crossover, and mutation. The best-fitting individual in the initial population are selected for the production. Some individuals can be selected several time sand some are non. To produce offspring, genes are exchanged. Across [?] can be selected randomly. On this slide, the first offspring has first three digits or genes from the second parent and the last three digits from the first parent. Newly produced individuals can be added to the total population. Individuals with low fitness can be excluded from the population. In addition to crossover or gene exchange, genetic algorithms also include mutation. Cross overs correspond to the most significant changes in population and mutation corresponds to minor changes. Those mechanisms contribute to maintaining diversity in the population and ultimately finding a solution. In this mini-lecture, we looked at how genetic algorithms work. Genetic algorithms are great for finding optimal or good enough solution sin complex situations where most standard approaches may struggle. Later in this module, we will consider a case study that employs a genetic algorithm, and you will have an opportunity to look inside closer. If you're interested in this topic, I recommend you to check additional literature."
DSM100-2022-OCT,Topic 2: Problem solving agents,"Lecture 5: Nondeterministic search, partially observable space",https://learn.london.ac.uk/mod/page/view.php?id=96139&forceview=1,"[music]Previously, we assume that an agent is acting in a deterministic and observable environment. In this mini-lecture, we will consider non-deterministic, partially observable, or even unobservable environments. Just to remind you, if the next state of the environment is completely determinedby the current state and the action executed by the agent, then it is a deterministic environment, otherwise, it is non-deterministic or stochastic environment. If an agent's sensors give it access to the complete state of the environmentat any time point, then it is an observable environment. Otherwise, it is a partially observable or unobservable environment. For such environments, percepts are of particular use. Every percept gives additional information about the environment and it helps to narrow down possibilities. If the environment is non-deterministic, percepts tell the agent which of the possible outcomes of actions has actually occurred. If the environment is partially observable, percepts narrow down the set of possible states. In any case, percepts cannot be determined in advance. Agents' future actions depend on future percept sand the solution to a problem is no longer a sequence of actions but a contingency plan. What to do depending on what percepts are received. This is how humans act. The environment we live in is stochastic. We don't know the certainty, what is going to happen tomorrow or in a week. It is partially observable, there is a lot that we still don't know about our environment. We don't have a full map of the ocean floor. We don't fully understand how financial markets works. We don't know how many students we have for the next term, yet we can make short-term and long-term decisions. We are also good at it. This is intelligent behavior. In this mini-lecture, we will look at how artificial agents can actin such complex environment. First, we will consider non-deterministic search and we need to change how a problem is defined. We need to generalize the notion of a transition model, a description of what each action does before actions had unexpected effects. In a non-deterministic environment, a transition model has to define a set of possible outcomes for each action. We also need to generalize the notion of a solution. If before, it was a sequence of actions, now it is more of if-then-else statements. This can be represented as AND-OR search trees. We have already seen examples of search trees in the previous lectures. The branching was introduced by the agent's own choice and that corresponds to OR nodes. In non-deterministic environments, environment also can introduce choice sand such choices correspond to AND nodes. Now what we'll consider, so erratic familiar example, it's a vacuum cleaner. We will introduce nondeterminism in the form of a powerful but erratic vacuum cleaner behavior. Now, the suck action works erratically, as follows. When applied to a dirty square, it cleans the square, but sometimes it also cleans the next square. When applied to a clean square, sometimes it deposits dirt. The transition model now contains a set of possible outcomes for the action suck. The figure on this slide shows an example AND-OR search tree, a fragment of it. For the erratic vacuum cleaner, the suck action is now non-deterministic with two possible outcomes. It is depicted with OR node whereas the action is deterministic like move right, it is depicted with AND node. A solution for AND-OR search problem is a sub tree set, has a Goal node at every leaf specifies one action at eachOR node and specifies every outcome branch at each AND node. Now we'll consider partially observable environments, an agent would know where it is, in what state it is. The key concept required for solving partially observable problems is the belief state. The agent's current belief about the possible physical state it might be in, given a sequence of actions and percepts up to that point. As a definition for problem of partially observable environments has to be generalized further. Initial state now can be any state. It is a set of all states. Actions depend on the state the agent is in. Suppose the agent is in the belief state B, this possible states S1 or S2, and the maybe actions for state S1 differ from actions of state S2. The actions for the belief state B can be defined, for example as union of all actions for state S, and all actions for state S2. Percepts, all observations are critical for solving partially observable problem sand it also considerably harder to define a transition model. Instead of defining the next step or the next state, there are now predicted. We will consider this on the classic example for a robot that has to work out where it is, the localization problem or a maze problem. The robot in this maze-like environment has sensors that can detect obstacle sin all directions, north, south, west, and east. Obstacles like walls are depicted by dark yellow on this slide. The robot just has been switched on and it doesn't know where it is. It can be any state in any square in this figure. The robot receives a percept. Obstacles are at north and west. This narrows down the possibilities. There are only three states where obstacles are at north and west, then the robot moves. It still doesn't know exactly where it is. It can be in any adjacent squares to the previous identified three squares. It received new percept and percept tells that obstacle is at ea stand there is only one location where it is possible. Is the problem solved?Of course, it is not obvious possible to find solution in such complex environments. However, often it is possible. In this mini-lecture, we'll look at how intelligent agents can solve problems, find solutions in a more complex environments. When environment is non-deterministic and not observable. Stochastic and non-deterministic search algorithms are important class of algorithms with many real-world applications in robotics, diagnostics, and manufacturing."
DSM100-2022-OCT,Topic 2: Problem solving agents,Lecture 6: Games,https://learn.london.ac.uk/mod/page/view.php?id=96144&forceview=1,"[music]In this mini-lecture, we will consider multi agent environments where each agent has to consideractions by other agents and their effects. We will consider a special case, a competitive environment where agents' goals are in a conflict. Such problems are called adversarial search problems or games. We will first consider deterministic observable environments where agents act alternately and in which the utility value sat the end of the game are always equal and opposite. If one agent wins the game of chess, another loses. We will start from considering a standard example in the game series, the prisoner's dilemma. This simple example shows the complexity of a multi agent environment. The example is particularly interesting because it demonstrates tendencies in corporate right behavior. This game was introduced by two American mathematicians in the '50s, Merrill Flood and Melvin Dresher. The rules of the game are two members of a criminal organization are arrested and imprisoned. Each prison is in a solitary cell with no means of communicating with the other. The prosecutors lack sufficient evidence to convict the pair on principal charge, but they have enough to convict both on a lesser charge. The prosecutors offer each prisoner a bargain. Each prisoner is given the opportunity either to betray the other by testifying that the other committed the crime or to cooperate with the other by remaining silent. The possible outcomes are:If A and B prisoners each betrays the other, each of them serves two years in prison. If A betrays B but B remains silent, then A goes free and B will serve three years in prison. If A remains silent, but B betrays A, A will serve three years in prison and B goes free. If A and B both remain silent, both of them will serve only one year in prisonon the lesser charge. The optimal strategy is to betray the partner because overall it offers a greater reward than cooperating with them. Interestingly, in the reality, humans display a systematic bias\ towards cooperative behavior in this and all the similar scenarios. This has been demonstrated by many studies. Another example of a multi-agent environment is a RoboCup, a soccer tournament for robots. RoboCup was introduced by Professor Hiroaki Kitana from Japan, more than 20 years ago. I'm privileged to know Ki tana-sensei, it is how professors are called in Japan personally. Generally in AI game research, the focus is on abstract games because it is easier to represent the game states. Physical games are rarely in the focus because it is difficult to represent all the states, it's a more complex description, and the range of possible actions is wider. RoboCup is an example of a dynamic environment with many agents. Some of them cooperate as members of the same team with a shared goal to win the game and other members of different teams have competing goals. The robot world cup initiative is an attempt to force AI and intelligent robotics research by providing a standard problem where a wide range of technologies can be integrated and examined. Design principles of autonomous agents, multi agent collaboration, strategy acquisition, real-time reasoning, robotics, and sensor fusion. I recommend watching a video of earlier Robocup. The robots are really clumsy. However, this initiative has boosted the research in this area and the target now is for a robot team to beat the best human team by 2030. In this mini-lecture, we looked at multi-agent competitive environments or games. First, we considered a deterministic two-agents environment, the well-known prisoner's dilemma. Then we considered a more complex dynamic situation with many agents, robots playing a soccer game. There are many more examples of significant advance sin the game series and practice. More than 20 years ago, Deep Blue beat Russian chess master Garry Kasparov. 10 years ago, Watson won in Jeopardy. In my view, the most impressive exampleis for an artificial agent to win in the game of Go. ves. Go has long been viewed as the most challenging of classic games because of its enormous search space and the difficulty in evaluating both positions and mo In 2016, the program Alpha Go defeated the human European gold champion by five to zero."
DSM100-2022-OCT,Topic 2: Problem solving agents,Lecture 7: Optimal search,https://learn.london.ac.uk/mod/page/view.php?id=96148&forceview=1,"[music]-In this mini-lecture, we will consider optimal strategies for making decisions. Minimax and Alpha-Beta pruning. In normal search problems, an optimal solution would be a sequence of actions leading to a goal stateor in case of a game, to a win. In adversarial search, an agent aims to maximize its utility value by selecting moves that leads to an outcome with the best utility. An agent also acts under the assumption that the opponent try to minimize its utility value. Such strategy is called minimax. A minimax search tree is very similar to AND-OR search tree that we considered previously. Max-nodes playing the role of OR-nodes representing actions by the agent. Min-nodes are playing the role of AND-nodes representing the actions of the opponent. The minimax algorithmis essentially depth-first search algorithm. The minimax algorithm can be extended for more than two players. As we discussed previously, a depth-first search algorithm is grid. It requires exponential amount of time and space to complete the search. Consequently, minimax algorithm is so very demanding. This can be rectified in several ways. Alpha-Beta pruning can cut away branches of the search tree that are unlikely to contribute to final solutions. For example, if it is clear that the certain branches cannot supply minimum or maximum values. It is also matterin what order the nodes or moves are selected. If better nodes are selected sooner than larger portions of a search tree can be pruned. Search strategy can be improved further by utilizing the main knowledge. For example, if an agent remembers previous situations, what move was successful?What moves led to good solutions?The best moves are called killer moves, and the search strategy that is using such moves it called killer move heuristic. The minimax algorithm works with the entire game tree. Alpha-Beta pruning cuts off unpromising branches. For real-time decision, it makes sense to reduce the depths of the searchor length of branches. The rationale for that is that if the decision time is limited, there is no time to do a full-length search and the search has to be cut off at certain point. Such cut can be fixed or iterative deepening approach can be applied. The minimax and alpha-beta pruning also can be improved by replacing a utility function with a heuristic evaluation function. Heuristic evaluation function estimates the utility of the game from a given position just as the previously considered heuristic function estimates the distance to the goal. The performance of game-playing programs depends strongly on the quality of its evaluation function. A good evaluation function will guide an agent towards the best positions and moves. For example, in chess, an evaluation function can calculate the number of white pawns, black pawns, white knights, black knights, and so on. These features taken together form categories of game states. Certain states may lead to wins more often than others. For example, when category contains all two pawns versus one pawn end games. It might be known from the previous experience that 72% of such situations lead to a win. The environment can beeven more complex than we already discussed. For example, in card games, the environment is stochastic and partially observable. In mini card games, cards are dealt with randomly, with each player receiving a hand that is not visible to other players. Even in such complex situation, minimax and heuristic search still can find good solutions. We have already talked about using domain-specific knowledge for improving search strategies and also using the past experience. An intelligent agent can record past moves, analyze which ones were successful and try to use them again. This kind of learning is called meta-learning. A meta-search is a search over other searches to learn what types of searches with what parameters work best in what situations. It is sometimes also called meta-reasoning. Such meta-knowledge is usually transferable to other problems. In this mini-lecture, we discussed optimal decision-making strategies, minimax, and alpha-beta pruning, and also there are modifications to improve their performances. It is particularly important to restrict the search depths in real-time scenarios. An agent may have only minutes, maybe seconds to decide on the next move. In such situation, there is no time to go through the entire search tree. Unpromising branches should be pruned. The depth length should be fixed and heuristic can be applied."
DSM100-2022-OCT,Topic 2: Problem solving agents,Lecture: Topic 2 summary,https://learn.london.ac.uk/mod/page/view.php?id=96152&forceview=1,"[music]In this topic, we discussed how intelligent agents can find solutions and make decisions. We can see that uninformed or blind search, informed or heuristic search, and we also discuss that domain-specific knowledge can improve decision-making substantially. The more agent knows and remembers from the past, the more intelligent behavior it can exhibit. We also talked about meta-learning or meta-reasoning. When different search strategies are analyzed and compared, in what situation, what strategy would work best. If you're interested in this topic, please check the recommended literature. Of course, there are plenty of resources available in our online library and around."
DSM100-2022-OCT,"Topic 3: Knowledge, reasoning and planning",Lecture: Introduction to Topic 3,https://learn.london.ac.uk/mod/page/view.php?id=96156&forceview=1,"[music]Welcome to my favorite topic, reasoning. The ability to reason over knowledge. That is what makes intelligent systems truly intelligent. Mere knowing, remembering, and using it for making decision sis a highly intelligent behavior. Being able to make logical inferences over such knowledgeis the next level. AI systems have superhuman ability in making flawless logical conclusions from available data and knowledge. To unleash this power, data and knowledge have to be modeled and encode din a machine-processable form. In this topic, we will talk about technologies for knowledge representation, have a look at knowledge representation language sand discuss examples. We will consider different types of logics to reason over knowledge. Deduction, abduction, and induction. You will learn why abductive and inductive reasoning requires an extra step of verification. By the way, the deductive method of Sherlock Holmes is actually abductive. We will revisit knowledge-based agents to see how planning and scheduling can be done. Good luck with this exciting topic."
DSM100-2022-OCT,"Topic 3: Knowledge, reasoning and planning",Lecture 1: Knowledge-based agents,https://learn.london.ac.uk/mod/page/view.php?id=96159&forceview=1,"[music]-In this mini lecture, we will talk about knowledge-based agents. We have already seen in the previous topics that knowledge makes an agent more intelligent, be it domain-specific knowledge, or knowledge of the past experience, what worked well and what did not. The more an agent knows and remembers, the better decisions it can make. Agents that are using knowledge for making decisions are called knowledge-based agents. This component of a knowledge-based agent is called, unsurprisingly, a knowledge base. Knowledge has to be encoded in a machine processable way, usually, using knowledge representation languages. What is knowledge?Generally, there is no clear separation in the means between data, knowledge, and information. It is often the case even in computer science. It is, well, stating explicitly what we will consider as such in this module. Data are facts. Typically, data are encoded as a table. For example, if a table stores various chemical element sand their freezing and melting points, then an example of a data item or a fact would be, melting point of helium is -272. 2 celsius. Typically, knowledge is encoded as a set of rules. For example, if the temperature is below the melting point, then the chemical substance is in a liquid state. More advanced knowledge can be represented in the form of an executable model. For example, a metabolic model captures our knowledge about the metabolic pathways, and molecular mechanisms of a particular organism. Information is a more generic term that combines data and knowledge. Problems-specific and domain specific knowledge is called background knowledge. This can be available to a knowledge-based agent from the very beginning. As the agent gains experience, learns from errors, and also successful moves, it updates its knowledge. Knowledge base can be divided into declarative and procedural parts. The declarative part contains statements about the problem, domain, goals. The procedural part defines agent's behavior. An example of a knowledge-based system is a medical diagnostic system that uses metabolic model as the background knowledge. It can take as an input, test results of a particular patient and make inference about what metabolic pathways could be disrupted, and what drugs, possibly, could mitigate such disruptions. The first knowledge base systems were rule-based expert systems. One of the most famous was MYCIN, a program for medical diagnosis. MYCIN was developed in the early 1970s at Stanford University by Edward Shortliffe. It could identify what bacteria were causing infections, for example, meningitis. MYCIN could recommend antibiotics with the dosage adjusted for patient's body weight. Later in this module, we will talk about other knowledge-based systems. In this mini lecture, we introduce a concept of a knowledge-based agent, and we discuss the difference between data and knowledge, and how a knowledge base can be separated into"
DSM100-2022-OCT,"Topic 3: Knowledge, reasoning and planning",Lecture 2: Knowledge representation and knowledge base,https://learn.london.ac.uk/mod/page/view.php?id=96163&forceview=1,"[MUSIC]We have already seen that knowledge is essential for intelligent behavior of artificial agents. In this mini-lecture, we will talk about how to represent and encode knowledge in a machine-processable wayso that a knowledge-based agent can access, process, and update its knowledge base. Knowledge can be represented and encoded in many ways. For example, it can be encoded directly in Java or Python but it is not how it is usually done. Usually, it is done with the use of knowledge representation languages. One of the main reasons for thatis that the power of knowledge comes from reasoning a logical inference. To enable logical inference over knowledge, that knowledge has to be encoded in logic. Conventional programming languages are not designed for that. However, the situation is changing and there are ways around. Logics can be different. Description logic, propositional logic, first-order logic, higher-order logic, monotonic, non-monotonic, temporal, modal, and so on. Different logics feature in the different balance between expressive power and reasoning complexity. In this topic, we will briefly look at two logics. First-order logic or FOLis one of the most expressive logic sand description logic or DL. DL is popular because, in contrast to FOL, the core reasoning problems for D Ls are usually decidable. This means that an inference engine usually can come to a conclusion, while for programs in FOL, it is not always the case. We also will look at knowledge representation languages that are popular for encoding statements in those logics, Prolog and RDF, or Resource Description Framework. There is no expectation in this module for you to become proficient in RDF or prolog programming. The expectation is for you to be familiar with RDF and prolog capabilities and understand how they can be used to encode knowledge in logic. We will start from considering an example. Let us model some knowledge about our program. Master in Data Science is a postgraduate program run by the University of London in partnership with Goldsmiths' academic provision. It includes such modules as machine learning, neural networks, artificial intelligence, et cetera. Students enrolled to any of the module scan raise academic queries with their tutor sand all other queries with Student Relationship Managers. First, we will extract entities and then we will model relations between those entities. Entities, physical things that exist in the world. Mentioned in the description are a postgraduate program, MSc in Data Science program, the University of London, Goldsmiths, students, tutors, Students Relationship Managers, and so on. These entities are linked by different relations. For example, a student enrolled to a module, a student raises a query, and we see data science program includes modules. We can link the academic query and tutor by the link handles, and non-academic query, we link to the Student Relation Manager. We will represent this knowledge in the form of a knowledge modelor a knowledge graph shown on this slide. Such a knowledge model can be encoded in different languages so that a knowledge-based agent can understand and use this knowledge. First, we will look at how it can be encoded in RDF, Resource Description Framework. RDF provides a model for describing resources. RDF defines a resource as any object that is uniquely identifiable by a URI, Uniform Resource Identifier. Other objects that we identified in the considered example like a student, a module, we can represent as a resource. Resources have properties that link resources to other resources. In our examples, relations between entities, or they can link resources to some values, like text strings or numbers. Such properties are called data properties. RDF essentially encodes knowledge in the form of triplets, resource X, property Y, resource Z, or student, raises, query. The logic behind RDF is description logic. This means that prisoners can easily process models in RDFand in FOL conclusions. As you will see from the examples, we will mainly describe things not much reason-based. More precisely, we will be considering the RDFS, Resource Description Framework Schema. There're different RDFS formats that have different purposes. The Turtle format is a syntax that is easy to read for human sand that is what we will be using. Classes in RDF notation, RDF class, are used to classify resources. An instance of an RDF class is defined using the predicate rdf: type. For example, we can define that module is a class and that machine learning is a subclass of the module class. Total offers the abbreviation of rdf: type in the form of the letter A, which makes the syntax short and easy to read. In this notation, we use ex or example, as an abbreviation for the IRI prefix. As I explained, each resource must have Unique Resource Identifier. We will use this name space for our example. This is not a real URIbut I could register it for useor I could use my university URI for this example. Rdfs: Property is used to add relations or attributes to classes. Now we can encode student, raises, query, as it is shown on the slide. In practice, you don't need to write such statements manually. They can be automatically generated from a knowledge model. The attractive features of knowledge model sis that they can be extended and integrated with other knowledge models. From our example, you can see the major difference betweenRDF and programming language sis if something is defined in RDF, it is defined globally and uniquely. Therefore, if there are other knowledge models that define such resources as a person, and there are, then those resources can be explicitly linked, or imported to your knowledge model. For example, Person is already defined by FOAF or Friend of a Friend resource. We could simply use their class instead of defining ours. Integration is achieved via linking entities defined in different knowledge models. For example, we can integrate our knowledge model with an event model by adding a link located in between Goldsmiths and London. We then can combine the knowledge about the MSc Data Science program and events in London. We can imagine developing an agent, advising students about relevant and interesting events. We can easily extend our initial knowledge model. For example, we can add the entity module leader and link it to Goldsmiths by the link hired by, and to academic query by the link handles. This will reflect our current knowledge that tutors can raise academic queries with the module leader. Capturing existing the main knowledge is useful even for understanding of the domain. The example explains the division in handling academic and non-academic queries since Goldsmiths is responsible for academic model sand the University of London for all others. We can think about several scenario show such a knowledge model can be used. For example, we can design an agent for classifying queries to academic and non-academic, and advising whom to contact and how, and monitoring the progress of the query, or we can have an agent for an internal use to better direct your queries internally to assess if you raised it correctly and redirect it if necessary to speed up the process. We also can analyze what sort of queries were raised incorrectly, and learn from that to improve the overall process of communication. In this mini-lecture, we can see that knowledge-based agents and their co-component are knowledge base, and how it can be represented as a knowledge graph and encoded in RDF."
DSM100-2022-OCT,"Topic 3: Knowledge, reasoning and planning",Lecture 3: Ontological engineering Part I,https://learn.london.ac.uk/mod/page/view.php?id=96167&forceview=1,"[music]In this mini lecture, we will discuss how to construct good, logically consistent knowledge models. As with any area, there are good and bad practice sin designing knowledge models. The area of research that deals with engineering of knowledge model sis called ontology engineering. Ontological research has a long history over 2, 500 years. It dates back to Aristotle. Our focus on computational aspects of ontological research, not historical, even if it is really interesting. A good approach the knowledge modeling is to define first what is what. That is like putting a backbone in place before defining any other relationships. Such a solid structure when all entities are explicitly linked, helps with reasoning. In the previously considered example about master data science program, some of the entities were not well-connected, and there was no overall clear structure. We can remedy that. All things can be grouped into four categories. First, organization, education unit, and query. We can link all entities in the considered example by is-a relations or subtype, or subclass relations. University of London is organization. Student is a person. Now, we have a good structure in place. Moreover, the relation is- a has important properties which are used by inference engines. Everything, what is true for an upper-level entity is also true for all its subclasses. If we define that a query has such a property, that it can be raised by a student, then both subclasses, academic and non-academic queries, will inherit this property. If we define that a person has a name, then all sub entities, a student, a tutor, a manager, will inherit this property. No need of defining it separately for each of subclass. A good practice is to have a tree-like is-a hierarchy of classes. This is called the single inheritance approach. It is usually easier to follow multiple inheritance modeling approach where a class can have multiple parents, but then it is harder for reasoners to process such models. How to define categories?This question dates back to Aristotle who proposed the realist theory of categories all the mates. Aristotle defined such categories as substance, quality, quantity, location, and so on. This didn't change much. Knowledge engineers are still using some of the categories defined by Aristotle. The selection of categories or upper level, or top-level classes, as they're called in ontology engineering, depends on the domain one has to model. There are some ready sets of categories available. They are called upper-level ontologies. They're like pamphlets which you can use for the development of your model. What is an ontology?The most common definition in computer scienceis given by Tom Gruber in 1995. An ontology is a formal and explicit specification of a shared conceptualization of a domain. The key features of ontologies from the computational perspective are, an unambiguous representation of a particular domain, makes implicit knowledge explicit, enables reasoning, both human-readable and machine-readable, shared by a group of stakeholders. Each feature is important, including sharing. Developing knowledge models is time consuming and requires expertise. Therefore, they must be reusable. What is an ontology engineering?It has aspects of both software engineering and knowledge engineering. From knowledge engineering point of view, acquisition and structuring of the relevant knowledge, implementation of the structured knowledge into knowledge bases, testing and validation of the inserted knowledge. From software engineering point of view, identification of user requirements, versioning, testing, documenting, evaluating, and so on. Typical applications of ontologies include using an ontologyas a controlled vocabulary and as a standard. An example is gene ontology, one of the first, and probably the most famous, ontology. Gene ontology provides a computational representation of our knowledge about genes and gene products across all species. Ont ologies are great for data integration, especially of heterogeneous datasets. A good example is linked open data initiative that links well over 1, 000 datasets. More and more often, ontologies are used as knowledge graphs. They define the meaning of the nodes and links, allow to derive new information, and support dynamic updates, data governance, provenance. Example is Google knowledge Graph. What is the most relevant to our topic is the use an ontologyas a knowledge base for knowledge-based agents. It can model not only the main knowledge like in the example we considered, but also problems, goals, and the agent itself, its components, functionality and means of communication. In this mini lecture we talked about ontology engineering, my favorite topic. Well-modeled ontologies can be used as knowledge bases for knowledge-based agents, as tools for data and knowledge integration. Also, it's very useful for humans in various applications"
DSM100-2022-OCT,"Topic 3: Knowledge, reasoning and planning",Lecture 4: Ontological engineering Part II,https://learn.london.ac.uk/mod/page/view.php?id=96168&forceview=1,"[music]-In this mini lecture, we will continue discussing how to construct logically consistent knowledge models. In particular, we will look at how to define instances of classes and what is the difference between instances and classes. We'll also will talk about best practices in defined relations. We will start from considered relations in a knowledge model. So far, in our example, we identified such relations as is-a and roles, races and [?] and so on. We can define more relations if necessary to better reflect our knowledge and also for specific applications. For example, if we are designing an agent to handle financial operations, we can add to the knowledge model such classes as payment, module cost and relations like [?] cost made payment. We have seen in the RDF notation that each resource has a globally unique identifier. This is true not only for classes but also for relations. It is a good practice to import already defined classes instead of redefining them with a different UDI. The same applies to relations. Instead of defining some ad hoc, you can use already defined relations. For example, FOAF or friend or friend ontology, define such relations as name, title, [?]. Dublin Core ontology defines such relations as creator, identifier, references. Now, we will talk about instances. As we discussed before, a good approach to knowledge modelling is to define first what is what. To put an is-a hierarchy in place before defining any other relationships. An is-a hierarchy can be extended by instance of relations. Not all knowledge models include instances, and it is perfectly all right for a model to define only classes. It is considered to be a conceptual modelling. For many applications, it is necessary to define instances of classes. For example, we may need to add to our example knowledge model about the program concrete students, tutors and module leaders as instances of generic classes, student, tutor and module leader. If an agent has to deal with a particular query, then it needs to know who raised it and to whom it assigned to handle. Sometimes, it's difficult to decide if the model and entity as a class or as an instance. Let us consider the class machine learning module. Depended on an application, we could model ML, AI or neural networks as instances of the class module. However, for dealing with queries, we have to define ML as a class and each version of it as an instance. There are several versions of the same machine learning module, for examples in modules that started last year in October and another one that started this year in April. It is important to specify to what version a student is enrolled even if the module is essentially the same. One reason is the tutors are assigned to a concrete version of the module. Another reason is that it actually beneficial to keep the history of all previous queries and perhaps an answer has been already given to a similar query. What is important to understand is if a relation is defined between two classes, for example, student enrolls module, then all instances of those classes should also be connected by that relation. That means that each student has to be enrolled to at least one module. If a relation handles is defined between the classes, academic queries and tutor, that means that each instance of an academic query must be assigned to an individual tutor. Instances of a class are defined by the RDF statement similar to how classes are defined. In this mini lecture, we'll continue to learn how to design good knowledge models. We talked about how to define instances of classes and how to reuse already defined relations."
DSM100-2022-OCT,"Topic 3: Knowledge, reasoning and planning",Lecture 5: Logic,https://learn.london.ac.uk/mod/page/view.php?id=96173&forceview=1,"[music]Agents intelligence relies not only on knowing the problem domain and remembering the past experience. Intelligence also relies on the ability to reason over such knowledge. What worked best in the past and why?What new knowledge can be inferred from that?To enable reasoning, knowledge has to be represented and encoded in logic. There are many logics; propositional, declarative, temporal, monotonic, and so on. In this mini-lecture, we will talk about the first-order logic, or FOL, as one of the most expressive logics. It is also one of the best-supported logics. FOL reasoners have been around for decades. They're powerful, reliable, and have a healthy community of users. FOL-based language Prolog is considered as one of the main languages of AI. For example, components of the famous IBM Watson, the winner of Jeopardy, were written in Prolog. We will be looking at the first-order logic from computational perspectives, from Prolog point of view rather than from theoretical perspectives. There is no expectation in this module for you to become proficient in Prolog programming. As I already explained, the expectation is for you to be familiar with Prolog capabilities and understand how it can be used to encode knowledge in logic and for programming intelligent agents. What does it mean to represent and encode knowledge in logic?Let us assume there is a knowledge mod eland it includes a set of object sand relations between them. Just like in our knowledge model about the MSc data science program, we can make statements about those objects and relations. For example, we can state that if an academic query was handled by the module leader, then it was handled correctly. To represent and encode this statement in logic means that a logical inference engine can determine if the statement is true or false. This is, of course, under the assumptions that the model provides the required information, that it includes instances of queries by whom they will handle and if they will handle correctly. Perhaps we can determine the truthfulness of the statement by other means. Manually by inspecting all the queries, but this is not what we are after, or by employing machine learning, but for that, we need enough data and queries. Moreover, if a statement is complex, then it may be difficult to represent itin a form suitable for machine learning tools to work with. Inference engines can work with small data, with rules, and models. In principle, they can work with big data too, but it is when it is better to switch to tools that are designed for that. The key logical operators are negation, conjunction, disjunction, implication, and equivalence. It is best to demonstrate some sets A and B that are locate din the universe U or in the closed world. Note A, it is all the gray area outside of the set A, including what is covered by the set A and B, or yellow and blue area, A or B, the intersection between yellow and blue areas, the greenish area. Implication is easier to explain with the use of a truth table. If A is true and B is true, then A implies B true, and so on. You can see that any set can be inferred from F. If A is false, then it doesn't matter if B is true or false. Do not start from a false statement, any inference from that is meaningless. If you wish you can check truth tables for other logical operators. You also can check logical formulas and tools. In this topic, we're focusing on computational aspects of logic. We assume having a reason to determines the truthfulness of our statements instead of calculating them manually following logical formulas. Our job is to encode statements in logic so that the reasoner can process them. Other logical constructs we will need later on in this topi care quantifiers, universal quantifier or any and existential quantifiers, or some. If something is true for every instance X, then in logic we can use the universal quantifier to state that. Now we will turn our attention to how to encode knowledge in Prolog. Prolog is well suited for solving problems that involve object sand relations between them. Such fact as pete raises a query 001can be encoded in Prolog as, raises(pete, query_001). In terms of Prolog and first-order logic, this is a clause or a logical statement. In this case, a clause declares a fact but it also can be a complex statement with logical operators and quantifiers. Pete is an instance of the clause student. Starts with a low-case letter. Variables like student starts with a capital letter. Raises is a relation or predicate, starts with low-case letter. This clause can be communicated to the Prolog inference engine. The engine can be posed with questions. Since our program is very simple, so are the questions we can ask, raises(pete, query_001). Having found this as an asserted fact, Prolog engine will answer true. We can also ask raises(Student, query_001)where Student is a variable. That will be not only true, but also student is Pete. Based on our [?] knowledge model, the engine figured out how to substitute the variable and who raised the query. You can try it out in the web Prolog. Just insert your facts, questions, and hit the round button. The simple example shows that Prolog is remarkably good for retrieving information represented with facts. Of course, you could retrieve that information from a database, but trust me, it is so more fun to use intelligent tools like Prolog. There is even a special area of research that is called inductive databases that is dealing with that. The main power of Prolog comes with reasoning all rules and databases or conventional programming paradigms do not have such capabilities. Let us introduce a new relation or a predicate, classmates, via a group. Students are classmates if they are enrolled to the same module. In Prolog, if S1 is student one, S2 is student two, and M module. Classmate S1, S2. Implication, enrolls S1 out, enrolls S2 out. We also can add facts like Pete, Ann, and Joe are enrolled to the module ml_april2021. Now, if we hit the round button, Prolog will output all possible ways of classmates. There is an unattended answer though like a pair, Pete, Pete. We can amend our rule by stating that S1 and S2 should be different. We can use for that dif, an inbuilt predicate that checks if S1 is different from S2. Let us analyze the considered rule. The left part of the rule, classmate(S1, S2), is called head. The right part enrolls(S1, M)enrolls(S2, M) and dif (S1, S2) body. The logical operator is implication. If the condition in the body is true, then the logical consequence of itis the statement in the head. In Prolog, conjunction or logical ANDis represented by a comma. Both logical statements enroll(S1, M)and enrolls(S2, M) must be true for the body of the rule to be true. Disjunction logical OR prolog is represented by a semicolon. For example, successful if rich or famous. In this rule, either of statement has to be true. Questions that we pose to a Prolog agent are called goals. Clauses and questions can be complex logical statement sin first-order logic with logical operators and quantifiers. Now let's have a look at description logic versus first-order logic. The syntax of first-order logic is designed to make it easy to say things about objects. The syntax of DL is designed to make it easy to define and describe objects. You can recall it was easy to define in RDF that Ms C data science programis a postgraduate program and postgraduate programis an educational unit. We defined a whole tree-like structure of definitions of all object sin our model. First-order logic is not good with defining and reasoning over such structures. The main inference in description logic are subsumption, checking if one class is subclass of another classby using the definitions and classification, checking to what class an object belongs to. Such reasoning capabilities are not advanced, but they are important for ensuring the logical consistency of the knowledge model. As is it is often the case, these different approaches can be integrated. It is possible to define objects and relations in RDFand translate them to Prolog, and then define rules for those objects in prolog. In this mini-lecture, we'll look at the key logical constructs, operators and quantifiers, and how first-order logic statement scan be encoded in Prolog. Prolog is a powerful and well-supported language and probably the best for reasoning over knowledge."
DSM100-2022-OCT,"Topic 3: Knowledge, reasoning and planning",Lecture 6: Logical reasoning,https://learn.london.ac.uk/mod/page/view.php?id=96179&forceview=1,"[music]In this mini-lecture, we will consider different types of logical reasoning, deduction, abduction, and induction. Deduction, if we have a rule and a fact, then we can deduce a fact. The conclusion is guaranteed to be correct. Abduction, if we have a rule and a fact, then we can abduce a fact. In this case, the conclusion is not necessarily correct. It is only a plausible explanation. We need an extra step to come from or reject the truthfulness of the conclusion. In this example on the slide, we may need to catch Daffy and check if he's a swan or a duck. Why is it so different while the rule-fact notations look similar?We can look at deduction and abduction reasoninga little bit closer. Deduction [?] dates back to antiquity. It is one of the standard patterns of inference then can be plot to derive chains of conclusions that lead to the desired goal. The pattern is if P, then Q. This is a condition. P, assertion. Therefore, Q, a conclusion. Abductive reasoning was formulated as a form of logical inference by American philosopher, Charles Sanders Peirce in the beginning of the 19th Century. The pattern is if P, then Q, a condition. Q, observation. Therefore, P, possible explanation. People often do not understand this difference. For example, the famous deductive mindset of Sherlock Holmesis actually not deductive. Sherlock Holmes used abductive reasoning, looking for plausible explanations. In this topic so far, we saw examples of deductive reasoning. Subsumption, inference about class-subclass structure. Classification, inference about to what class an object belongs to, based on the definition of the object and classes. Prolog reasoning is also deductive, to identify the truthfulness of logical statements or goals. We will have a chance to discuss the usage and implementation of abductive reasoning in a different topic where we will consider the Robot Scientist AI System. Induction is generalization of facts. Like abduction and unlike deduction, it is not guaranteed to be correct. A verification step is required to establish the truth. Machine learning essentially is based on inductive reasoning. Many facts are processed and general conclusions, for example, predictions are made. Induction reflects our common sense. We observe that the sun rises every day and it is reasonable to assume it will rise every day. I do hope this generalization is correct. In this mini-lecture, we inspected different types of logical inference, deductive, abductive, and inductive reasoning. There are of course other types of reasoning, for example, qualitative reasoning, case-based reasoning, [?] reasoning, and probabilistic reasoning. All these forms of reasoning can be employed for intelligent agents to make decisions and to search for solutions."
DSM100-2022-OCT,"Topic 3: Knowledge, reasoning and planning",Lecture 7: Planning,https://learn.london.ac.uk/mod/page/view.php?id=96184&forceview=1,"[MUSIC]Planning. Devising a plan of actions to achieve one's goalis a critical part of AI. It is at the core of such applications as manufacturing and robotics. In this mini-lecture, we will consider classical planning and also planning in the real world where the situation can change unpredictably. We will consider planning in multi-agent environment and also scheduling. As we discussed previously, a problem-solving agent has to find a sequence of actions that result in a goal state. The focus in this lesson is on that sequence of actions, its representation, implementation, and communication to other agents, if necessary. The importance of planningis reflected in the fact that there are dedicated languages like PDDL, Planning Domain Definition Language and has numerous variants and also standards. For example, I was involved with the development of robotic task ontology that standardizes the representation and communication of typical tasks or problems in the terminology of problem-solving agents. There are also standards for defining actions and their properties. Classical planning deals with observable, deterministic situations. We can represent a planning task as a search problem. Given an initial state, a list of possible actions and their results, find a sequence of actions that leads to the goal state. We can use for planning all the algorithms we have previously considered for solving problems, breadth-first, depth-first, heuristic search, and so on. It is also possible to apply deductive reasoning and search reasoning agents as Prolog to find solutions. Thus, for planning task, we need to represent states including an initial and goal states and actions. Typically, actions are described following a formal schema that defines its parameters, preconditions, and its effects. For example, the schema for flying a plane may look like this, action, fly (p, from, to), precondition, at (p, from)and plane (p) airport (from) airport (to), effect, not at (p, from) and at (p, to). An action is applicable in a stateas if the preconditions are satisfied. One of the most famous planning examples is the blocks world. Its domain consists of a set of cube-shaped blocks sitting on a table. The blocks can be stacked but only one block can fit directly on top of another. A robot arm can pick up a block and move it to another position, either on the table or on top of another block. The arm can pick up only one block at a time, so it can't pick up a block that has another on it. The goal is to find a sequence of actions that achieves the goal state, for example, a on b, and b on c. Let us represent this example using Prolog notation. There are two situations in the blocks world that can be represented by predicates or relations, on (Block, Object)and clear (Object), where object is a block or a space on the table. Object (X)implies place (X) or block (X). The example shown on this slide has three blocks, a, b, c, and four places, 1, 2, 3, 4. We can represent the initial state as the list, clear (2), clear (4), clear (b), clear (c), on (a, 1), on (b, 3), on (c, a). Actions. For this simple example, there is only one action, move (Block, From, To). Preconditions can be defined by the procedure can(Action and Precondition). In Prolog, it will look like this. Can, action move with parameters (Block, From, To)and precondition as a list that block has to be clear, the place where it's moving clear, and what it's going to do. Effects can be defined as a list of relationships that action establishes. They become true after the action is executed. Classical planning problems are NP-hard or worse. Optimal planning is often not possible. However, sub-optimal planning is more achievable. As discussed before, finding good solutions requires good heuristics. A heuristic function can be determined by considering a relaxed problem that is easier to solve. One of approaches for relaxing the problem is decomposition, dividing a problem into parts, solving each part independently, and then combining the solutions. The independent assumption is not always justified, but in many complex situations, it is the best approach. Planning in the real world is different from classical planning. An agent may need to act in a partially observable, nondeterministic, or unknown environment. In classic planning, the planning is done on this so-called closed-world assumption. It assumes that if something is not explicitly defined, then it doesn't exist. Agents acting in the real world have to act on this open-world assumption. If something is not defined in their knowledge base, it means unknown. A partially observable and non-deterministic environment, agents have to produce alternative plans with conditional branches, they're called contingent planning. In dynamic environments, agents need to continuously monitor the situation and do replanning if necessary. There can be changes in the goal. Actions may have unexpected results, and the states can change. Classical planning representation focuses on what to do and in what order. It doesn't take into account how long and when. The real world poses resource constraints, for example, an airline has a limited number of staff. Planning problems that include temporal and resource constraints are known as scheduling problems. A typical approach is ""plan first, schedule later, ""separating these problems. Time and resources also require representation. Resources can be reusable like stock and consumable like chemicals. Planning problems are usually NP hard and scheduling problems taking into account all the constraints are even harder to solve. One of the approaches to reducing the complexity of planning and scheduling problem sis hierarchical task networks or HTN planning. Actions can be generalized or abstracted into several levels of abstraction to the level where it is easier to do high-level planning, for example, to drive from town A to town B. Then such an action can be refined to the operational level like to the left, right, and [?]Humans are good in high level planning and then the refinement and implementation of such plan scan be done automatically. Multi-agent planning is even more complex than planning for a single agent. An extra layer of planning and scheduling may require to coordinate agents' actions. Such planning may need to generate plans for joint actions or joint plans. Agents may share the same goal and the same representations or they may not. The agents may cooperate in achieving the same goal or they may compete. Communication between agents may require or may not. In this mini-lecture, we looked at planning and scheduling. We discussed the complexity of classical planning and approaches for finding good solutions, for example, using hierarchical planning. We briefly considered non-deterministic and partially observable environments where agents need to produce contingency plans. We also talked about planning for multi-agent environments."
DSM100-2022-OCT,"Topic 3: Knowledge, reasoning and planning",Lecture: Topic 3 summary,https://learn.london.ac.uk/mod/page/view.php?id=96188&forceview=1,"[music]In this topic, we discussed several truly advanced AI technologies, knowledge representation, ontology engineering, reasoning, planning, and scheduling. Research in these areas still has not reached its full potential. We are at the very beginning of an exciting journey of developing highly intelligent systems capable of cooperation in working on complex tasks in ever-changing real world. I recommend checking the further reading if you're interested in this particular topic. Of course, there is plenty of freedom content in our online library."
DSM100-2022-OCT,Topic 4: Quantifying uncertainty,Lecture: Introduction to Topic 4,https://learn.london.ac.uk/mod/page/view.php?id=96192&forceview=1,"Welcome to Topic 4, uncertainties. Intelligent agents may need to act in uncertain situations. They should be able to make decisions under uncertainty. In this topic, we will discuss what sources of uncertainty can be, and also how to quantify and represent uncertainties. In particular, we will be talking about probability theory, probabilistic reasoning, and Bayesian reasoning. We will inspect concrete examples of Bayesian networks, and we also will see how they can be implemented in [?]We also will look at various techniques used and decision support systems, physiologic, qualitative reasoning, and case-based reasoning. I wish you very best of luck with this topic."
DSM100-2022-OCT,Topic 4: Quantifying uncertainty,Lecture 1: Quantifying uncertainty,https://learn.london.ac.uk/mod/page/view.php?id=96195&forceview=1,"[music]In this mini lecture, we will loo kat how uncertainty in the environment can be handled by a rational agent. Uncertainty can be due to various factors:knowledge gaps, contradictory information from different sources, or just is the caustic nature of the environment. Intelligent agent should be able to make decisions under uncertainty. In this mini lecture, we will talkabout how to quantify and represent uncertainty. Intelligent agents may need to handle uncertaintybecausethe environment is partially observable, or the environment is nondeterministic, or both the environment partially observable and nondeterministic. For that, an agent need to represent the uncertain knowledge and also to reason over uncertain knowledge. An example of uncertain reasoning is diagnosing a dental patient's toothache. Diagnosing, be it a medical diagnosing or diagnosing of malfunctioning equipment almost always involves uncertainty. We can try using logic to write rules for dental diagnosing, like if toothache, then cavity. The problem is that this rule doesn't represent the knowledge and reasoning process well. Not all patients with toothache have cavities. To say can indicate other consequences like gum disease or abscess. To make the rule correct, we will have to add all possible consequences, and it may be difficult to do. If toothache, then cavity, or gum problem, or abscess, or something else or something else, we may even not know. From the other hand, cavity does not necessarily is manifested as toothache. There can be many other reasons for developing cavity. This example shows that it is hard to capture such uncertain knowledge as medical knowledge using pure propositional logic. Logic is not the best option for capturing uncertain knowledgebecausewe often can't enumerate all possible outcomes, symptoms, causes. Our knowledge is often limited, uncertain, and it has gaps. Or it can be difficult to do from practical considerations. There can be too many possibilities to list. Instead, we can provide a degree of belie fin such statement as if to say exam cavity. We can express a degree of belief as the probability of the event. To say it leads to cavity with probability 0. 8. Probability theory is a generally accepted way to represent such knowledge. Probability provides a way of summarizing the uncertainties that can be due various factors, like knowledge gaps, too many options to count, or contradictory information from different sources of knowledge. We might not know what afflicts a particular patient, but we believe that there is 80% chance or probability 0. 8that the patient who has a toothache has a cavity. Such a belief could be derived from statistical data about patient with toothache and cavities, if such data is available, or general dental knowledge, or from a combination of various sources of evidence and knowledge. How to make rational decisions in uncertain situations. Consider an example of an agent deciding how to catch a flight to get to the airport in time. Option 1, follow a short route, but it has a high chance of traffic jams and the possibility of missing the flight. Option 2, follow a short route, but start two hours in advance to minimize the possibility of missing the flight. Option 3, follow a long route with less traffic and a small chance of missing the flight. If it is absolutely imperative to be on that flight, Option 4 can be the best choice. Drive to the airport the night before and wait for the flight. In most situations, Option 4 is not desirable because a too long wait doesn't justifya decrease in the possibility of missing the flight. To enable the agent to make a decision, preferences between different outcomes have to be defined. It can be expressed as a utility function. Each output can be assigned with a utility value and a rational decision would be to maximize the utility value. An important consideration, though, is that the utility values are relative. Different agents or different users may have different preferences. For example, in a chess game, some players will be happy to have a draw, while many will be determined to win the game. Preferences expressed as utilities combined with probabilities in the general theory of rational decisions are called decision theory. An agent is rational if and only ifit chooses the actions that yields the highest expected utility, averaged over all the possible outcomes of the action. This is called the principle of the maximum expected utility(MEU). In this mini lecture [?]a great way of handling uncertaintyis to quantify it via probabilities. Probabilities can be used to expressa belief that the certain proposition is correct. There are, of course, other ways of handling uncertainty. However, probability theory isa well developed, well supported, and widely used mechanism for reasoning over uncertain knowledge."
DSM100-2022-OCT,Topic 4: Quantifying uncertainty,Lecture 2: Crash course in probability theory,https://learn.london.ac.uk/mod/page/view.php?id=96199&forceview=1,"[music]Probability series, well-established series with its notions, rules, and formulas. In this mini-lecture, we will take a bit of more AI and agent-oriented approach to probabilities. Probabilistic assertions are about possible worlds, and about how probable possible worlds are. The set of all possible worlds is called the sample set and denoted as a capital omega, Ω. The possible worlds, denoted as small case omega, ω, are mutually exclusive and exhaustive. Two possible worlds cannot be both the case. If one world is the case, then no other can be. For example, if we roll two dice, there are 36 possible worlds, one and one, one and two, one and three, and so a fully specified probability model assesses a numerical probability be with each possible world. We will consider discrete countable sets of worlds. It is interesting to consider the continuous case, but it is less relevant to AI and agents. Basic axioms of probability theory says that every possible world has a probability between zero and one and that the total probability of all possible worlds is one. [?] you set each dice fair and the throws do not interfere with the chance, then each world has a probability 1/36. Probabilistic assertions are usually not about particular worlds, but about sets of them. For example, we might be interested in cases where doubles are rolled, such sets are called events. Such events are described in AI as propositions, in a formal language. The probability associated with a proposition is defined to be the sum of the probabilities of the worlds in which it holds. For example, the probability of the event where doubles are rolled, is probability of doubles is probability of having (1, 1)plus probability of having to (2, 2) plus probability having (3, 3) and so on. There are six of them, and each has probability of 1/36so total probability is 1/6. Probability is like probability of having doubles or probability of the total being 11 are called unconditional or prior probabilities. This represents the degree of belief that proposition in the absence of any other information or evidence. If we have some evidence, for example, the first die is showing five, in this case, we are interested in the conditional or posterior probability of rolling doubles. Probability of doubles given die one is five. If a patient is going to then to [?] and the prior of cavity, maybe 0. 2, but if a patient goes to a dentist because of toothache, then the patientis more interested in the posterior probability. It can be higher probability cavity given to stake, maybe 0. 6. Posterior probabilities are defined as probability of A given B equals probability of A and B divided probability of B. In our case, the first line, tells that now only rolls where die one equal five are possible. The probability of this event is 1/6, the sum of six of 1/6 is 1/36. Among st them, we are interested in only one world, probability of having two fives, and this probability is 1/36. The definition of conditional probability can be rewritten in a different formas a product rule. For two events A and B to be true at the same time, B has to be true, and also A given B has to be true. Variables in probability theory are called random variables. Every random variable has a domain, the set of possible values it can take. The domain of the variable die one is one, two, three, four, five, six. The domain of the variable weather be sunny, rain, cloudy, snow. We can talk about probabilities of all possible values. Probability of weather being sunny is 0. 6. Obviously, we're not talking about Britain, probability of weather being rain, 0. 1, being cloud is 0. 29, and snow 0. 01. Only one world is possible at any time point, only one value for the weather so the total probability of all value sis always one because at least something of all those worlds would happen. This is a probability distribution of the random variable weather. We can consider probability distributions of several variables at the same time. That is a joint probability distribution. For example, probability of weather and toothache. The last notion, we will consider in this mini-lecture is independence. Two events, A and B are independent if probability of A given Bis equal probability of A. The fact that B has happened has no effect on A happening. For example, if A is sunny day, and B is oil price is up, the day is sunny independently of the oil price. Example of a dependent event is a sunny day and the sale of ice cream is up. A and B are not independent because sale of ice creamis up on sunny days. They are dependent but dependence does not necessarily mean causality, it may. In this case, a sunny day indeed can be the reason for sales of ice cream to go up but it's not always the case and one needs to be careful with causality. Events can be dependent, but not be a cause of one each other. In this mini-lecture, we briefly refreshed our knowledge about probabilities. Please refer to probability theory for the full list of formulas and tools."
DSM100-2022-OCT,Topic 4: Quantifying uncertainty,Lecture 3: Bayes rule,https://learn.london.ac.uk/mod/page/view.php?id=96204&forceview=1,"-We will start this mini-lecture from discussing what probability is. There are different approaches to the source and status of probability numbers. The most common approaches arefrequentist or objectivist approach. The position is that numbers can come only from experiments. If we test 100 people and find 20 of them have cavities then probability of having cavities is 0. 2. In this case, probabilities are determined by a statistical approach. The problem is that the approach is that we do not always have experimental result sand some of the experiments are not possible. For example, what is the probability that Boris Johnson will be reelected as the Prime Minister?It is impossible to run experiments to determine such a probability. Bayesian approach is to assign a number based on our knowledge or beliefs. This approach is subjective. Bayesian approach is based on Bayes theorem. It is also known as Bayes network, belief network, decision network. Bayesian model, probabilistic directed a cyclic graphical model, probabilistic graphical model. For networks the possible rules are assignments of values to variables. Bayesian network is a well-developed representation of uncertain knowledge. Bayes theorem provides a formal way of representing and calculating such probabilities. If we consider two events, A and B stands, the Bayes theorem states that probability of A given Bis equal probability of B given A multiplied on probability of A divided probability by B. Of course, probability of B should not be equal zero. The probability A given B is the likelihood of event Aoccurring given that B is true. Probability of B given A accordingly is the likelihood of the event Boccurring given set A is true. Probability of A and probability of B are the probabilities of observing A and B respectively. Prior probability A represents what is originally believed before observing B. Posterior probability P of A given Btakes this new information, new observation of B into account. Bayes theorem can be rewritten in hypothesis, evidence notation. This notation is useful for understanding the Bayesian reasoning. When some evidence E is becoming available, we can change our beliefs. If more evidence would become available, we can change our beliefs again and so on. Changing a belief about one event can lead to change sin beliefs about other related events. We can propagate such changes in our beliefs network following the Bayesian approach. The change probabilities can be pre-calculated in accordance with the Bayes theorem. Bayesian network or BN for short is a graphical structure consisting of nodes representing random variables or events, and directed arcs, links, connected nodes. Now we will consider an example Bayesian network. You can ask Alexa to send email to your boss. We can enhance Alexa by developing a decision support system that enables to ask Alexa to contact your boss and to decide about the best way of doing that. Maybe by email, phone, or maybe by Skype. We'll have to pass to such a decision support system, your knowledge about ways to connect to contact your boss. Something like that. I can contact my boss by email or Skype. I can get a response or not. If she is in office, and that happens only 50% of the time as she is often away on business trips, then she will respond in 50% of cases. The boss usually doesn't check Skype messages when in office but checks it 50% of the time when not in office. The boss usually checks email messages when in office and is unlikely to check it when not in office. The boss is very likely to respond if he has checked both Skype and email. The boss is likely to respond if has checked Skype or email. The boss doesn't respond if has not checked either Skype or email. We can represent this example as a simple Bayesian network these four nodes. The immediate question is how to construct a good Bayesian network?What entities to select as nodes. In the background knowledge, we have such entities as boss, me, email, Skype, office. The advice is to select an ordered list of random variables when causes received effects. How to deal with knowledge gaps. Are there other factors influencing the likelihood of response?How to deal with uncertainties. How to represent usually checks likely to respond. What links to use to connect the nodes. Contact, check, respond. These are all good questions, and it is precisely with what knowledge representation and knowledge modeling approaches are dealing with. It is possible to construct different Bayesian networks for the same problem. In this example, we selected in office, Skype, email, and response as the nodes. Probabilities are great to represent our knowledge about such events as the boss is in office in 50% of cases. Such knowledge is based on the previous experienceas is of objective nature. Probabilities are also great to represent uncertain knowledge like, boss usually does not check email messages when in office. We do not have any numerical statistical estimates but we can assign an arbitrary probability maybe 0. 1to represent the uncertain expression, usually does not check. In this way, we can assign a probability distribution for each node in Bayesian network. The node, in office represents the event or a random variable, the boss is in office. This variable has two possible values, yes or no. Boss [?] equal likelihood. Probability of boss is in office is 0. 5 and probability of bossis not in office is also 0. 5. The node Skype represents that random variable. The boss checks Skype messages. It has four possible values, they can be represented as conditional probabilities. Probability checks Skype given boss is in office is 0. 1. Probability checks Skype given boss is not an office is 0. 5. Probability does not check Skype given boss is in office is 0. 9, and probability doesn't check Skype given boss is not in office is 0. 5. Similarly, the node email represents a random variable, the boss checks email messages. It has four possible values that can be represented as conditional probabilities. Probability of checks emails given boss is in the office is 0. 8. Probability of checks emails given boss is not in office is 0. 2. Probability doesn't check email given boss is in office is 0. 2and probability doesn't check email given boss is not in office is 0. 8. The not response represent our knowledge since the boss is very likely to respond if has checked both Skype and email. The boss is likely to respond if she has checked Skype or email. The boss doesn't respond if she has not checked either Skype or email. The random variable responses has the following possible values. Probability response given checks both Skype and email is 0. 99. Probability of response given checks Skype and not email is 0. 9. Probability response checks email and not Skype is 0. 9and probability response doesn't check neither Skype nor email is 0. 01. If we need we can extend our Bayesian Network. In a company, the messages is usually sent to a management team and they are sent using a common email address or address of director or deputy or they can be sent on Skype account only that the director has access to it. If at least one of either director or deputy receives the check message, then it will usually be responded. We can easily expand the Bayesian Network by adding a new node, deputy. We also can express the likelihood of responding by assessment of probabilities. We used Bayesian Network to represent our knowledge about contacting the boss. Our Bayesian Network can be used to answer various questions about contacting the boss and Bayes theorem provides the means for calculating corresponding conditional probabilities. For example, if suppose has responded to your message, Skype, or email, what is the chance that she's in the office?We can calculate this probability using the Bayes theorem and other formulas, butit is tedious even for a small Bayesian Network. For a large Bayesian Network with hundreds or even 1000s of nodes, it's not practical. Instead, we can implement it in Bayesian. Bayesian Networks also can be implemented in many other languages not only Bayesian. For example in R or prolog. In this mini-lecture, we learned about Bayes theorem and how it can be used to represent background knowledgeas Bayesian Networks."
DSM100-2022-OCT,Topic 4: Quantifying uncertainty,Lecture 4: Bayesian networks,https://learn.london.ac.uk/mod/page/view.php?id=96208&forceview=1,"=[music]In the previous mini lecture, we considered an example Bayesian network. In that example, you could contact your boss by email or Skype message, and she would respond depending if she is in office or not, and the likelihood of you responding. In this mini lecture, we will see how to implement this network in Python. For that, we will be using pgmpy. Pgmpy is a pure Python implementation of Bayesian networks. You can visit their website, pgmpy. org, and you can download it, unless you already have it. It comes with Anaconda, for example, but it's very easy to install. Just use pip install pgmpy. I recommend visiting their website in any case, because they have really good tutorial, and many implemented Bayesian networks. Please study these examples, it will help you. For our example, first of all, we make sure that we have all what we need, like libraries. We will import NumPy, and we also will import TabularCPD. CPD is conditional probability distribution, and Bayesian model, and we will import [?]. First thing that we do is here, redefining our Bayesian model. It's just like an array, and we explicitly state what nodes are connected to what nodes, like In Office is connected to Skype, and In Office is connected to email node. Skype is connected to response node, and email is connected to response node, so this is our structure. Now, we have to define all TabularCPDfor each node, so all our probability distributions. The node In Office has two values, yes or no, so it's a variable card with cardinality 2. We just state these values, and we do the same for each node. Other nodes are more complicated, but the same logic applies. The node Skype is still cardinality 2, but values are of more complicated structure. Pay attention here because it's conditional probability. Evidence is In Office, so it's evidence card, so what we defined before. It's here now, evidence card. If it's In Office, probability of answering on Skype message is 1, if she's not in office, it's less than 0. 2. The same with the node response, or the same with the node email--It's very symmetric, and the response is even more complicated. It has 2 evidence now, Skype and email, and the values reflect it, so this is a more complicated structure, evidence card is 2. 2. It's a very simple Bayesian network, and you can imagine that you have to be careful with this structure when you import it for more complicated Bayesian networks. That is why it's quite important to check that the [?] is attached correctly. You attach it, and then it's really good to check get_cpdsand see if it can really process it. You can activate these nodes, and it's also good to check these independencies, so get_independencies, and you will see what depends, and also what response. If it's In Office, it depends Skype, email, and so on. To do some fun stuff, like probabilistic inference, you need this import variable elimination. We do that, and now we can calculate all sorts of probabilities. For example, here, probability of response if--Probability of response will be this, so what is probability of getting response? It's 0. 65. Then, what is probability of getting response on Skype message?Here are your probabilities and so on. You can ask more interesting questions like, what is probability of response if boss is in office?You can calculate this. If boss is in office, what is chance that she will respond?Then you can see probability of getting email response and Skype response if boss in office. Some probabilities are probably most obvious, and some are less obvious. Let's see something more interesting. Here, they're just the probability--Let's see--If you got response by Skype, what's the chance that the boss is in office?It calculates--Behind, there are all these laws of probabilities, and of course, [?] starting from the very top node, it propagates to other nodes. Whatever you can think about any conditional probabilities that involve these four modes, you can express, and it will calculate. What can you do?You can extend it, so here is an sample an extension. In company, messages usually are sent to management team. You can use a common email address, and it can go to director or deputy. You also can send a message to Skype account which only director has access to. If at least one of the directors or deputies receive sand checks the message, it will usually be responded. We also can [?], like director is in office, there's probability of 50%, director is at home, there's probability of 20%, director is in business trip, there's probability of 30%, deputy is at office, there's a probability of 70%. Director usually doesn't check Skype messages when in office, 10% checks. Director sometimes checks Skype messages when at home, about 30%. Director very unlikely to check Skype messages in business trip, 1%. When one of the director or deputy are in office, the email will be checked with high probability, 90%. When both director and deputy are in office, the email will be checked with a very high probability, 95%. When none of the director or the deputy are in office, the email will be checked with a low probability, 5%. How to capture it-- We can extend our Bayesian network. Basically, where [?] node, we have director and deputy, so a structure change. Then here, we have a different Bayesian model, so very similar, but we added this director and deputy thing. Again, we have structure, and then we define this conditional probability distribution for each node. For director, we now have three cardinalities. Three, because it can be in office, on business trip, or at home. We have deputy, there are two, in office and not office, and so on, so each node is defined. Then again, we can check if we understand it so correctly, and we then check in independencies. We got more nodes, you see already how complexity grows. Again, we can then do variable elimination, and then do reasoning. We can ask all sorts of questions. What is the chance of getting a response?Quite high, 0. 68. Thenwe have here, what is probability of getting a responseon Skype message?Not very high. All these probabilities can be used when decision support system makes decision, how to contact by Skype or email. If you can imagine a system that is connected to some other systems, so maybe there is some calendar where it's marked business trip. If there is this additional evidence that the boss is on business trips, it all can be recalculated in this Bayesian network, and you can have more accurate estimates. Here, probability email responseis very high, 0. 95. The probability of getting a response from deputy also, 0. 87. You can calculate it all again manually, but it's very complicated. [?] formulas, it's all hardly encode din this implementation. This is how to implement the Bayesian networks. You can ask all sorts of questions. What is probability of getting a response?What is probability of getting a response from a deputy?What is probability of getting a response by email if deputy not in office?Whatever you can see in combinations, you can ask these questions, and you will get numerical estimates of the probabilities. Once again, I recommend visiting pgmpy. org to see other examples of Bayesian networks, and I hope you can extend these examples further just to practice, or you can create your own Bayesian network."
DSM100-2022-OCT,Topic 4: Quantifying uncertainty,Lecture 5: Decision making,https://learn.london.ac.uk/mod/page/view.php?id=96212&forceview=1,"Decision CT deals with choosing among actions based on desirability of their outcomes. In this mini-lecture, we will consider non-deterministic partially observable environments. In such an environment, an agent may not know the current state, but it still should know what actions are available. We will consider Results(a), where A is an action as a random variable whose failures are the possible outcome states. The probability of outcome Sgiven evidence observations Ecan be expressed as a conditional probability. Probability of Result(a)=s'given a and ewhere the eon the right-hand side of the condition involved stands for the event that the action a is executed. The agent's preferences are captured by the utility functions U, which assign a single number to express a disability of a state. The expected utility of an action, given the evidence, is just the average utility value of the outcomes weighted by the probability that the outcome occurs. The principle of Maximum Expected Utility, MEU, says that a rational agent should choose an action that maximizes the agent's expected utility. The MEU principle can be seen as defining all of AI. All intelligent agent has to dois to calculate various quantities to maximize the utility over its actions. It formalizes the notion that an agent should do the right things. However, it is difficult to quantify and estimate the utility values. Nevertheless, MEU enables a rational approach to designing AI systems. It can be difficult to define utility values. To start with, an agent should be able to express his preferences like A is better than Bor there is no difference in preferences between A and B. In this case, the space of preferences is an ordered space. Then, what does A and B can be?They can be many things. For example, it can be certain amount of money, but, often, they can be uncertain. For example, if an airplane passenger is offered the choice of meals, a chicken or pasta dish. There is a lot of uncertainty about such a choice. A pasta dish can be fantastic or overcooked. A chicken dish may be over spicy, and sooften making the choice is a lottery with possible outcomes. Then probabilities of those outcomes have to be specified. If a rational agent knows all possible choices and all possible outcome sand has assigned to them probabilities, then it can compute and maximize expected utility. This is all rather complicated and requires estimates of many probabilities. There also can be numerous constraint son the preferences that a rational agent should have. On the other hand, we humans do not compute multiple probabilities, but we are still able to make decisions. Can a rational agent imitate the decision process of a human?Humans not always act rationally and their decision process can be affected by many factors. For example, the certainty effect. Let us go and see this Allais paradox designed by Maurice Allais in 1950s. People are given the choice between lotteries A and Band then between C and D, which has the following prices. A, 80% chance of $4, 000. B, 100% chance of $3, 000. C, 20% chance of $4, 000, and D, 25% chance of $3, 000. Most people consistently prefer B over A, and D over C. To maximize the expected utility, the choice of A is better than Band D is better than C. Thus, there is no utility function that is consistent with the choices by humans. Thus, their choices appear to be irrational. However, people may take other considerations into account. They tend to choose a sure thing with 100% chance. This saves them calculation sand also reduces the emotional burden, a chance to be disappointed in their choice afterwards. These effects are known as the certainty effect and that regret effect. Another interesting observation is ambiguity aversion. In the Ells berg paradox, the processes are fixed but the probabilities are under constraint. The payoff depends on the color of a ball chosen from an urn. The urn contains one-third of red ball sand two-thirds either black or yellow balls, but it is not known how many black or yellow balls. Again, people are given the choice between lotteries A and Band then between C and D. A, $100 for a red ball. B, $100 for a black ball. C, $100 for a red or yellow ball. D, $100 for a black or yellow ball. Most people prefer A over B and D over C. If one thinks there are more red balls than black ones, then the choice should be A and C. If one thinks there are less red balls than black ones, then the choice should be B and D. There is no rational strategy that favors A and D. This shows that most people make choose irrationally. They are avoidant in most ambiguous situations. They know one-third are red and two-thirds are black and yellow. That is what they choose. Thus, to imitate human decision can be difficult, but also it may be undesirable. Another difficulty is in assigning sensible values. For example, if they assign monetary values, they may not necessarily reflects rational strategy. Example, suppose you triumphed over the other competitors in television game show. The host now offers you a choice. Either you can take $1 million price or you can gamble it on a flip of a coin. If the coin comes up heads, you end up with nothing, but if it comes up tails, you get 2. 5 million. The expected utility value is $1. 250 million, which is more than the original and the rational choice would appear to be to gamble. However, it is generally agreed, unless you are a very rich person. The thrill added value of the second millionis considerably less than of the first million for the quality of your life and also for your financial status. Therefore, it is better not to risk one million. An example of another difficulty in assigning values to utilities is assigning value to human life. Many decisions include consideration of the value of human life, like a decision about what treatment to selector a decision about possibility of a car accident. However, there is no any agreed approach to making such valuations. Moreover, researchers all show that such evaluation can be varied different from self-evaluations. One more consideration to take into account when designing a rational agentis that the expected utility functions tend to be over-optimistic. Mr. [?] said, ""The more choices are available, the more bias towards over-optimistic expectations. This then results in what is called post-decision disappointment. ""To appreciate the complexity of the decision-making process, it is also important to take into account that utility values can change over time. Such rational agents should make rational decisions by maximizing the expected utility value at each step. However, in nondeterministic, partially observable environments, it is extremely hardto account for all possible outcome sand assign sensible probabilities and utility values to them. Decision Support Systems is a hot research area, and the steady progress has been made over recent years. A Decision Support System is an information system, or it can be subsystems that enhances the ability to make decisions by a person, software agent, a robot, or a group of agents. An example Decision Support System is a system supporting diagnosis of patients. Decision Support System requires specification of a task. For example, diagnosis with an error rate below 5%. Objectives, for example, to minimize suffering or maybe to maximize survival in five years. Constraints, for example, financial constraints or have an explainable decisions. Decision Support System benefit from different approaches and techniques. Fuzzy logic has been studied since the 1920sand recently became popular for modeling decision-making processes based on imprecise non-numerical information. In Fuzzy logic, an element may belong to a set with a certain degree. Typically, represented as a real number from the interval 0, 1. In non-fuzzy conventional logics like Boolean logic, there are no degrees, only true/false and 0/1. Therefore, fuzzy logic models can utilize data and knowledge that lacks certainty. Case-based reasoningis based on exploiting old experiences to understand and solve new problems. A reasoner identifies previous situations that are similar to the current one and reuses the original solutions to solve the new problem. It is much like what lawyers do. Case-based reasoning may adopt all solutions to meet new demands or to explain or critique new situations. Qualitative reasoning or QR is using qualitative rather than quantitative information. Numerical values of quantities are avoided and qualitative values are used instead. For example, add more water, reduce hit. QR is popular for human-like computing application sand it is motivated by human cognition. QR is used in such applicationas monitoring, diagnosis, explanation generation, and tutoring. One of the most popular techniques for Decision Support Systemis probabilistic reasoning. This is what we are considering in this module. Probabilistic logic is employed for reasoning. It is particularly suitable for reasoning under uncertainties and with partial beliefs. Probabilistic logic is suitable for both narrow and general AI. This isthe most popular and widely spread type of probabilistic reasoningis based on reasoning. In this mini-lecture, we discuss the difficulties in making rational decisions under uncertainty. Rational agent has to account for all possible outcomes of all possible actions, while it is difficult to assign sensible utility values to all possible outcomes. Nevertheless, there is a steady progress in this area of decision-making. Nowadays, intelligent agents are capable of making better and better decisions. We talked about several popular techniques for Decision Support Systems, fuzzy logic, case-based reasoning, qualitative reasoning, and, of course, the most popular approach, probabilistic reasoning."
DSM100-2022-OCT,Topic 4: Quantifying uncertainty,Lecture: Topic 4 summary,https://learn.london.ac.uk/mod/page/view.php?id=96216&forceview=1,"In this topic, we've focused on handling uncertainty and making decisions in uncertain situations. We revisited the probability theory and we talked about Bayesian reasoning as the most acceptable way of representing and reasoning under uncertain knowledge. We inspected several examples of Bayesian networks, and we saw how they are implemented in Python. Bayesian networks are well-established and well-supported techniques. Bayesian networks can be implemented in many languages, not only in Python. After the completion of this topic, you should be able to design and implement your own Bayesian network."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Introduction to Case study 1 – Evolving artificial creatures,https://learn.london.ac.uk/mod/page/view.php?id=96220&forceview=1,"[music]Welcome to the evolving creatures case study. In this video, I'm just going to give you an intro and tell you a bit of my inspiration for wanting to teach you all about this. Then I'm going to give you an overview of the things we're going to be looking atover the next five weeks of material. Yes, first of all, what are we doing here?We're learning about genetic algorithms. That's the headline. We're going to be building a genetic algorithmin this part of the material. We're going to be looking-- Genetic algorithms are a branch of bio-inspired computing. Bio-inspired computing is a branch of artificial intelligence. This is really a key topic in artificial intelligence, the use of these evolutionary algorithms to design things and to solve problems. Specifically, we're going to be using genetic algorithms to design robotic formsre-implementing a famous research paper. Now, I wish I could show you a clip of the original videos of this 1994 research paper, but due to copyright reasons, I'm not able to. If you go on to YouTube, you can find Karl Sim's Creatures pretty easily. Suffice to say that this was a really seminal piece of work, where Karl Sims was able to actually use evolutionary computing and genetic algorithms to design morphologies of creature sin a virtual world. Which you could then run and see moving around. The idea is they would evolve to be able to move around in the world in different ways, and to follow lights and things like that. My personal interest in this field. Why am I interested in evolutionary computing?Why am I trying to tell you all about it?What's my path for getting in here?Yes, basically, I started out doing biological science sin my undergraduate studies. This is when I found out all about evolution theory, and also genetics, both of which I studied. Evolution theory was absolutely fascinating for me for something. There was something about the idea of these really complicated search spaces. The idea that nature was searching through these amazing abstract spaces for solutions to problems. What's the best way to design a beak for a parrot?What's the best way to design a wing, and so on?All these things that nature was solving, using its own processes were absolutely fascinating and also combined with genetics. The idea that there's a core substrate of all this stuff was the DNA and the fact that DNA was changing over time. Reading things like Dawkins' Selfish Gene theory, and beyond that, learning about how genes can regulate each other. Really, the genetic system of an organism being this amazing computational system, which is self-regulating and is able to control the organism over many years to achieve its goals. These two things were really inspirational at the undergraduate level. Following that, I actually went and studied evolutionary computationat master's level. I found out there was a degree course where I could combine my fascination with computers with also my new fascination with evolutionary theory and genetics, and I've not been doing much programming for many years. I had to get my C Programming book out and learn how to program in C and Java, and all the different languages back in the late 1990s. I was able to learn all about evolutionary computation and implement my own genetic algorithms. At the time, I was quite interested in music as well, still am. I was actually using these to solve problems which I was encountering in my musical practice. The problem I was solving was how to design really interesting sounds, because that's one of the key things for a producer of music. How do you make a really good sound?I was using evolutionary computation to do that. Eventually, as you'll see here, I actually ended up studying that for my PhD. I actually did a PhD, where I was looking at using genetic algorithm sand other things like neural networks, and so on, to design sounds, to listen to sounds, and to understand how sounds are constructed using different types of synthesizers. Okay, that's my personal interest in all of this. Now, what we're going to do now is just quickly go through the various things we're going to study, but before we do that, I'm going to show you this cool animation which is going to give you some of the flavor of the things we're going to be building and seeing in the next few weeks. I've got it on my system over here. I just have to run this command here and then we will seethis really interesting system being built. What we're watching here is a bunch of random creatures being designed by the machine and eventually dropped onto the floor. Not only that, we've got I think, 25 creatures in here, and these are just random ones. Now, if we zoom in a bit, we can see that actually, they're all moving. They all have motors in them and they're moving around. Then what we're going to be doing over the next few week sis building a system which can basically design these things for us, such that it finds the designs which are the best of moving. It's going to look at all these designs and learn how to build them and to optimize them over time, so that you can find the ones that are best at moving, which is very similar to what Karl Sims was doing in his original paper. We're going to follow along some of the technique she was using in that paper. You can see they're all moving in interesting ways. How are we going to go about doing it?Well, here is the basic plan. We're going to have five weeks of material. This is organized into five chunks, if you like. The first chunk, week one is where we're going to find out about the basics of bio-inspired computing. We're going to find out what it is, and where it fits into the field of artificial intelligence, and what their latest work isand what the history of it is a bit as well. Then we're going to have an evolution theory crash course, because I'm not going to assume that you know what evolution theory isor how it works. I'm going to give you a very focused description of it, which is applicable to what we're doing. Then we're going to have an introduction to evolving creatures. I'm going to talk about the Karl Sims paper, and how that works. Also, I'll talk about genetic algorithms, and why those work and give you some of the insight into why they are powerful algorithms. Okay, that's the background that built the foundations of what we're going to learn. In week two, we're going to be looking at actually building a system. We're going to start programming in week two. The videos are going to be showing you in great detail exactly how to build all of these algorithms, and so on. We're going to be finding out the substrates of our evolution, which is these creatures encoded into URDF files. Now a URDF is an XML file format that's used in a lot of robotics work. Originally, it came from the ROS, Robot Operating System project, but it's now available in a range of robot simulation systems. It's becoming quite commonly used. There's lots of ready-made robot models that you can see, but we're going to be evolving new robot models in this format. We're going to be learning all about joints, links, and motors, which allow us to design these things and make them move. Okay, week two, we're going to get-- Sorry, week three, we're going to be getting into some more intense programming. There's going to be quite a lot of videos in this material, because I like to show you exactly how I'm going to program this whole thing from scratch. I've built a complete re implementation of most of the Karl Sims' work. You're going to see how that is put together over these videos. We get really into it in week three, when we start building our genetic encoding, which takes a list of random number sand basically turns it into one of these things, basically, a moving structured creature form. Okay, week four, we're going to be continuing with our fairly intensive programming. We're going to build out the fitness function and population model, where the fitness function is actually used to evaluate all the creatures. Actually, here's a bit of a head- a clue here. The fitness function is really simple. It's just measuring how far they move. That's the most basic form, but you're going to be encouraged to goand investigate that more yourself. Yes, and the population model and how that's done in genetic algorithm research, we'll find out all about that. Eventually, at the end of week four, we're going to have our complete genetic algorithm which will allow us to evolve and optimize those robot forms. Week five is where we're going to look at the state of the art, and the ethics, and other aspects of this evolving morphology research. We're going to find out what happened after the Sims. The Sims paper was '94. What did people do next?After this seminal work, what happened next? We'll find out. We'll follow people like Jeff Koons work, who now works at Uber, I believe. Yes, some really interesting work that he did with soft robots and things like that. We'll be looking at some of that. We'll also be looking at the state of the art in other areas of evolving morphology. Not just moving creatures, but also evolving neural network architectures. We'll be seeing that is something which is a very active area of research, where people are trying to create more optimized neural network designs than humans can do themselves. You can see that's really cutting-edge work. Okay, that's the general plan. At the end, we'll have a fully functioning genetic algorithm, which can evolve creatures and which you can run machines. Throughout the course, as I say, you're going to be seeing loads of intense coding videos. If you find the coding videos are too long, because I deliberately made them long so you can see all the nasty details, but if you find it too long, it's okay. You can skip them or play them at a faster speed. You can basically jump ahead to the lab worksheets where I repeat the process. I lay it out in a text form, so you can copy-paste the code into your code editor and build up the thing that you saw me building in the videos and have your own version of it. I'll also be supplying the source code for everything that you see, and you'll be able to modify that and do what you want with it. Okay, that's really all I wanted to say. I just say welcome to my creatures case study, exploration of evolutionary computing, which I think is a fascinating area. I hope you really enjoy building out this genetic algorithm system. I hope that you find some really cool creatures which move in very interesting ways, because it's a very creative type of algorithm."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Introduction to Lesson 1,https://learn.london.ac.uk/mod/page/view.php?id=96224&forceview=1,"[music]-Welcome to Week 1 of the [?] case study. Let's jump onto the learning objectives for this week. Essentially, at the end of this week, you're going to be able to explain the key features of a genetic algorithm and explain how they can be used to address a given problem. Secondly, we're going to be able to compare and contrast examples of artificial life systems. Thirdly, we're going to be able to develop an argument about the potential of evolutionary and bio-inspired computing. That's the basics but just checking this diagram up here, this is something we're going to encounter late rand it's a key feature of the genetic algorithm, which is the idea of variation. You can see you've got different shapes in there, and different selective power for the different shapes. It's a very simple diagram, but it summarizesa lot of the features of the genetic algorithm that we're going to see in the next few videos. Good luck this week."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Bioinspired computing,https://learn.london.ac.uk/mod/page/view.php?id=96225&forceview=1,"[music]-In this video, I'm going to be talking about bio-inspired computing. In summary, we're going to start by giving you a brief definition of bio-inspired computing. I'm probably going to be reflecting a little bit on some of my own experience sin this area as we go. Then we're going to ask the question, is it artificial intelligence?Then we're going to look from a historical perspective at some of the key areas that have developed in relation to bio-inspired computing. Let's start off with our definition. This is my attempt to summing up what it's all about. What I've got here is looking at the algorithm sand functions in the natural world and thinking there must be something we can use computationally here. Just reflecting on my own experience, I did a genetic zoology undergraduate degree and it was all great. Dissecting things and drawing pictures of animals. Frankly, I didn't find it that interesting. Then I hit evolution theory and I hit genetic sand the DNA self-controlling system. My mind was blown. It was really brilliant. I was really hooked from that point on. For me as a person who started programming at a very young age, then stopped, and then got back into it later, evolution and genetics and bio systems were the bridge between the two areas, if you like. I found that looking at evolution theoryis this amazing system which could develop all these fascinating form sand had all these weird and strange characteristics inside it. Then again with genetics, it's like a crazy computer, your genome, right? It's like a self-modulating, self-replicating crazy machine. What inspires people when they're doing bio-inspired computingis they look at these amazing complex system sand they think, ""Surely some of these algorithm scan be taken out and we can take the essence of them. We can use them to do computational things such as optimization or automatic design and so on. Then we're going to look at a few things which can be done with bio-inspired computing in a sec. The next question, is bio-inspired computing equivalent to AI? Is it AI?In this review paper, which is 30 pages long in the Swarm and Evolutionary Computation journal. I recommend reading this, but scanning through it, just to give you an insight of depth of the field and what's going on. It's from 2019 so it's fairly recent, but they gave a definition of bio-inspired computing or a description of it saying it's emerged as one of the most studied branches of artificial intelligence during the last decades. Really if you haven't heard of it yet, this is key, really a large area of AI. If you trust the experts, then yes, it's AI. What we're going to do now is do a brief historical view of various bio-inspired technologies and systems that were developed over the years. They don't sit neatly into each decade. They all spread through and mix with each other, but I just want to mention some of them and say what they're about because it shows how it gradually grew as a research field, I suppose. In the 1950s up there, you can see we've got cybernetics. Then we moved into connection ism and then we've got genetic algorithms, artificial life, and then onwards and upwards after that. Let's go through those and look at them in a bit more detail. First of all, cybernetics. Cybernetics is a word which was first coined for this area by Norbert Wiener. He wrote a book in 1948 called cybernetics. The word comes from the Greek word for steersman the person that steers the boat. Let's just read that out. It says, ""The science of control and communication in the animal and the machine. "" That's what cybernetics is. It's about control and communication. Interestingly, it's not so much about form. It's not really about studying what's out there in nature, what it looks like, what color it is, how many legs it's got. It's much more about how it works, how it functions, how it controls itself, and how the different entities might communicate with each other or different bits of an entity might effectively communicate with each other. Then I also wanted to mention Ross Ashby. I've got his book Introduction to Cybernetics up on the shelf there. It's got a really cool font but I can't show you it for copyright reasons. Sorry, but go and look it up, Introduction to Cybernetics by Ashby and you'll see it. Basically, he focuses a bit more, again, he was a contemporary of Wiener. Wiener was based at MIT, Ashby as a British computer scientist or electromechanical engineering person, I would say, and a mathematician. Drilling down a bit more, so coordination, regulation, and control. Building on that science of control and communication. It's all about not about the form of things, but the way that they interact. Just worth saying, this is an interesting essay, which you can go and dig out online about the British cybernetician sand what they're up to. Ashby, Beer and Pask, really interesting stuff. What was particularly interesting about cybernetics maybe is the fact that it was very cross-disciplinary. It turned into something that was quite cross-disciplinary because people started applying the concepts to all kinds of things like buildings to corporations and so on. Can you look at how some sort of natural system works like an ant colony or something, can you apply some of those concepts of communication coordination to a corporation and make that work more efficiently, for example?It's worth having a read of that to see the wider range of cybernetics. Connection ism, this is the big one. [chuckles] This is neural networks. I've got a quote here from Rosenblatt who's credited with having come up with the perceptron or invented the perceptronand the perceptron was exactly a minimally constrained nerve net. Nerve net, not a neural net, but a nerve net consisting of logically simplified neural elements, which has been shown to be capable of learning, to discriminate and to recognize perceptual patterns. Rosenblatt in 1960 there. This is connection ism, this is the birth of neural networks, which obviously led to deep neural networks, which is what everyone is crazy about now. There's no doubt that neural networks are bio-inspired, they're basically looking at the nervous systems and the brains of animal sand people and thinking, ""Mmh, that seems to be doing something pretty interesting and useful. Maybe we can simplify it, extract the essence, which is this idea of a network of things which are connected together, and let signals flow through with different activation patterns. Then maybe it can learn to do things too, and indeed it can. ""Moving on. Whoops. Artificial life. Artificial life, before we talk about the more '80sand '90s artificial life, I just wanted to mention Barricelli's work who is working on simulating dynamic system son IBM valve computers. I cannot imagine how he did this in the 1950s, working on punch cards or whatever, to program these systems, to run these really computationally expensive simulations. There must have been some amazing programming going on there. That's all I'm going to say. That's worth reading about as well. Another interesting, if you're interested in where this stuff comes from and the history of it, that's definitely worth reading about. Jumping forward into the '80s and '90s. Chris Langton in the late '80s set up the artificial life conference and this field grew out from there. It really bloomed and lots of people were looking at this stuff. The take-home is this quote, which I really like, which is ""life as it could be"" it's life as it could be. It's not about simulating life as realistically as possible so much. It's more about thinking about, ""Okay, if we've got some basic evolution, a system that's got some basic evolution like behavior, and we can have entities in there growing and whatever it's artificial, what kind of things can we grow?What kind of life is possible in a simulation?""It's really fascinating. A lot of the early work was looking, and to this day, in fact, doing things like trying to create a simple version of the primordial soup where RNA evolved or emerged from these massive chemical son the earth and all that. It's looking at early origins of life and simulating that, but also looking at other new types of life, artificial life that run son silicon and how that could work. That's really an interesting field connected with complexity theory and chaos theory as well. What about genetic algorithms?Well, we're going to be looking at loads of stuff about genetic algorithm sin the next few videos. I won't go into much detail, but essentially genetic algorithms are something which allows you to evolve solutions to problems inside a computer in essence. It's all about defining your problem in a way that the algorithm can use and then defining a correct way to evaluate different version sand iterate on those solutions to make them better and better over time. That's what we'll see in the next few videos. In summary, what we've just been talking about is bio-inspired computing. I've given you a definition. We've considered whether it's artificial intelligence or not. Then we've looked at some examples of bio-inspired computing going right the way back to the '50s and cybernetic sand moving through connection ism and neural network sand artificial life and genetic algorithms. This video was all about bio-inspired computing."
DSM100-2022-OCT,Topics 5&6: Creatures,"Lecture: Evolution theory crash course: evolution is optimisation, terms (fitness, genotype, phenotype,morphology etc.)",https://learn.london.ac.uk/mod/page/view.php?id=96228&forceview=1,"[music]In this video, I'm going to attempt to give youan evolution theory crash course. In summary, we're going to start by looking at the context. In other words, why am I giving you an evolution theory crash course, and then we're going to talk about the selection of breeding cycle, genotypes, phenotypes, and how selection works. Let's jump into it and get started. First of all, we're going to look at a particular field of biologically inspired computing which is the genetic algorithmin later videos. What I want to do here is show you the evolution theory that inspires these algorithms. I'm going to jump over to my drawing program here and attempt to draw you a illustration of the selection and breeding cycle. It all starts out with three stages really. Let's just draw a circle for each of those stages. You've seen a sneak preview of this on my slide. I'm going to draw it because maybe it's more interesting to see it being built up over time. We start off and we have a population. Population which has variation and heredity. Let's think about variation. Variation means there are different things in it. Different versions of the creature. I'm going to come up with this six-legged creature that's going to have slight variations on it. That's a long one with a small headand that one's a shorter, bigger-headed one. Maybe we could have a really tiny one. We've got our variation. We've got different versions of the funny six-legged creature, and they're all the same species if you like, but they're just slightly different versions of it. The key thing about this variation is its inheritable. There's heredity. It means that if these creatures are to reproduce, they will come up with creatures which look a bit similar to them but there may be variations, especially if they're crossbreeding. If you bread the one with the big head with the one with the small head, maybe you'd get a medium-sized head or whatever. The idea is that they can pass on their characteristics from one generation to the next. The next thing is that in order to decide who is going to passon their characteristics we say, 'Well, they have some test. "" This one. What we're going to say is this one has to climb a hill in order to get to some food. There's the food at the end there. It looks like a speaker at a rave but that's okay. There's food. He's trying to get to the food. If he gets the food, then that's great. That means he can then go on to Stage 3 which we're going to talk about in a minute. We're going to test each of them. Maybe we test the big head one as well. He has to climb up the hill or it has to and it has a problem. Maybe its head's too big so it can't quite climb up the hill. It's going to get stuck and it won't get the fluid or it doesn't get as much food because the other one got there firstand ate most of it, whatever it is. The idea is depending on how successful they are in life which might be just climbing a hill, then dictates how successful they areat passing their genes onto the next generation. What we can say in the third stage then, so we've got Stage 1, 2 tests. Variation with heredity, test, and then selection. At this point we do selection. Selection -- actually that would be breeding. Let's do that. We'll call that selection because that's where we're selecting. Then finally, in the breeding stage, we're going to say, ""Well, the successful ones get to propagate and there's basically more of them. ""That's going to be the little one because he managed to run up the hill, get the food, and maybe the really long one was successful as well. We'll put one of those in there. Maybe it changed a little bit because it combined with one of the big head ones, and so we ended up with some long arms in that one as well. We ended up with a new population. A new population comes through with variation again base don that selective pressure. That's the essence of evolution through natural selection. The idea is over time that the characteristics vary, and the entities might undergo changes, mutations. If there's radiation or just general mistake sin the DNA copying or some bacterial infusion, whatever it is, can change the genetic material of the creatures so that over time new variation come sand is tested and the population as a whole changes over time. That's the basic idea. It's very simplified but it gives you the idea. Now, moving on. The genotype encodes the range of characteristics of the individual. What do I mean by that?Well, essentially we talk about nature and nurture. Nature is what it comes with. Nature is what's in the DNA. It's like this is the range of possible things you might be able to do. Let's say you had two identical animals that had the same DNA, but they each experience different things, or maybe one doesn't have much access to food for some reason. Maybe it ends up being lost on the wrong side of the rive rand then the other one has access to loads of food. Yes, at the end of their life, they're going to be different. They're still going to have the same DNA but they're going to be different. That's what we mean by nature and nurture. Nurture is the process that it goes through in its life and nature is what it starts out with which defines the bounds of what it might be able to do. Again, this is not a super-precise definition of evolution, but it's giving the idea that there's these two things, nature and nurture. The genotype is the DNA strands basically. Now, there's another thing called the phenotype which is how the individual ends up in the world. The example we saw earlier of the things that grew up in the different side of the river with different access to food, one might get really tall and large and the other one might be smaller because they didn't have as much food, and that's the phenotype. The phenotype is what we can see, how the genotype manifests in the world, if you'd like, as a result of the nurture. In terms of how DNA is expressed into a phenotype, let's just go back to my drawing over here and I'm going to try and very briefly explain how DNA works. DNA is a four-letter sequence. It has four different letters: A, G, T, and C, and those are the letters of the DNA alphabet. It's that simple. Then the DNA that you will find in a cell will be made up of different sequences of these letters, and so that's what you have. That'd be your DNA. Then what happens next is we need to get this DNA and do something with it. As the organism is growing and their genes are being expressed through this amazing machinery that we all have inside us, what happen sis basically the simplified way the DNA is expressed is it gets converted into proteins and those proteins then help build the anima land control what it does. Going back to cybernetics, it's about control, and coordination, and communication. DNA is the essence of this. DNA and how it gets expressed into proteins. This is cybernetics right here. How does it work?You get three of these and they'll be mapped to one amino acid. Proteins are made out of strings of amino acid folded into three-dimensional structures. How does that three go into amino acid?Well, there's a bank of 21 amino acids. At least there were last time I remember looking, [chuckles]so you've got 21 different amino acids which have all different structures or whatever. They've got all different bits sticking out. They're basically chemicals. What we do is we say, ""Well, AAG maps on to number 2in the set of amino acids, then maybe TCA maps onto number 5and then a 6 maybe, and we've got a 1 and a 20, whatever it is. '' What we end up with is a string of amino acids, which as I said, then that's basically protein but it folds into a weird three-dimensional structure of some sort where all those are bonded together. But because of the electrostatic properties and other things, they fold and some bits of the string attract each other and it falls into this incredibly complicated 3D structure. I should say that in 2021, Deep Mind, the company published the human proteome. They basically worked out the whole-- We'd already sequenced the human DNA genome many years ago, but the problem was once you've got that stranded DNA, what kind of protein is it going to produce?It's a very complicated problem to convert a list of amino acids into a three-dimensional structure because as I said, there's these complicated interactions that go on to form a 3D structure. Deep Mind have solved it might've solved it with AI. They basically trained an AI to put it simply to convert from a string of amino acids and a bunch of knowledge about chemistry into the 3D structures successfully. They're able to say what the 3D structures of all the proteins are that are encoded by the human genome, which is unbelievably mind-blowing. When I was studying genetics in the mid-nineties, that was unthinkable. That's very cool. Anyway, suffice to say that it's possible to get your DNA and convert it into a 3D structure. The 3D structure is what we might call the phenotype, basically. A bunch of proteins will stick together and interact, and then they'll create an organism and that is the phenotype. That is the thing that gets tested. The environment applies to what we call selective pressure to the phenotypes. Remember this diagram here where we're looking at them climbing up the hill and that's the selective pressure being applied. The phenotype is how the DNA is expressed in the world and how after interacting, the chemicals interact with the world, and the fit ones, the good ones, successful ones are able to reproduce in the next generation. That is really it. Obviously, depending on the DNA sequences, then you will end up with slight variations in protein. If you swap out a couple of those letters, you can end up with a different protein sequence and that's a different 3D structure, and that's a different behavior in the world. That's how the variation is expressed and tested. That was my evolution theory crash course. I hope that was useful. We come up with these key terms like the selection and breeding cycle, which we're going to be using in genetic algorithms. We have the genotype, which is the underlying string, which represents the hereditary material of the individuals. Then the phenotype, which is what that hereditary materialis expressed into. In other words, the animalor the creature or a plant, whatever it is that we're expressing it into. Then we talked a little bit about how selection is applied to the phenotype, not to the genotype because the phenotype is the thing that gets tested, that that's the animal that walks up the hill. The genotype is the thing which encodes its range of possible ways of walking up the hill. In this video, I've been giving you an evolution theory crash course and introducing some of the key concepts we're going to be looking atin genetic algorithms."
DSM100-2022-OCT,Topics 5&6: Creatures,"Lecture: Artificial evolution and genetic algorithms: fitness functions, genetic encoding and manipulation, population models",https://learn.london.ac.uk/mod/page/view.php?id=96231&forceview=1,"[MUSIC][MUSIC]-In this video, I'm going to explain how genetic algorithms work. In summary, I'm going to start out by just explaining, giving you a brief definition of genetic algorithms. Then we're going to talk about encoding with a genotype, decoding and testing with a phenotype. Then we'll talk about selection and breeding and mutation and go back tothe start again to show that it's an iterated process. Let's start out by looking at some definitions. The question is who invented genetic algorithms?John Holland, from the University of Michigan, is credited with a lot of the early work in the '60s and '70s on genetic algorithms. Obviously, many other researchers contributed as well. Since then, many more have come in and added all kinds of things to it. John Holland's 1975 book Adaptation in Natural and Artificial System sis considered to be the seminal textbook if you like, earliest textbook in this area. What about a definition of genetic algorithms?It's a probabilistic search procedure designed to work on large spaces involving states that can be represented as strings. Who said that?It is Goldberg and Holland. Holland, we just mentioned. Goldberg's another key researcher in the field. Let's pick that apart a bit. It's a probabilistic search procedure. Probabilistic search procedure. That means that it's essentially--a genetic algorithm is a search algorithm. That's one thing to know. The question is, what's it searching for?Well, it's searching through a large space and the space contains states. In that space, there are a whole bunch of different states, and those states are described or represented using strings. That's the abstract description of what a genetic algorithm isand what it does. Now, the question is, how does that relate to evolution theory, if you like, why is it called a genetic algorithm?Well, we've got selection going on up there. The probabilistic search is this process of selection and breeding. Then down there, we've got the representation, which is the genotype. The string representation is the genotype. Over there, way over there, we've got the phenotype, which is the states if you like. Typically, in a genetic algorithm, it's simplified a bit. It's simplified into-- the state is directly derived from the string, the genotype. The phenotype is directly derived from the genotype, typically. Unlike natural evolution, where the genotype gets expressed, and then evaluated in natural world. It's a simplification of natural evolution and abstraction of itto extract the useful behaviors and characteristics of natural evolution. What's the point of all this?That's all very abstract. What's the point of it all?We define the space. The space that we're searching through is the space of possible solutions to a problem. A genetic algorithm is a search algorithm that's searching for solutions to a problem, which is normally what a search algorithm should be doing. When you do your search on your search engine you're--The problem is, I don't know which website to look at for this particular thing. It searches and it sends you back, ""Here's a set of possible websites. ""Similarly, instead of it being a search for websites, we're searching for solutions to problems. The space or the states are the solutions and the strings are how we describe those solutions. Typical examples of problems are listed below. You can see we have, for example, what's the ideal shape for an aero plane wing?That's the example we're going to be using in this video. Another one might be what are the ideal settings for my data center to reduce power consumption?Then the final one is the final example, what's the most efficient robot form for walking?Shocker, that's what we're going to be doing. In later videos, we're going to be actually building a genetic algorithm which allows us to evolve robots which can move, basically. Our space of possible problem is, we don't know how to design a robot that can move, and so the space of possible solutions will bea whole bunch of different shapes and configurations, which allow us to find one. Moving on. What I want to do now is jump over to my editor program over here. What we're going to do now is we're going to look at how we can possibly encode a problem as a string and then we're going to work through and see how we can then manipulate that string to implement the genetic algorithm. Let's first off, think about a problem. I'm going to take the aero plane wing problem. Imagine I need to know what is the best aero plane wing design?I've got a range of different designs. I've got a flat wing. I've got a triangular one. Maybe I've got a curvy one like that. I could even have something that looks more like that. I've got four different shapes of wing. The question is, how might I describe those using some string representation?Remember, the string can have anything. Any characters in, whatever you want. Commonly in genetic algorithms, the stringis actually a binary string, zeros, and ones. I want you to pause the video and have a think about how you would describe these various wing shapes using a standardized string description. Maybe pause the video and have a think about it. I'm back again, if you paused it. What we're going to do is now come up with a solution to this. The first thing is, I noticed that there's four distinct shapes here. Maybe I can start out by saying, ""I need some bits to describe which shape it is. If there's four, I can use two bits to describe that. ""I could say, maybe this one's going to be represented by two zeros. This one's going to be a zero and a one. That's going to be a one and a zero. That's going to be a one and a one, so with two bits, I've now described each of the possible shapes. The first two bits of my representation are, in fact, going to be the shape of the wing there. Next up, I might say, ""Well, what other characteristics does this wing have?""Well, I could say, ""It's got a length like that. ""Each of them has a distinct length. That might be the next two bits or three bits. Maybe I could be generous and say, ""I can have eight different lengths. ""I'm going to have three bits for that so that's my shape. Then I have another bit, which is another three bits for the length maybe. I can have eight different lengths. Then the next thing is, I might say, ""It's got this idea of being bent around. ""It's got this kind of angle concept. I might say, ""Let's have a look at the angle. "" That one has got this angle like that. Then this one has that angle. This one's got that angle, and so on. I define an angle for each one if it's bent or not. That allows-- I could do that with another. Maybe I can be generous with the angle and maybe have four bits or whatever. It's up to you to describe how precisely each of these things is represented. Now, I've actually represented a wing as a bit string. That's great. I've now got my-- The space of all possible bit strings. Think about that, it's 2^9. What's that?512?I've got 512 possible wings that I could have with that bit string. That's pretty good. That's a good starting point for my genetic algorithm. Generally, you'd have a longer bit string to give you a really large space. Remember that definition, where it said a large space 512points is not really a large space, is it?Imagine a much bigger space. Anyway, that's just a simple example. I've got my genotype. That's my genotype. The next step is to express that genotype into a phenotype, which remember, is the physical expression of this. We know how we're going to do that. We're basically going to chunk through those bits and interpret it into our wing and say, ""Well, okay, that's going to be a wing. ""The question is, how do I then?Now, I've got that description of a wing, how do I then test that and see how good it is?That's the next step. I'm just going to pause again. I want you to have a quick think, how am I going to test this wing?How would you test these different wings in a computer?I'm going to show you how I'd think about doing it. I might say, ""I'm going to build myself a really basic physics simulation, where I basically get my two-dimensional wing shape. ""It's going to be 2D physics, really easy, and I'm going to blow some wind at it I'm going to create some forces. I'm going to blow those at the wing. I'm going to simply measure what the force is going up. If you like, the uplift. What's the lift generated by that wing?My fitness, if you like, the result ofmy evaluation is how big that is there. How much does it raise up?We literally have a physics simulation. I'd instantiate each wing into a little object in the physics world and I would run it with some wind, and I would measure how far it goes up. That would give me a fitness score fo reach of the wings in my initial population. It's worth saying that you need a starter population. I'll start out with a population of a whole bunch of variants of the wings. Maybe one will be like that, maybe like that, and maybe a triangle, maybe a bent triangle, and so on. Maybe I'd start with, say 10, but typically in a real genetic algorithm, you'd have thousand sin your initial population, possibly. It depends how computationally expensive your physics simulation is, and so on. There's various things. Yes, it's fine to say, we've got a population of different things, we're going to test them all, and say, ""Okay, what's the lift on each of them when we blow wind at it?""Then that gives us a score for everything in the population. That's the first part of the testing and the breeding. The next step is, I need to say, ""Okay, I need to generate the next generation, ""because it's all very well to have scored maybe 10 solutions. I could just stop here and say, ""Well, I'll just get the best one out of the 10. ""If you've randomly generated them, you can't guarantee you're going to have a good solution so you need to be able to iterate on these. That's the whole point of the genetic algorithm, you're searching, you don't just do a one-step search and that's your solution. Actually, it's an iterative search through this space. That's what we need to do. How do we iterate?Well, so the next step is to select two of the individuals for breeding. Okay, how does breeding work?How does selection work?Well, let's do selection first. Selection works like this. It's called roulette wheel selection. I'll draw a wheel or a wheel of fortune selection but yes, typically in the literature, it's called roulette wheel selection. Let's say I've got a population of four to make it simple for me to draw. Okay, so I've got four, that would give me four segments. One, two, three, four. I've got four segments, and the angle of the segment, that angle, or that angle, rather, that angle is basically the fitness. It really fits wings, so maybe the triangle wing does really well in the simulation, maybe a weird stubby wing like that doesn't do so well. Maybe the banana wing is the worst one. Maybe a really thin triangle. Say I had four individuals, I'd measure the fitness of each, and the higher the fitness, the greater the angle it gets on the wheel of fortune, the roulette wheel. The next step is, so if you've ever watched a James Bond film, or whatever, or been to a casino, et cetera, you spin it around, so you spin around the roulette wheel, and wherever it stops, that's the first parent. Then you spin it around again and that'll give you the second parent. Now, you can imagine that the more of this you have, the higher the chance of you being selected when you spin around. That means that the fittest individuals are more likely to get selected. It also means that the less fit ones have a possibility of being selected as well so they don't get completely discarded, just in case they've got some useful stuff in there. I'd end up with two individuals. You can see I've selected the triangle one, and the stubby square wing. Though that's my selection process, and I would do that multiple times, to get multiple pairs of parents. The next step is I need to take those two parents, and I need to recombine them in some way to generate the next generation. The first individual that's going into the next generation is going to be some combination of those two. How am I going to do it?Well, there's two steps, there's crossover and then there's mutation. Let's look at those two and see how they work. This is crossover. Imagine I've got my two genomes or genotypes, like this. That would be the triangle, and that would be the other one I selected. Those are the two parents I've selected. Crossover essentially is looking at taking part of one and then part of the other. One point crossover would be-- I'm going to take the top, maybe that bit of that genome and that bit of that genome and that would give me the next generation. You can also do a multi point crossover. You can say, ""Well, I'm going to get that bit and that bit, ""or with really long genomes, maybe youcan end up with different whatever you want All kinds of combinations, but the basic one is say, we'll call it single-point crossover like that. You would typically use a random number to get to choose where you crossover or maybe you'd look at the fitness and say, ""Well, again, I'm going to apply the fitness again and allow it to choose so the fitter individual, I'll take more of its genome. ""Again, you've already applied the selection pressure, so why reapply it?I think in the previous video, I've given you a link to this Inman Harvey paper where he goes through and really picks apart the genetic algorithm says, ""Well, why would you apply selection pressure twice?You've got selection pressures, just apply it once and simplify your algorithm. ""Let's just say it's a random number. What's the result of that?Well, if I just change the color for the bottom one, I'll end up with that of that one, and that of that one, and that's my new genotype. That is my new genotype, it's got abit of one parent, a bit of the other. If I was to express that into a phenotype, maybe I'd end up with, I don't know a slightly, slightly blockier triangle, I don't know, whatever it is. The bottom one is a combination of the other two. That's how I do selection and breeding. Remember, we need to do mutation now. The final step is to take this child and then we mutate it slightly. Again, randomly, we would choose places on the genome to mutate it. If we just redraw it and put in those mutations, what I'm going to do is I'm going to pick some random points on it, I'll say, well, that bit there that bit is going to flip, that bit's going to flip. Depending on, you have some percentage of them that get mutated. You could basically flip the bits on those, and then you end up with a slight variation. You've got a combination of the two parents with a bit of change, to give you a nice amount of variation. Remember, we do this multiple times, we go back, we do the roulette wheel, we spin it around until we get lots of pairs of parents, and we do the crossover mutation. Eventually, we end up with, say 10, or whatever our population size is, we end up with a whole bunch of new genomes in our population. Then, of course, we go back to the start again, we go right back to this step We evaluate the whole population again and then we do the selection. We do the crossover and mutation and we keep going, and we go around and around until eventually, we end up witha population of really good solutions. As I'll show you in a later video, so John Holland's schema theorem describes why this works. It explains why doing this weird looking process, why it allows you to search highly complicated spaces, and find decent solutions in those spaces that other types of algorithms might not be able to find. For example, hill climbing, where you just always choose the next, you mutate and find the best next one, you don't have a population doing crossover and all of that We put all these features into the genetic algorithm. Holland describes why they work using his schema theorem, which we'll talk a bit about in a later video. Okay, I think that's it. Let's just summarize. We've just been talking about the genetic algorithm. I've told you who's credited with the main seminal work on this, which is Holland. Then we've talked about encoding. How we take a problem and express it in the form of a string of bits in our case, or numbers. Then we talked about how we express that and test it, how we convert the string of bits, and run it and test it, and that gives us scores for various strings. Then I talked about how we select and breed those strings to produce the next generation and how that continues iteratively until we havea whole population that's fit. Okay, in this video, we've just seen a worked example of how genetic algorithms work."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Why do genetic algorithms work?,https://learn.london.ac.uk/mod/page/view.php?id=96233&forceview=1,"[MUSIC]In this video, I'm going to be discussing why genetic algorithms work by highlighting some features of a search spaceand explaining how the genetic algorithm deals with those. Let's look at the summary. We're going to be introducing the concept of local and global maxima, and talking about hyperplane sampling, which helps deal with the problem of local and global maxima. Then I'm going to introduce the schema theorem which is John Holland's explanation of why the genetic algorithm allows a hyperplane sampling. Then we're going to talk about implicit parallelism, which is another key concept in genetic algorithms and why they work. Finally, we'll talk about computational parallelismas a balanced view because those are two types of parallelism. This is a nice quote from Whitley's genetic algorithm tutorial. At this point, when people are learning about genetic algorithms, ""The question that most people who are new to the field of genetic algorithms ask at this point is why such a process should do anything useful?""Because you've been probably looking at this algorithm thinking, ""Yes, that's kind of interesting, but it seems really complicated. It's got all these weird bits in it like breeding and crossover and mutation, why should any of that work?""In Holland's book, that's one of the things he explains. I'm going to introduce a couple of concepts which help to understand that. First of all, the concept of maxima, global and local maximum. What do we mean?Imagine here, we have a space of 1D space basically, and we are moving through that space. As we search, we're moving through the space. Let's imagine a simple hill-climbing algorithm. A hill-climbing algorithm simply takes a solution. You're using the same encoding, a bit string, a solution, and then it just mutates it, makes multiple mutations of it, and it searches around, then tests them all, and whichever solution gets it to go up slightly on the hill, it will choose that one, so every generation is always going to go up. Not a genetic algorithm, but a simple hill climber. What will happen is, let's say you are here and you're rising up, you'll find that you only ever are going to go up to that one, you'll never going to find that one over there. You're never going to find the global maximum because you're only ever going to climb up your local hill. That is something that genetic algorithms aim to solve. Imagine that your search space is far more complicated. This is a 1D search space, we've just got one thing that we're varying. Imagine if the search space is 100 dimensional, then this space, you can't even conceive of what 100-dimensional space looks like. It's a really complicated space, there's going to be all kinds of hills and troughs, and it's going to be really complicated. If you think about a natural evolution again, I talked about searching spaces in natural evolution. There's a great book about this, Origins of Order by Stuart Kauffmanwhich inspired me to understand this better. Think about the natural worldis like this really complicated abstract search space that's changing all the time. You need a really hardcore algorithm to be able to search a really complicated high dimensional space to find local or global, at least high peaks. How does a genetic algorithm do this?Let's think about two things that we want to consider here. Hyperplane sampling, which is being able to break a solution into component sand to test these component sin multiple combination with other components. Let me jump over to this drawing. Remember this, where we're talking about crossover?This is what we mean, we mean the idea that we can basically treat--First of all, our encoding allows us to break it into components. We actually explicitly say, ""The first few bits are going to be the shape of the wing and the next few bits are going to be the length of the wing. ""Those are distinct components within that. Crossover allows us to, in a fairly, maybe more loose way, have sections of the genome which can get frozen, and then we can test them out with other sections. In this crossover example, we're testing the top bit of the parent, that red bit, with loads of different versions of that from other individuals, for example. It's sort of like saying, ""Okay, well this bit looks pretty good but we want to see how it interacts if we change a little bit of it. ""That's what we mean by hyperplane sampling. Then we get to the schema theorem. I'm going to skip implicit parallelism, come back to that, and continue with this feel of the schema theorem. The scheme of theorem, pushing on, is the idea that you can essentially describe a schema which is like a regular expression. It's kind of like a regular expression. It's saying, ""Okay, if you've got a bit string and here, my schema is 1**0. ""Imagine a 4-bit string and your schema is 1**0, that means that any strings that start with a 1, end with a 0 would match that schema. Similarly, we could say that the schema here is whatever we have in that red section, 10001, whatever it is, that's our numbers. Then we have stars for the green bit. We're saying, we're freezing the red bit and we're trying out different variations of the green bit. That's the way of describing a genotype in a schema type of way. The schema theorem says that if you do this and you start sampling by sampling through things that match the schema, then you are doing hyperplane sampling because you're freezing yourself in a particular plane, so the plane that you're freezing is the bit of the genotype that's got the numbers in, not the stars, so you're freezing. Here's a plane that goes through state space. Then you're going to search by the bits that have stars, you try variations on the stars. You're cutting a plane through state space with your frozen part of your genotype. Then you're searching with the other bit. That's what we mean by hyperplane sampling combined with schema theorem. That was, I guess, John Holland's contribution to all of this. That is expressed quite neatly and mathematically in the various references I've given you on this. Just to clarify that, maybe with a visual. Obviously, I've got colored strips going across here, and each colored strip represents a plane, if you like, that we're freezing, and then we can swap out the other colored strips for different ones. That's the idea is that you can jump around in state spaceby freezing one bit and searching the rest. I want to go back now to implicit parallelism. Let's think about implicit parallelism. This one here, let me just [?]. Implicit parallelism is looking at a search at the next level up, we're looking at the population. We're using a population model to maintain, optimize, and even recombine multiple solutions at the same time. The difference between this and the hyperplane sampling is, hyperplane sampling, you're saying, within the genotype, we're freezing bits and allowing some--We can say this bit of the genotype is good, but we're going to try it out with different other bits to see how it combines. That's hyperplane sampling. Implicit parallelism is going up a level to the population and saying, we're going to allow multiple bloodlines within the population because if you think about how the roulette wheel works, what it does is it makes it possible for multiple individuals to contribute to the next generation, which means if you've got a large population, you can have multiple families going through the generations because they can potentially breed. That's what we mean by implicit parallelism, it's maintaining several parallel solution sand evaluating them, and even allowing them to recombine with each other at the same time. That's a key feature of their genetic algorithm. Let's just see. That's Holland's idea of parallelism. It's different from the next idea of parallelism which is computational parallelism. Computational parallelism is because GAs lend themselves to being computed in parallel. In other words, you can--We'll see this when we build our own GA. What we mean is on a computer, you don't just have to evaluate each individual in series, you can evaluate them in a multi threaded setup. It's more to do with how you program it. The fact that having a population allows you to evaluate multiple itemson different cores on your CPU or whatever. That's the idea of computational parallelism. That's different from implicit parallelism which is more of a subtle thing relating to the idea of different families flowing through your genetic algorithms population. That's enough. There's a lot of concepts there but I wanted to try and explain why the genetic algorithm has these feature sand why they contribute to its success as a search algorithm. We've introduced a few concepts, so we've had local and global maxima. The idea that the search space has these different positions in it which have varying fitness. If you don't have an elaborate enough algorithm, you might end up stuck on a local maximum and will never find that high peak. Hyperplane sampling, we talked about the idea and how that relates to the schema theorem that you can freeze one bit of your genotype through crossover, and then through mutation and crossover, then sample through that frozen plane for various other solutions. Then the idea of implicit parallelism, where the population model allows us to maintain multiple bloodlines or families within the populationso that we don't just optimize one solution, we optimize multiple possible solutions at the same time. Finally, we talked about computation parallelism, which is that when you're programming these things, you can actually use parallel computing techniques, threading, and so on to make them faster. In this video, I've been explaining why the genetic algorithm works."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Karl Sims creatures,https://learn.london.ac.uk/mod/page/view.php?id=96235&forceview=1,"[music]-In this video, I'm going to be talking about Karl Sims' creatures, which is quite a famous paper about genetic algorithm sand evolving morphology. In summary, I'm going to, first of all, explain what evolving morphology actually is. We're going to talk about Sims' creatures and how he defined morphology in this system, and also how he defined a control system for the creatures. Then we're going to talk about the simulation that he used to assess the creatures and the fitness function sand how they were evolved through mutation and population models. Let's start by talking about evolving morphology. Before we get into that, I'll just say that it is, for me, the inspiration point here is probably Dawkins' biomorphs. That's one of the earlier papers where a clear attempt was made to use a genetic algorithm to evolve form. Dawkins actually was using it in his blind watchmaker bookas a demonstration or a proof if you like that, with a very simple algorithm, which has an evolutionary feel to it, you can evolve very complicated forms quickly in order to build on the argument that evolutionis capable of generating really complicated things over a long time period. You don't need an intelligent designer. That was Dawkins' angle on it. Just to explain what Dawkins' biomorphs was, to draw it, you would start by basically saying, well--You get a bunch of variations. You get a bunch of different creatures that look like, say this and maybe another one would look like that. One might look like that. You get a bunch of different creatures. It's the idea of variation in the population. Then as the human operator of the program, you would say, ""Okay, I like the look of that one and I want to make variation son that. ""Then it would basically generate the next generation for you, which would be based on that. You'd end up with, maybe extra joints on the legsor something like that. Very quickly, over a few iterations Dawkins was able to show that it was possible to come up with very elaborate and natural-looking forms just through this processof interactive evolution using interactive genetic algorithms. That's what, evolving morphology--It's a really good example of it and a basic version, which was used to make a philosophical point if you like. The next thing we want to talk about is Sims creatures. It's a big development of this into three dimensions. What I want to do is just quickly run a simulation here. Whoops, let me just run that and I'll zoom in so we can see it. There's the creature. What you can see here is an evolved creature. What it's doing is it's moving its various weird joints around so that it can ambulate along. It's figured out a way of moving along by itself. It's exploiting features of the physics engine to jump in the air by pushing itself off and things like that. That's not Sims' creatures though, but it looks like it because, for copyright reasons, I'm not really allowed to show you, the actual original videos of Sims' creatures. If you go onto YouTube and search for Karl Sims' creatures, you'll very quickly find it and have a look and you'll see it quite similar, look into that one, but that gives you the idea. The idea is we are evolving a 3D morphology. Let's just read the first paragraph from Sims' paper. This paper describes a novel system for creating virtual creatures that move and behave in simulated three-dimensional physical worlds. It's basically about evolving creatures that existin a simulated physical world and they move and behave, behave bit is interesting. We'll come back to that in a minute. The question is, how does it all work?How did you do it?This is 1994 just to say. When I first started studying a life, which is when I did my master's degree in 1998I was introduced to this paper. It was only four years old at the time. It was reasonably fresh as a state of the art. This is amazing work and it was so--It made a big impression on me at the time, because I'd come from a biological sciences background. To see a fully working, evolving simulated evolution system, it was really, really impressive. How does it work? Well, it's got two parts. You've got the morphology of the creature and you've got the control system for the creature. Let me just explain with another drawing, how the morphology works. It's a directed graph. In other words, we've got something like this where we have node sand the nodes are connected together. What we do is we basically have a graph like that. If you had a simple graph like that, and you turn that into a structure for a creature, you would end up with the shape, it could be different shapes, but basically let's say, just to differentiate between the graph on the left and the creature you'd end up with a creature that's basically three bits connected together. That's not particularly interesting, right?It's pretty obvious mapping between one to the other, but the graph can also have recursion in it so it can connect back to itself and it can have multiple connections. Let's consider those two things. First of all multiple connections. If I had one like this, where we have two connections, and then maybe that one connects to one as well. That would mean that I actually end up with a structure like this, with two connections coming off of it, and then two copies of that, oops, sorry. That's supposed to be a square, two copies of the thing. That goes to that and that goes to that and that because we've got two connections, so that shouldn't be--Then we've got one more connection coming off of it, which is this one. We've got one more connection on that. Then of course you'd have it on that one as well. In that case, you end up with a creature with two dangly legs coming off the side. Let's just see one more, which is the one with recursion. I'll try not to get in the way. If we draw a graph like this and this one's going to have--This is the example from the paper actually. This one has 1, 2, 3, 4 connections coming out to this one and then it has one recurrent connection going back. That's the graph that Sims used as an example in the paper. Let's just iterate on that and turn it into a morphology and see how it works. This one would turn into a creature with, so that would be the head connected to a body. The body's got four connections that'll go 1, 2, 3, 4, and each of those has a box. Then because we've got this bit because the leg bit has also gota recurrent connection on it, it means it's got an additional link on it. We've got jointed legs and arms, if you like. That's the example from the paper. Using both recursion and the multiple connections idea. That gives you this jointed two arms, two legs creature with a head and a body. That's essentially how the morphology is represented. The genotype is actually a graph structure, a directed graph with this recursion and multiple connections allowed. That's the morphology. Then what about the control system?In order to move these creatures, you need a control system. The control system maps from basically sensor input son the left to motor or actuator outputs up there, right. We've got J1, J2 represent two joints. This is a thing with two joints and the joints are the things that connect the two pieces together. The J1 J2, is saying, ""What angle is that joint at?""Then A1, A2 are the actuators. They're sort of the motors which set the position. It's basically putting some power into that joint. Then in the middle, you've got this network, another directed graph, basically. Again, that's encoded genetically as well. In later example, so remember earlier I was saying that the fitness function there's different fitness function sand what we mean here is that he also had different sensors coming in. More on that in a minute again. This is how he did the simulation. ""In order to test these creatures I need to simulate, ""so he says, ""Dynamic simulation is used to calculate the movement of creatures resulting from the interaction with a virtual three-dimensional world. ""In the paper, he basically says he ran them in simulation. This is 1994, I think he had a population size of 300. If you think about it, how would you go ab outrunning 300 complicated physics simulations multiple times for all the generations?The answer is you'd get one of these, which is a connection machine, a CM-5, which is, I think one of the later models. It is basically a multiple processor, really powerful machine and in fact, the fastest computer in the world in 1993, apparently. He had access to one of these things, lucky Karl Sims. He was able to run his physics simulation fast enough in parallel on these machines, which had, I think had like 1024 SPARC CPUs, which are those kind of Sun Solaris operating system ran on it probably. Not sure, but you can look it up anyway. That is a pretty mean-looking machine with that red light and a sort of precursor to the gaming rigs we have now, isn't it?Anyway, so that's the machine you ran it on in 1994, but we are going to be building our own version of this that will run on my little laptop here no problem. That's cool, isn't it?That we're now able to implement our own version of this and test it out. That's how we ran the simulation, but what's it actually doing?You got different fitness functions. In this work, virtual creatures are evolved by optimizing for a specific task or behavior. He was interested in a few different tasks and behaviors. One of the ones was swimming underwater, trying to go towards a light, in which case they'd need to have an additional sensor beyond the joint angle sensor to show them where the light was so some sort of light sensor. There was another one where they had to compete, actually, use competition as well. It's not in the original paper, but in the secondary paper that came out later, he talked about competition as well. There's various fitness functions he used, but the most basic one, the one that we are going to implement is basically measuring how far the thing can travel in a certain amount of time. How good at walking is it. To generate the next generation, the offspring are generated from the surviving creatures by copying and combining their directed graph genotypes. When these graphs are reproduced, they're subjected to probabilistic variation and mutation. Let's look at a few bits there. Copying and combining their directed graph genotypes. Basically, you get two parents and you mash their graphs together. That's one thing. Then you've got probabilistic variation or mutation. That's when you mash the graph together, then you maybe increase the amount of recursion on one or increase the number of connections between one another or add a whole new connection. There's various ways of mutating, those graphs that he used. The results. I just want to read this because I can't show youthe actual YouTube video on here, but you can go and look at it, but this gives you a flavor. This is a fitness function where he was measuring how far they can walk. ""The walking fitness measure also produced a surprising number of simple creatures that could shuffle or hobble along at fairly high speeds. Some walk with lizard-like gaits using the corners of their parts. Some simply wag an appendage in the air. "" they basically go like this. It allows them to move. It doesn't work for me, but it works for these creatures. ""They rock back and forth in just the right manner to forwards. ""You'll see all these wonderful movements in the YouTube videos and more. That's it. Again, like I said, at the time and even a few years afterwards, this was the classic example of a genetic algorithm being used to evolve forms. Maybe in later videos, we can come back or I'll give you some references for state-of-the-art systems because, after this, a lot of work went on beyond Sims. People did all kinds of things and I'm going to give you some references for those up to the present day. In summary, we've been talking about evolving morphologies and we've looked at a specific example of one of the classic papers from 1994, which is Karl Sims' creatures paper. I talked about how he encoded the morphology as a directed graph and how that could be expressed into a phenotype. Then we talked about control and how there wasa neural network type of thing in there as well, which was also a directed graph. We talked about how he was able to run these things in simulation, using a super-powerful computer and the fitness function, which there were various fitness functions that he used, including seeing how far they could walk. We talked about mutation and how he generate the next generation. In this video, I've been talking about Karl Sims creatures."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Lesson 1 summary,https://learn.london.ac.uk/mod/page/view.php?id=96238&forceview=1,"[music]-Well done, you've reached the end of the first week of the creature's case study. At this point in time, you will have covered the following learning objectives. You should be able to explain the key features of a genetic algorithm and how they can be used to address a specific problem. It's worth noting that we also had a crash course in evolution theory, so now you should also know the basics of how evolution works, your natural selection, and actually, we're going to be using artificial selection in our genetic algorithm. In fact, the whole thing's artificial. Secondly, you should be able to compare and contrast examples of artificial life systems. We've seen the creature system, and we've seen the general idea of how evolution works. When you encounter artificial life in evolutionary computation systems, you should have a few tools at your disposal for understanding what they're all about. Thirdly, you should be able to develop an argument about the potential of evolutionary and bio-inspired computing. I'm showing you the general area of bio-inspired computing, I'm giving you some historical examples of different research fields which have explored this fascinating area. Yes, so well done for reaching the end of week one, and we'll see you in week two."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Introduction to Lesson 2,https://learn.london.ac.uk/mod/page/view.php?id=96240&forceview=1,"[music]-Welcome to the second week of the creatures case study. This week we're going to be achieving the following learning objectives. First of all, we're going to be learning all about the URDF file format, which is originally from the robot operating system project but it's actually more widely used in a variety of robot simulation environments now including pybullet, which is what we're going to be using, but also gazebo is another example. Secondly, we're going to be learning about creating simulations using the pybullet environment. We're going to be learning more about offline later but certainly, we're going to be seeing how to create online simulations, real-time simulations, where we're running robots in an environment. Thirdly, we're going to be learning about how to describe the key components of a morphology evolving system because that's what we're going to bebuilding over the next few weeks. I've got a little slide here just to get to whet your appetite as it were. You can see here, a screenshot of the pybullet environment showinga weird kind of robot embedded in the middle of the floor, and a strange Samurai castle surrounding it. If that's intriguing, keep watching and I'll tell you all about it in the next few videos. Yes, welcome to the second week of the creatures case study."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Key components in the creatures system,https://learn.london.ac.uk/mod/page/view.php?id=96243&forceview=1,"[MUSIC]In this video, I'm going to give you a bit of an overview of the key components that we're going to be having to developin order to implement this creatures system in the next few weeks. Let's have a look at the summary. I'm going to give you an overview of the GA, just as a reminder to contextualize what we're going to be talking about. Then we will discuss phenotypes, simulation, genotypes, fitness functions, population models, computing in parallel, and finally have a quick look at the technology stack as an overview. There's a lot to get through, but don't worry, we're not doing it all in this video. This is just an overview. In the next few weeks, we're going to be building out this system. Let's have a look at that GAand just remind ourselves what we're aiming for here. The GA has these three phases, more or less. We start out with this varied population over there, and then we pass, we go through some selection test in the middle there. We test the various individuals in the population, and then we select them for breeding, and then we end up with, at the end, this one variations of the most effective individuals. Then we go back again over to number two, which is we test them again. We test, breed, test, breed and keep going until they get fitter and fitter. That's basically what we're going to be doing with this GA. This is a really interesting quote from Harvey's paper. Now, remember, I mentioned Harvey's paper because he essentially took the GA and simplified it into its cleanest possible form. I don't want to do that here just because in order to do that and understand that properly, you need to have, basically, you need to know how GAs workin their more complicated form before you can simplify them. That's why we haven't done it, the exact Harvey way in this video, but I recommend you go off and read that, but here's his quote about where the interesting bit is when you're building a GA. ""The most creative and challenging parts of programming a GA are usually the problem specific aspects. ""What he's saying is the bit where you are designing all the interesting stuff about how you're going to simulate it, how you're going to test the fitness, how you're going to encode it, what's your genetic encoding going to be, all that stuff, that's all the creative and challenging stuff. For Harvey, the GA itself is not that complicated an algorithm. It's pretty easy to understand how it works. It gets a bit more complicated when you actually code the thing up. Anyway, moving on. We are looking at the creative and challenging stuff now. The phenotype. We're going to roughly follow Sims with our phenotype, but we're not going to have such a complicated control system. We're not going to have controlling neural networks, but we are going to use the URDF file format. Now, let me just run you something here. This is--Oops, I'm already doing that. This is what this does, is it basically shows you my simulation running, and you can see it's iterating through several random creatures, so that one's quite good. That seemed to get some good movement on, didn't it?Let's look at another one. I feel like a sports commentator here. Here we have a weird fairly spaced out creature, which doesn't seem to be moving an awful lot. You can see there's definitely some movement going on. Here comes another one. How's this one going to do?This one's got a twisty movement, and you can see it slowly moving, again, not a very successful. These are just random individuals, as you can imagine, and these are what we call phenotypes because they're in instantiations of the genotype in the real world. This one, maybe it's going to be moving a little bit but it seems to be fairly static, not too successful. These are all quite large complicated creatures as well. We'll end up with something like this at the end. We will be able to build this, but I just wanted to give you an insight into that. Let me just kill that now. Those are phenotypes. That's why I'm showing you it. Those are examples of the phenotypes we're going to be working with. You can see there's lots of interesting shapes there, but the problem is, if you just generate random ones, most of them are not very good at moving. That's where the GA comes in. Now, all of those creatures we just saw were rendered out as a URDF file format. I'm going to show you full details on that. We're going to be programming a system which outputs these files, but we can have a quick look at it now. It comes from the robot operating system technology stack, which is this operating system designed for building and testing robots. It comes with simulation software and stuff like that. PyBullet seems to integrate quite well with it, and have a [?] for URDF files. This is what they look like. Let me get that out the way. They basically have their XML files, and they have a robot tag at the top level, and then they have a series of a link tag. That's not dropped down to the next line, but yes, so each part of the robot is specified in a link tag, which has a geometry. It has one or more parts to the link, in this case a cylinder, and then a collision box. How does this collide with the world, and what was the inertia of this part?You can have really heavy bits of the robot or light bits there. We have loads of links. These are quite complicated robots. Once you get through the links, you then get to joints. The joints actually specify connections between links. You have the parent link and the child link. That's the graph based thing where we're connecting from one node to another. I have a whole bunch of things which specify, like how it connects. Does it connect?You've got two things, right?Then your joint specifies how they're connected and what angle, how far apart they are, things like that. That's what we're specifying here. This is quite complicated URDF file. I didn't write this by hand. This was generated from a genotype and automatically, and that's what we're going to be coding up. That's how we're going to specify our phenotype, so it can be loaded into the simulation. Let's go back to the slides. Next thing is the simulation itself. We actually saw the simulation running just then, didn't we?We saw a phenotype being expressed and run in simulation. As I said, we don't have a CM-5 handy. We don't have one of those amazing, most powerful computer in the world from 1993 with nice red lights. We don't have one handy, so we're going to have to run it on our own machines, but that's okay because at the time of filming, it's 2021. We've got pretty fast machines lying about the place, which we can use and we're going to use the PyBullet physics engine to run our simulation. The creatures are going to be interacting with the physics engine through that because we don't have to build our own physics engine at this point. What about the genotype?Well, the genotype, we're going to follow SIM, so we're going to represent, we're going to have a genotype which is manipulable, but which can represent some directed graph. There's going to be some simplifications around the control system just to make it easier to code up in the time we have. Then what about the fitness function, which is basically going to be, how far does it move?That's the short answer for what the fitness function does. That should be fairly easy. We're just going to measure where it starts, and then run it for a few cycles and see how far it's got. That's going to be the fitness function. You could see when we were watching the animations earlier, that that should be fairly easy to implement. Now, the end problem is they're going to cheat. When I was developing this, so before making the videos, I built out the whole thing and planned itand made sure it is working more or less how it should. In that process, I found that the big problem and the very often reported and entertaining problem with evolutionary algorithms is whatever simulation you create, they will find the problems with it and exploit it. They'll figure out a way of basically, I don't know, finding some free energy somewhere by wiggling in a funny way, or they'll figure out how to fly when they're not supposed to be able to fly. I'm going to show you exactly that now. I've got an example ready of one of the cheating ones, and that is over here. Let me run the cheater. You can see, so I call this one helicopter because it basically spins like it. I figured out, oh, look, see, I guess it's more like a fly or something, isn't it?One version of it actually span around. You can see that it's whizzing, flipping around, spinning around and getting higher and higher away from the ground there. Yes, they are going to cheat, and that is an example of it. I was exploiting the fact that the joints between, I couldn't fully work it out, but I think the joints between the parts of the robot are stiff, have a certain springiness in the simulation. I think what's happening is it's worked out that if it moves in a certain way, it can generate a load of energy from the spring. That allows it to move through the air without actually gravity being able to pull it back down again. We'll have to work out ways of stopping the things from cheating. Within our testing environment, our fitness function environment, we'll have to figure out a way to block that bad behavior that we don't want, because the evolutionary algorithm will try and cheat, but we can then put an extra layer of testing into make sure that if it does certain things we don't want, we can stop it, but we'll get into that later. That's the interesting bit of the fitness function, stopping it from cheating. Then in terms of the population model, we're going to implement the roulette wheel selection that I talked about in an earlier video. The next thing, we're going to be doing computational parallelism, because we're going to get Hollands implicit parallelism for free because it's a genetic algorithm. It comes with that. We want to also do computational parallelism where we actually are able to run multiple simulations on different cores, because even a simple laptop like the one here, I think has six cores. I can, in theory, run six instances of the simulation in parallel, which means I'm going to get six times to speed up pretty much straight away because I can evaluate six individuals at the same time. Yes, we're going to use that. We don't have 1024 cores like Carl Sims hason his amazing CM-5, but that's okay. CPUs that we do have are a lot faster than the sparks that he had. Okay, moving on. What about the technology stack?Well, basically, it's going to be Python, we're going to be writing in Python. It's going to be using the PyBullet physics engine, which is, as I say, used by a lot of robotics researchers. We're going to be using the URDF file format, which is also used by robotics researchers. We're using some of the--I've deliberately chosen this technology, firstly, because it gives me the appropriate functionality without having to build it all myself from scratch, but also, because it's an opportunity for you to learn these environments, which are used for this research. You'll find that when you dig around in the PyBullet manual, the documentation, that lots of people are using this stuff for doing interesting stuff with machine learning, and robotics, and so on. It's good stuff to learn. Okay, the end. In summary, we've just been having a quick overview of what we're going to be implementing in the next few weeks, and seeing some examples of how things are going to work and how things that might go wrong. Yes, I've talked about the phenotype, the simulation, the genotype, the fitness function, the population model, how we're going to be doing this as a parallel computing system. We're going to use the parallelization features of Python. Finally, I had a quick look at the technology stack at the end there. In this video, we've just been having a quick overview of the system we're going to be building in the next few weeks."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Introduction to pybullet,https://learn.london.ac.uk/mod/page/view.php?id=96245&forceview=1,"[music]In this video, I'm going to give you an introduction to Pybulletand show you how to install it. To summarize, we're first of all going to answer the question, ""What is Bullet?""Then we're going to see some Bullet demos just to see what it's capable of. Then we're going to install Pybulletor look at Pybullet and install it, and then verify, I'll show you how to verify that it's working. By the end of this video should havePybullet up and running, if you follow it along. First of all, what is Bullet?Well, it's a rigid and soft-body physics engine originally created by Erwin Coumans, and it's a really powerful tool. I'll just say that when I was evaluating how to best deliver the materials on this course, how to design it, which framework to use, and things like that, clearly, Bullet came outas being a very strong contender. I'll show you why when we look at the demos in the next slide, so demo time. Let's go over to my browser here. Now, it's worth saying Bulletis written in C++ and C, but that, in ammo. js, is a transpolation of that into JavaScript, so its a direct translation of the code into JavaScript, an automatic one, and therefore, it behaves very similarly, pretty much the same, as Bullet, but the difference is, I can run it in my web browser, so it's really easy for meto show you some demos. Let's jump in and look at those. This is the same as if you'd built the same thing using Bullet itself. That's the starter demo, just a calibration type of demo. You can see a bunch of cubes fell down from the sky, and that's kind of fun. Now, let's look at another demo. This one I really like because it shows the soft body dynamics. A soft body is when something can deform. You can see that that sheet over there--so when I swing the stick backwards and forwards or the thing back, you can see the sheet is flopping around, it's deforming the shape. That's what we mean by soft-body physics. Now, the wall, on the other hand, is not soft, it's actually made from bricks. Oh, you can see, but I can knock them off if I swing my sheet around in just the right way. That's a really good demo of the soft body capabilities of it. This is a bit more satisfying in terms of knocking things over. Here, I've got a soft body rope, but I've got a hard body or a rigid body sphere on the end, so it's much easier for me to smash that wall down. Now, moving on, I just wanted to show you one more. Now, this one is a soft body demonstration where you actually have very deformable bodies, so I can throw cannonballs at them, and you can see that they deform in a very satisfying and realistic way. That's all running in real time in my browser, which is pretty impressive when I throw a load of bricks at it there. That's just a quick demo of some of the capabilities of Bullet. You can see it's a really powerful physics engine, and it's also being ported to various different environments as you just saw, so it can run in the web browser, it can run as a C++ Library, and now there are Python bindings for it. I'm not sure when they did these, but the citation is 2016, so it's fairly recent that they've added the bindings. I think the original version is 2006, or something. Yes, so let's just read this now. According to the authors. ""It is highly recommended to use PybulletPython bindings for improved support for robotics, reinforcement learning, and VR. ""What they've done with the Python version of Bullet is they've added some extra features really to allow it to more effectively help researchers to work with robotics and VR. That's kind of what we're doing, we're going to be evolving some weird, robotic creatures. As I say, when I was evaluating different physics engines, Pybullet clearly had the capabilities. The fact that there's a community of people into robotics who are using it, I thought it'd be really useful for people to learn about it as well, so I wanted to expose that a bit. That's various motivations for me using it. Let's get into the nitty-gritty, let's see how we actually go about installing Pybulletand getting it up and running. Now, before I get into my terminal and start typing things in, I just want to say that if you're new to Python, I'm going to provide a little bit of reading material for youto help you get up and running, but if you just follow along with my commands, and you've got a nice Linux installed, you should be fine, but these commands should also work on a Mac. If you've installed Python with Home brew on a Mac, for example, this should work just fine. If you're on Windows, you can use Anaconda or Conda, or even use Windows Subsystem for Linux, they should all allow youto run these commands. It should be fine. Now what I'm going to do is I'm going to install a virtual environment, which is a local install of Python, a little miniature install, which allows me to install packages without having to be the administrator. Typically, to install Python packages globally, you have to be an administrator and it's a bit of a pain, and it changes everything on the system, but with virtual env, you can just have a local setup just for one particular task, and that's exactly what we're doing. How do we set up a virtual environment?We run Python 3, importantly, Python 3, and we type ""-m venv"" like that. Let me pull this down a little bit. We do python3, module venv. It's going to run this module. I need to specify where I'm going to create my virtual environment. I create all my virtual environment sin a Python folder in my home directory, so that tilde sign you see before Python there, it says home directory, Pybullet. You can put any path there, C/ whatever, temp, whatever you want to put it, or whatever is appropriate on a Mac, or you can use the tilde to default to your home directory. That's where it's going to create the environment, it runs. Now, it's created the environment. Now, I need to activate that Python virtual environment, so I do source, and I put in the path to the environment, which is Python Pybullet, then I put bin/activate, like that. Let me just try and widen this out a little bitso you can seethe whole-- There we go. Now you can see the whole command. It's sourcebin/python/bin/activate. I'm now in my Python virtual environment. The next step is to use pip, which is the package manager for Python to install a bunch of packages, so I'm going to install Pybullet, I'm going to install I Python, which is my preferred interactive interpreter for Python, and I'm going to install NumPybecause we might need that later. Oops, I forgot to say install, pip install. Then because I've installed this before, it uses the cached version, so it doesn't have to download them, so it's rather quick. It's just installing it, so they're all installed now. Basically, at this point, basically, I've set up a virtual environment. I've got a little miniature Python install into which I can control exactly which packages are in there. I've now installed the packages I want, which is Pybullet, I Python, and NumPy. The next step is to see if I Python is working correctly. I'm going to just fire up my I Python interpreter like that, and I do import Pybullet as p. It means that I'm going to have a variable called p, which represents the Pybullet library. You'll notice that it prints out a little message saying when it was built, so it gives you an idea of which version you're using because and usually, I noticed that Pybullet doesn't have a version flag, which you would normally findin a Python library, but that's okay, we don't need that, because it just tells us what the date is. The next step isto actually run it. To run it, let me just clear that off, I do p. connect, and that allows me to connect to a simulation environment. I need to say what type of environment I have. There's a few different ones, there's network ones and things like that. I wasn't able to get the network ones working, but there's also an offline one. This P. GUI one will fire up in a user interface, and basically, allow me to run simulations in real time, but later, we're going to be running simulations in non-real time, because we're going to have to basically evolve these creatures, and it's going to involve running lots of simulations in parallel, and eventually, we will implement that. For now, we're just going to use the GUI one, so let me run that. You should see something like this when you run that command. That means you've got Pybullet all installed, and it's up and running, and it's great. We're done here, we've installed it, and it works. Just to play around with that a little bit. You can see I can scroll up and dow non my mouse wheel and it zooms in and out, and I can hold down Ctrl, and then I can move the camera angle, so I can zoom in and outand I can move around. That's just giving you a couple of ideas. By just saying p. connect, I can get this whole environment up and running. In a later video, we're going to see how we can put things into the environment. In summary, we've just had a bit of an intro and the setup instructions for Pybullet. I showed you what Pybullet was. I showed you some demos of what the underlying Bullet physics engine can actually do with soft bodies, rigid body dynamics. Then we ran through the commands to install itand check that it's working. In this video, we've just been getting up and running with Pybullet."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Creating things in the world,https://learn.london.ac.uk/mod/page/view.php?id=96247&forceview=1,"[music]In this video, I'm going to show you how to put objects into your PyBullet physics world. In summary, I'm going to start by explaining the difference between collision and visual shapes, and then we're going to see how we can put a floor into our world, and we're going to see how we can drop a cube on the floor and then run the physics simulation in real-time. Okay, so let's start out by considering this. In PyBullet, we separate the look of a shape from the way it behaves when colliding. What does that mean?Let me explain with the use of a diagram over here. If I've got two spaceships like this, and I've got another one down here, maybe an attacking spaceship. What I want to do is I want to work out, is there a collision here?Imagine I'm trying to work out if these two things have collided, I might need to say, ""Have they collided there?""The computational complexity of basically comparing each of their coordinates to each other or whatever it is we're going to do, we'd have to look at--if we're fully computing all of the visual coordinates that we can see and whether they've clashed with all of the other visual coordinates of the other spaceshipis quite computationally expensive. It's not that difficult to program in a way, but it's just that it takes a lot of computation time and that might mean our simulation or our game runs too slowly. What we typically do is we simplify itand we have basically a bounding box, which is the sort of collision box. If the two collision boxes, those two bounding boxes overlap, then that means there's been a collision and then we do the appropriate thing. That makes the physics engineor the computation of when the collision has occurred much easier. We do the same thing in PyBullet. Even in PyBullet, which is a really complicated full-on physics engine, you can separately specify the visual look, so how things look, the geometry of how something looks from the geometry of how it collides, in order to have control over how much computational power you're going to put into your collision computations. That's just an important concept that you're going to need to understand what we're going to do in the next few steps. The next step is to put a floor into the environment. Let's start up the environment first of allby doing i. python and doing importing pybullet as p, and then we do p. connect and (p. GUI) to get a real-time GUI. There's my environment. I want to put a floor in. Now, the floor is just going to be a flat lane, which sits on the surface and other objects can then fall onto it and sit on it. How do I do that?Well, first of all, I need to specify the geometry for a shape before I can create the shape. I'm going to use that geometry twice, right?Once for the collision and once for the visual. Let's do floor_shape = p. createCollisionShape(). The collision shape, I'm going to use one of the built-in prefixed geometries, which is GEOM_PLANE. That creates basically a 2D plane which I can then place into the environment. How do I place into the environment?Well, I do floor = p. createMultiBody. Now it is a multi-bodyin that it can have multiple geometries combined and connected together in different ways. We'll see more about that later when we get into URDF. For now, it's a multi-body that only consists of one shape basically. It's going to have the floor shape for its visual geometry, and it's going to have the floor shape for its collision geometry as well. Then hopefully what we'll see is the environment now has a floor. You can see it's created this nice checkerboard kitchen floor for me, and I can stick my robots onto that, and they won't fall through, into the abyss anymore. Okay, great. That's the first step. Now, the next step is I'm going to create an object, and this time I'm going to use a different geometry. I need to create a new geometry for the object I'm going to create. I'm going to use geometry box, which allows me to specify a box instead of a plane, which is what I used before. Let's do that. I'm going to do box-geom = or box_shape, maybe= p. createCollisionShape. I'm going to use collision shape again. This time it's going to be (p. GEOM_BOX, ). Because it's a box, I need to specify the proportions of it, of the width, the height, and the depth. I'm going to do that with an array. It is half-sorry, just reminding myself, halfExtends=, and then it's going to be [1, 1, 1]which will give me a cube because it's going to have the same proportions. I've got my box shape. Now, as you might imagine, I now want to create my box. Let's get that out there. I'm going to get out the way so we can see it. To create my box, I'm going to basically do box = p. create. Can you remember the name of the function?MultiBody. I'm going to pass it box_shapefor both the visual shape and the collision shape like that. Then hopefully I've got a box. Great. I've got a box, but it doesn't look like a cube that is not a cube. That is a cuboid. It's a sort of rectangular cube. What's the problem?You can see that half of it is sitting below the plane. The reason is it creates the objects by default at the origin point (0, 0, 0). The center of the object that you create is at 0, 0, 0 in the origin. It's basically just being bisected by the plane. That's okay. We'll deal with that in a minute. The next step is to add gravity. I do p. setGravityand you set gravity by passing an X value, a Z value, and then a Y value, which is a little bit unusual. It's normally X, Y, Z, but this is X, Z, Y. X is left and right, Z is backwards and forwards and Y is obviously up and down. Because I want it to fall down, I do -10 on the Yso (0, 0, -10). I've set the gravity, but still, nothing seems to be happening. Well, kind of is sitting floor so maybe, but the final step is to tell the simulation to start running. I need to do set real-time mode. p. setRealTimeSimulationand pass it at one and I press enter. Oh, did you see that?The box lifted up into the air, oh, and it's falling down again. Now it seems to be balanced perfectly like that, which is interesting. What happened is the physics engine threw the box up in the ai rand then it fell down because it threw it perfectly up in the air. It landed perfectly balanced. Then because of little variations in the physics engine, a bit of noise in the physics engine eventually it toppled over. There we go. That's the physics engine running. Now one question is, why did the box fly up in the air?Why do you think?Well, actually the plane was interacting with the box so the box was cutting through the plane. The physics engine didn't like that. It doesn't like things going over each other that shouldn't be allowed to. It gradually pushed the box up and then shoved it up in the ai rand then gravity started acting on the box, pulled it down, and the box then landed. As I said, because it went up perfectly straight, it landed perfectly straight, but then because of a little bit of noise in the engine to stop things like that from happening, it eventually toppled over. That is it. In summary, what we've just been doingis putting objects into our PyBullet physics engine environment. We started out by just thinking about the difference between collision shape sand visual shapes, just to make that really clear while we were passing into geometries when we went to create the floor. We created a plane geometry for the floor, put that there. and you saw it created a texture for me, which was quite neat and it put some lights on, and then we put a cube into the environment which dropped down and fell onto the floor eventually once we turned the gravity on and started the simulation. In this video, we've been looking at how we can create objects in the PyBullet world."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: The URDF file format,https://learn.london.ac.uk/mod/page/view.php?id=96250&forceview=1,"[music]-In this video, I'm going to be talking about the URDF file format. First of all, I'm going to ask the question, what is URDF?and then explain why we're using URDFand we're going to load a pre-made URDF model into pybullet and see what that looks like. We're going to talk a little bit about what's in that model, how it's defined in the file, and then finally going to load multiple models inand see how that all works. Okay, let's jump into it. First of all, what is URDF?Well, it stands for Unified Robot Description Format. It's essentially a standardized format for defining robots. It's written in XML. It's an XML-based file format and you can specify motors and other dynamics. This is a really cool format, it's a really powerful format. It's like a 3D model format so similar to things that you might generate from a 3D modeling tool, like Blender or something, but it's got extra features or it's focused on the idea that you're defining a robot. It's got things like defining motors and connections between joints and how they can be moved, and all kinds of robot specialized features, and is originally part of the robot operating system project but now it's actually part of multiple other projects, like pybullet, for example. There's a link there to some tutorials. I'll provide that as a reading after the videoso you can go and look at that, but that takes you through the idea of building a model from scratch, which is pretty cool. Why are we using URDF?Well, as I said, it's a standardized format for defining robots. In the creature simulation, we're essentially evolving robotic moving creatures. It makes sense to use a standard format to describe them because otherwise, we're going to have to be using lots of calls to this, create multi-body and creating all these complicated data structures to define the models, when actually, there's a really nice, neat, very readable file format already out there. It's just basically easier to work with that file format. More importantly, as I said up there, we can more easily share these models. By sharing an XML file, someone else can load up your model. Once we get into the evolution, start evolving some interesting shapes, we can share them with each otherso that we can see everyone else is evolving in their system. It should be pretty cool. Moving on. First of all, let's see how we can go about loading a pre-made model. Before we do that, I want to just show you this pybullet data module because this gives us easy access to the pre-made models. If I just go into I Python, I'm just going to show you this. If I import pybullet data as PD and I'll do ""pd. getDataPath"". That file pathis where the pybullet library stores all of its data files. By looking in that folder, if I get that file path, you can basically go into it, and then you can have a list. Let's see what's in there. If I do a list and filter for URDF files, you can see these are all the URDF files. there are also sub folders with files in as well, but nevertheless, these are the top level ones. There's loads of them that come with the library. If you're working on simulating standard robots, and you want to do some machine learning, there's lots of standard robot URDF files available you can work with already. We're going to be generating our own custom URDF files using evolution, which is cool but for now, let's see if we can load all these files into Python and see how it works. I'm going to load in this R2D2 one. Now, what I've done is I've created a little starter file called starter. py. If I just show you what's in it before we do it, it's really just got those commands to just import the right things, and then to create a play on the floor and get things ready to go basically. That's just to save me having to type it in every time. From I Python, I can type ""run starter. py"", and it gets that all going for me. The next step is I want to try and load in one of these files. What I can do is I can do robot = p. loadURDF. I'm going to just specify the complete file path here, and I'm going to load in this R2D2 one. If I do that, that seemed to work. You can see that something's appeared here. Let's zoom in and investigate. You can see that I've got something. Again, it's that origin 000, so it's stuck in the middle of the plane, but you can see that's clearly some sort of familiar-ish looking robot. That shows you that you can very easily loada pre-made model in. Now, the next question is, let's have a little dig around in that file and see what it's got in there. Before we do that, I want to talk a little bit about links and joints so that when we look at the XML, we understand what we're looking at. Now, here's my drawing program. I'm going to trying and illustrate the structure of a URDF file here. You can see in that thing, it seems to have different objects in it. It's got different shapes stuck in there, various shapes and different things going on. Well, the way that this works in URDF iswe have basically things called link sand things called joints. Personally, I find it a bit confusing. It's yet another set of terminologies for graph type structures, loads, edges, whatever, but these ones are called links. I find the word 'link' a little bit confusing so just to clarify, a link is one of the parts of the robot, it's like a robot part, and the joint is a connection between two robot parts. We have links, which are basically the shapes, the robot parts, we have joints, connecting them. You can have as many as you like, you can have multiple things coming off. For me, this looks ideal for our robot. If you remember back to the video about how [?] directed graph works, it's a similar idea here. In the XML file, you're basically going to find essentially a directed graph, if you like, but it's done in a flat non-hierarchical way. What's interesting about it is you can also specify things like this angle here. What's the angle?What's the angle there?That's obviously in 3D, so it's specifying what angle the joint is at. It can be a fixed joint or a movable joint. You can have motors in there. There's all kinds of stuff and you obviously specify the distance, you specify what shape is it?What is it? A cone?A cylinder? Whatever. It's a really powerful description language and it can even load in external OBJ files, which if you've created a model inside Blender or something, you can load that in as part of your URDF file, but we're just going to be using primitives for our purposes. That gives you the idea. Links and joints. That robot is entirely built out of links and joints. Let's have a little peek at the XML file that represents that robot and see what we got in there. If I do ""more r2dr"". There we go. There's the XML. You can see that we've got a link. We got the XML tag, and then we got a robot tag with a name. We've got named links, and then a bunch of links. If we keep going, eventually we find a joint. The joint is connecting that link to that link. As I was saying, it's a directed graph that's specified in a flat format. That's the basic idea. Now, before we get into designing these things by hand, which we're going to do in the next video, I wanted to show you how far you can go with this. I just want to load in another model into our environment here. Here we go. Here's our environment. Let's just go back to the code. If I do this, ""= p. loadURDF"", and it's called that one. Okay, get ready. What do we got?We now got an R2D2 robotin this authentic-looking Samurai space. It's doesn't have texture mapping and all of that on there but it's pretty detailed, the model, you can see and quite impressive looking, but he's stuck there in the middle of it wondering, ""What's going on?""That just shows you, you can load multiple URDF files, URDF files can be really complex and it's pretty efficient. It loaded that in very quickly, and those URDF files actually reference outto other OBJ files, and so on. In summary, what we've been looking at is the URDF file format. I've been just introducing it really. I started out by answering the question, ""What is it?"" then, ""Why are we using it?""and then we've loaded a pre-made URDF file into our pybullet environment and display that as a 3D form. I've talked a bit about how it works, that it's a directed graph based around links and joints, and then we saw that we can actually load in multiple models to a single environment and have these really elaborate models if we want to. In this video, I've just been giving you an introduction to the URDF file format."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Write a URDF file from scratch,https://learn.london.ac.uk/mod/page/view.php?id=96252&forceview=1,"[music]In this video, we're going to write our own new URDF file from scratch. We're going to learn exactly how they work from line by line. Okay. In summary, we're going to talk about how we can handcraft a URDF file and we're going to start with one link and we're going to test that out in various ways, and then we're going to add set up two links with a joint between them. Okay. First of all, handcrafting a URDF file with one link. Okay. Let's just have a look at that so you can, if I bring up my code editor here, I should have that code. I've got a little bit of extra. We don't need that for now. There we go so there it is. We've got the XML starter there and then we've got the robot tag and we can give the robot a name and then we have a link. This is basically a robot that just has one part and it's this link, and then we've got a visual geometry for that link, which is a cylinder with a certain length and radius. Let's try this one out and just see what happens. ipython and I'm going to do run start. I've just added an extra line to my starter code. I've added this line here. Now, this is really useful when you're working with URDF files and testing them out because i t tells it if you don't set that to zero, every time you edit your URDF file and try and load it back in again, it will have cached it so it won't update to the latest version and that's really annoying. I'm glad I found that and told you. Anyway, so let's run the starter code and get our environment running. Okay and then what we're going to do is we're going to try and load that--so let's just verify we're in the right folder. There's the XML file there and let's load it in so we do rob1 = P. loadURDF file and it's going to be 101. urdf. It's that single link file and there we go, did a bit of complaining. I wasn't totally happy, but I did a fair bit of work to work out the minimum file that we can use. Then as I developed the creatures, which we are going to be developing, I'll show you how we do it in the next few weeks. We'll build out this more and more complicated URDF file, but that's your basic one, and you can see if we look in there it's a cylinder embedded in the plane there as we'd expect. If we do p. setRealTime, so we run the simulation in real-time. Now watch carefully, I'm going to disappear and zoom that right in. Watch carefully what happens when I start this. Sorry for the sound effect, but you can see it basically sunk into the ground and that was interesting, wasn't it?Because remember before, when we created the R2DT robot on the other, the cube, it flew up in the air, didn't it? It didn't just sink into the ground. Have a little think, have a look at that XML file again, and have a little think why you think that might be happening. What's missing from my specification of my 3D model there, if we think about visual and collision. Okay. Maybe you figured it out. There's no collision tag there, is there?I'm not specifying that it's possible for this thing to collide with anythingso basically what happens, it's just sunk through the floor because it can't collide with anything. How about if I update it, add a collision tag?Well, what we can do is we just make a copy of the visual tag and just paste it there, and I'm just going to create that, rename that to a collision like that. Okay. Just rename the visual tag to a collision tag and let's just tidy it up a bit so it's really clear right. There we go so now we've a visual tag, which is a cylinder and we've got a geometry, a collision tag, which is a cylinder as well. Let's try loading that one in as a new robot. We've lost, the old one. It's gone. It's just, it's sunk into the ground. Gravity's pulled it away. I could probably set gravity to like mine to a hundred and get it to shoot back up again, but I'm not going to bother. Let's do rob2 and we're going to load it in again and remember because I've turned off the file caching it should sit there. Okay. That's more what we expect, isn't it?Did you see what happened there?Actually, it did the correct thing, and actually, I can even pick it up because the simulation's running in real-time. I'm now picking it up with my mouse and I can drop it on the floor like that, so it's now properly colliding with that plane there and there we go. That's my basic one-link URDF file, and so that's great. I'm happy with that. My next step is that I'm going to add, I've done that. I've had the collision now I'm going to go for two links. I could at this point, add an inertial tag, actually I'm going to do that. Let's go for the inertial tag. Why not?Because we notice when it loads, this one in, it does complain about something to do with inertia. If I put that inertial tag in, I'll get rid of those error messages, which is always good. Let's go here and see if we can find the inertia tag because I don't have to type the whole thing in because I'll probably do it wrong. Okay. Here's our inertial tag and I'm going to go back to my 101 file, which is this simple file with just this stuff in and it's again, inside the link tag and the inertial tag is, as I say, it's generating these error messages. It allows me to specify the inertial properties of this link so each link in the robot can have different inertial properties. You can have heavy ones and light ones, so you can get really complicated model. Okay. This one I've set the mass to something, otherwise it defaults to one or something like that. Then this stuff, all these numbers, I got those from the manual. I have to be honest. I'm not totally sure what they all mean, but I think it's to do with the way that it expresses its inertial property so you can basically see I'm setting inertia on the X, Y, and Z access to all being the same thing, which makes sense because we want it to move in a similar way in all axis and then there's a receptor zero which is what it says to do in the manual. We don't need to worry too much about t hat because it works for now. Okay. That's my updated 101. Let's make ourselves a robot number three. Let's see if we've got rid of the error message. Oh wow. Okay. Did you see the code over there?I've got rid of my dreaded error messages over here now. You see, when I ran the code it didn't produce any error messages and I've got a happy thing. It's probably a bit lighter than the other one because I think it defaults to one and there it is. That's another robot in my space now. What's next, I've added my inertial tag. Next up. I can try having two joints, two links, and a joint between them. Remember we can have as many links which are robot parts as we want and we can join them together with joints. Here, we have to specify joints so let's go back into our 101 file. I'm now going to make a complete copy of this link, and so that I've got two, but I have to give it a different name. I'm going to call that base_linkand we'll create another one call it maybe sub_link. Okay, we call it sub_link. It's got all the same properties. Actually, let's make it really obvious that it's different by making it into a kind of weird long cylinder that's very thin. A thinner cylinder so we can see that difference in the shape. Okay. That's my sub_linkand then I need to because I've got two links now I need to have a joint to join them together. If you look here, you can see I've got this kind of joint tags. I'm going to pull that out of the XML and copy paste it. I've got this one here, and there's the joint tag. That's the minimum joint tag and we can try with that one and see if it complains. There's a minimal joint tag and I'm setting the type to fixed at the moment. We'll talk more about that later so let's call it base to sub and it's called. Remember the first-- I need to know the names of my links. I specify so base_link is the first one and sub_link is the second one. The parent is going to be base_link and the child is going to be sub_link. Let's try loading that one and rob4--let's make sure we can see it before it disappears off. Okay. There we go. We can see we've got another robot now. Let's see if we can get a bit of view on that one. You can see it's got the-- You see the extra cylinder has appeared there. You can see it's got-- I didn't get out the way. You can see it's got the cylinder, is there. It's got the little cylinder in the middle so I specified a thinner cylinder of the second link, but it seems to just sort of overlaid, it's just sitting on top of where the other one was and so that's where I need my extra link properties. If you look here, I've specified the extra link properties, which isgoing back to a diagram from earlier. If I've got a link like that, whoop let's do it with a normal pen. The idea is I've got two links and they're in 3D and I need to specify the distance between them and then the 3D, the angles to which direction they point in and that's what this code here does. I need to do access X, Y, Z. Let's just type that in. We're going to do, and that's all inside the joint tag so it's access X, Y, Z equal and it's going to be 1, 0, 0. Let's try that, 1, 0, 0 okay, and then end that tag because we have to end our tags because it's XML not one of these easy-going HTML5, we have to end our tags, right?Then we've got a limit effort. We're going to go limit with effort, lower, upper and velocity, so we're going to put a limit tag in. Limit with effort, upper, lower, and velocity. This is specifying really the behavior of the joint and how it works as if it's a motor. Let's just put the numbers in, so 10, 0, 10, 1, so 10, 0, 10, 1. This is to do with how the motor works in this joint, so this is where we get into the robotic stuff. The effort, I believe, is the maximum effort that you can apply to this motor, so how powerful is the motor. The upper and lower are the range of effort, so basically you can set the motor at different speeds effectively and the velocity I believe, is the range of speed. Effort is the amount of force we can apply, velocity is probably the maximum speed it can go to, the upper and lower are the range of the effort, I think. [chuckles] The documentation is a little bit tricky but I encourage you to go and read it and my get out of jail free card here is, basically because we're doing evolution and we're evolving these robots, in a way we don't really need to know exactly how all these numbers work as long as it accepts our URDF files. The evolution will solve the problem of coming up with the right numbers. Next one, we're going to put an origin, this is the important one, this is the one that dictates how it is in relation to the other part. The axis is, the direction and the origin I believe, is how far away it is. Origin RPY and XYZ, so origin RPY and XYZ and let's end that tag and it's going to be 0, 0, 0. That's the rotation from the-- this is the rotation, R rotation PY, so how much-- what angle it's rotated at from the parent, so this is going to just say, ""no rotation. ""Then 0. 5, 0, 0, so 0, 0. 5, 0, so that's the-- so I guess that's the Y, so that means it 's going to be slightly white and interestingly, XYZ are in a different order again than they were in set gravity but that's just the way it is, sorry folks. That's the problem when you mash together different frameworks and different libraries, especially when it's 3D stuff. They have different ways of specifying what the order of the things are. Anyway, let's stick this one in the space and see what it looks like, so I put that over there, so this is going to be rob5. Oh, there it is. Oh wow, where did it go? Let's go and get it. There it is, that's my new robot. You can see, if I get out of the way, you see I've got two robots now. I've got the first version, which is when I didn't specify the extra data on the linked, the joint tag and then this is the one where I did specify the extra data. You see it's moved across slightly from the parent because of that origin tag which moves it across, so you can specify where the other link is, in relation to the base_link, so that's there. What's next? That's more or less it [laughs] actually. I'm going to just give the summary now. What we've been doing there is we've just been exploring. I encourage you to explore this. The reason I do it in this slightly clumsy way is, I think it's really-- it just shows you the practice. This is how I do it, I will work through and I'll just play around with it, and go, ""What happens if I add a joint there? What does that number do?""And adjust that number and see how it looks different. What's really nice about this PyBullet environment is, you can do that you can iterate on the file and reload it and create a new-- you're not with these weird robots lying around, so it's a fun way to experiment and explore. Please go ahead and do this yourself. What we've been doing is we've been handcrafting a URDF file. We started out by with one link, and then we had two links, and we added inertia properties and collision boxes and then looked at the interaction between the joints and how you can place them between the links and how you can use a joint to place them apart. In this video we've been playing around with hand making a URDF file and exploring the different parameters there and what they do. I'm hoping that you're going to go off and do some exploration yourselves and have fun and perhaps post some of the results in the forum for others to look at."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Moving a URDF robot,https://learn.london.ac.uk/mod/page/view.php?id=96257&forceview=1,"[music]In this video, we're going to be looking at how we can move our robots by setting various properties on the joints. In summary, we're going to talk about different joint types, first of all, and then how we can get information about joints, how we can move them, and we're going to see if we can add more joints and move multiple joints, and at the end, we might see if we can move the joint son one of our preset robots to see how that works. First of all, different types of joints. There were three joints available in PyBullet, but URDF actually supports more joints than that, but they just aren't currently implemented in PyBulletbecause they have more than one degree of freedom. The different joints are revolute. Revolute is basicallya joint that can rotate in one angle, so it can in that angle or that angleor that angle. You can rotate in one angle at a time, but that's not really much of a limitation because could you have multiple joints each of which is allowed to rotate at different angle sand that would give you a really flexible kind of movement, so it's not really much of a limitation. Then you've got prismatic joints which move along a single axis. They might move backwards and forward sand that's more for say you're building industrial robots like a thing that slides along and picks things up or whatever. Then the other type of joint is the fixed joint and that one that doesn't move. Just to clarify, in the manual for PyBulletat the time of filming at least, it says, ""We only support one degree for either motorized joint sat the moment;sliding joint and revolute joint, ""and the sliding joint is a prismatic joint. That's different types of joints. Let's get on with it and see if we can move something. I'm just going to put that one over there and I'm going to put the animation now. Just to show you I've added one more command to the starter file here which is this cooling configure debug visualize rand I'm setting the enable GUI flag to zero. Now, you'll see what difference that makes when I run it. I'll run starter and let's just pop that over there. You can see now over there that I've got rid of the panels on the GUI just so that we can see more of the robots. How are we going to move a joint?The first thing we need to dois just to check the XML file we've been working on. Previously, we had fixed here. We had a fixed joint there which is not a lot of use because we won't be able to move it. I'm going to send it to a revolute joint because that's the type of thing I want to use. I want to be able to have a natural-ish robot arm-type things. Revolute, so we send it to that, and then we go back to the code and what I can do is I can now load up a robot using that specification. rob1 = p. loadURDFand it's 101. URDF. Run that. Oops, sorry. There's my robot over there, and I'll just zoom in a little bit. Now what I want to do next isset the simulation to run in real-time. Yes, she does, so you see the robot flew up over there. What I can do is I can actually reset the position of the rob otto the center of the screen. Reset. Yes, that's it, reset face position and orientationso I can put Rob1 back where I want him. You'll see the numbers I'm putting in there. This X, Y, Z, so that's the position. I'm going to put the robot slightly in the airso it falls downand we'll see what that does. This is the orientation which is a 4D vector. It's going to do that like that, and so you see that drops it may be a little bit higherso it doesn't fall too far from the center. Here we go. I can run that command to reset the position of the robot. Now let's zoom in a bit and see how we can go about, or I can drag it. Let's zoom inand see if we can move that joint. The next step is to setsetJointMotorControl2. Now, there's an old version of this which is without the two and that doesn't work or is deprecated so we don't use that one. This takes various parameters. Now you have to look up in the manual what the parameters are here. I'll illustrate some of them for you actually. The way I learn this stuff is by reading the manual, of course, I didn't just guess. Let's put the commands in. The first option is the ID of the robot, so that is going to be Rob1. Oops, let's get it to the right. We know Rob1. Right, Rob1. The second argument is the idea of the joint. Now it's only got one joint, so that means it's going to have ID zero. Each joint is zero, one through the number of joints minus one basically. That's Rob0, and then we need to do is say what type of mode it is, so the control mode parameter. This is the manual here. The control mode parameter and I'm going to do velocity control for no wand then we're going to look at position control in a minute. We do control modeis p. velocity control, so controlmode=p. VELOCITY_CONTROL, and then I can set a target velocity. I'm going to set this to 0. 1. Let's look at that robot and see what happens. Can you see that it's slowly moving now?Let's make it go a bit faster. You can see over there, You can see that the robot is now moving. It's basically rotating that thing. You can see that the joint is got complete, it can rotate all the way around. I might want to constrain that, but I'll not bother at the moment. That's what velocity control is. Basically, you can set the speed that you want it to rotate at, you can rotate really fa stand you can see if you've got a nice fast-rotating thing, it's actually walking, so woo the evolved walking creature. What about if I set it to -5, is it going to come back?Where's it gone? Yes, there we go. It's a basic I've invented the wheel kind of. Remember, I can always reset it back with this command. There we go, dropping back. If at any point you want to stop it from rotating, you can just set the velocity to zero, and now it stops. That's the velocity control. Basically, it's like you've got a speed control on the motor and you're just setting that. You can see it actually isn't constrained to a maximum, so that allows it to actually really exploit the physics engine and spin around. Let's put that backand see if it falls back down to earth again. There we go and then bring it back. It's now moving slowly again. What about position control?That allows you to essentially tell it, ""Okay, I want you to move] to a specific positionat a specific speed. ""If I do position control here, and then a velocity. Also, I need to set a target position, so I need to get another one of these arguments which is the target position argument. I need to specify that as well, target position because otherwise, how does it know where to move to?Let's do that, 0. 5 like that and see what that does. The velocity seems to be a bit too fast. Let's just reset it again. It looks like these target velocity, maybe I need to switch it back to target velocity zero, reset it, and then maybe set the position control using that. It's moved in the right position. Let's go to 0. 5. Oh, it's a bit slow. Doesn't seem to be doing a lot. Let's try five. Yes, there we go. You can seeit's moving to a specific position and that isgoing at a certain speed. That's the position control, it allows you to move it to a specific position more like a server type of deal rather than a locomotive that you switch on and off. That's probably enough playing around. You can obviously go and experiment with this yourself and try out different speeds and so on. The next thing ishow do you get information about joints?How do you know what a joint is capable of doing or what the properties are?That, we can do P. getJointInfoand we pass the name of the robot and maybe a joint and you can see the ID of the joint, so Rob1 and 0. You can see I get a bunch of data back here, and it tells me what the name of the joint is. This one is interesting. This tells me what the type of the joint is. If we check what a zero is, that will be p. JOINT_REVOLUTE. I think you find that type zero, yes. The other one is the PRISMATIC 1, which is type 1. That's the type. If you look at the manual, it will tell you exactly what each of these properties are. It's to do with the maximum speed of itand other constraints on the joint there. That's just how we probe a joint. Moving on. Let's say you wanted to have some more joints. Let's see if we could quickly update this fileto add another link and create ourselves another joint. We're going to have sub_link2, and this one is going to be maybe a great bit long, one very thin and long, and then this one is going to beconnected to here. I need another joint because I've added another link, and sub_link2 since it's going to be baseline to sub_link2, and whatever. That's all fine. Let's just do that. Let's load in Rob3to the environment. rob2 = p. loadURDFand it's going to be ('101. urdf'). Yes, something wrong with my URDF file. ""Warning, joint is not ununique. ""I need to come up with a unique joint name. There we go. I just changed that. Let's try that again. Yes. Hopefully, we've got our Rob2 in there. Let's try resetting the position of that one, reset position and orientation. I'm just going up back through my commands here, so this is going to be Rob2. Really?Where is my Rob2?Oh, there it is. Right, there is Rob2. Now, that one's got two joints. You can see I've got the two different things. Let's see if I can do that. If I move Rob1 out of the way. Sorry, this is a bit of a slow process, but I'm clumsily moving these annoying robots around. Let's get that one right out of the way. Right, there we go. Shove that out of the way. Now, I can concentrate on this one. Now, I've got two joints, so if I set a joint position on Rob2, so let's do VELOCITY_CONTROLso we can basically set them both spinning around, right?VELOCITY. We don't need a target position on VELOCITY_CONTROL, so we'll set the velocity to 0. 5. Okay, so one's moving, and then let's do the other one. I need to change the joint ID, which is the second property there. Now, we've got two joints moving, so let's see if we can get a better view of that. Oops. There it is. Now, it's got two joints that are moving, so you can see, thinking ahead a few steps, we're going to be evolving these properties and how the joints move, and things like that and also what the shape of the things are and how they are in orientation to each other. You can see, they're already getting some interesting variation here around what we do with the different joints. I think I'm going to end the video here because I think this is a good point to end. We've seen how we can basically use the various functions built into PyBullet to configure the joints of our robot using a URDF file, and then switch the motors on and move them to different positions. We've seen, as you can see behind me there, that you can actually, very quickly, just a couple of joints, a couple of things just start getting some interesting moving behavior. That's exactly what we're going to get our genetic algorithm to explore, the space of possible configurations of link sand shapes and different motor controls, and we'll see whether we can evolve some interesting moving behaviors. Let me just quickly jump to my summary. In summary, we've just been looking at different joint types, we've been looking at how we can get information about joint sand how we can move the joints using positional moving and velocity moving. We've added extra joints and moved multiple joint sat the same time, and you can see that you get these interesting, weird movements happening very quickly, and that's more or less it. In this video, we've just been exploring joints, and links and move different types of movement and different types of joints."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Moving a preset robot,https://learn.london.ac.uk/mod/page/view.php?id=96259&forceview=1,"In this video, we're going to load up one of the pre-set robots that comes with PyBullet and move its joints to see what happens just as a sort of experiment. What we're going to do is, first of all, look at getting the joint info, so we're going to load the model and get the joint info, then we're going to iterate over the joint sand see what we can do with them, and then we're going to switch on all the motors and see what happens. It'll be a quick video. Remember, we've got these two functions, getNumJoints and getJointInfo. What I'm going to do is, I am going to fire up my environment here. I found that there's an interesting robot model called the humanoid robot. I'm going to load that one in. We can see that-- there it is, it's not really obvious, it's kind of half a humanoid at the momentso if I do set real-time on here, there we go. There's our humanoid robot. You can see, it's kind of got the legs, and arms, and head as you'd expect. Now the question is, how many joints does it have and can we move them?If I do p. get a number of joints, it gave me lots of warnings, it's not that happy. It's Rob3. Number of joints on rob3. Let's see, how many joints does it have?It's got 15 joints, and if I iterate over those joints and print out the types, remember we've got these joint types which are JOINT_REVOLUTE, and JOINT_PRISMATIC, AND JOINT_FIXED. Those are the available joints so if we iterate over the number of joints, which is 15, and I do print(p. getJointInfo(rob3, i)and I'm going to filter it down, I'm just going to select the--You can see I'm going to do the select of just the third item in the array, which is the joint type. Hopefully-- yes, there we go. We can see that's printing out of those 15 joints, all the different types that we got. We got some fours, twos, and zeros. Four is the fixed, there are a few fixed ones in there. There are some twos as well, I don't know what twos are, but they don't-- [?] supported. Maybe that's a prismatic joint or something. Possibly, it's what those warnings were about, if we go back, probably not too happy. Anyway, that's my model loaded in. Those are my joint infos. Now, how do I go about actually switching the motors on?Well, I'm just going to iterate over them and just dop. setJointMotorControl2, and I'm going to do Rob3, which is the name--idea of my robot, the joint id is going to be i, and then the control mode, I'm going to set it to velocity modes. I don't know if you can remember what velocity mode means?We've got position mode and velocity mode. Velocity mode is when you just basically switch the speed on the motor, switch the motor on and leave it running. I'm going to do p. dot velocity control and target velocity. I'm going to set it to something like reasonably not too high. That code there is going to iterate through all the joints in the robot and switch them on, hopefully. Something's happening. You can see, the arm seemed to be moving but not much else. Now, interestingly, you'll notice it's kind of moving that way. Maybe there are some constraints on the legs to prevent them from moving because obviously, my arms don't go that way, do they?They only go that way. These ones arms are able so if I set the velocity to -0. 25 instead, like this, hopefully the legs will start moving. Now we got some interesting stuff going on. It is now bending its legs and doing some weird stuff there. You can see that we can basically switch all of the motors on and things starts kind of articulating like that. I'm guessing that the spherical joints, on the arms there, on the shoulders, and possibly on the hips, which is why they're not moving how we'd expect because you have to move them in a different way. Anyway, that's really what I wanted to show you, to be honest. That's plenty. That is just a quick demonstration of how you can control a pre-set robot by sort of getting the joint info, having a poke around, seeing what kind of joints it has iterating over the joints, switching all the motors on and just trying different directions to see what it doe sand obviously, you can go and try out some of those pre-set models to see how well they work for you. Just in summary, this video we've just been looking at how we can switch all the motors on in a pre-set model and how it then moves."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Lesson 2 summary,https://learn.london.ac.uk/mod/page/view.php?id=96264&forceview=1,"[music]-Well done, you're working really hard here. You've reached the end of the first real serious programming section of the creatures case study, which is week two. What we've been doing this week, we've been experimenting with the URDF file format. We basically found out how we can use URDF files to create robotic structures. We've also been implementing online simulations where we can run a creature in a 3D environment and see it moving around. We've also found out how we can build a basic evolutionary system for it to evolve morphology. We've got a plan there in place, and we're going to be working towards that plan in the next few weeks. Well done for reaching the end of the second week of the creatures case study. We look forward to seeing you next week where we're going to really roll our sleeves upand do some serious programming and start building our genetic encoding so we can generate our own random robot forms from random genomes."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Lesson 3 introduction,https://learn.london.ac.uk/mod/page/view.php?id=96266&forceview=1,"[music]Welcome to week three of the creatures case study. If you've got this far, you're doing really well, you've got your PI bullet system up and running, and you've been able to create some creatures in the world. What we're going to do this week is really go to the next level, and allow ourselves to create creature sin a highly programmatic parameterized way. What are we going to achieve this week?Firstly, we're going to be able to explain how solutions to a problem can be represented using a genetic encoding scheme. Well, and that sounds a bit interesting so what is the problem?Well, the problem is, we want to know what kind of three-dimensional structure we need to design a robot, and we want to also have motor controls for that robot so that it move sin a useful way such that it can get as far as possible from the starting point, that's our problem. We're going to have to figure out how to encode solutions to that problem in a way that's amenable to the evolutionary process. That's the first thing we're going to achieve. As I said, we're going to implement a genetic algorithm, a genetic encoding scheme, in order to do that so we'll understand the basics of how to encode things genetically, and we're actually going to line-by-line implement a genetic encoding scheme to describe these 3D robots in a way that can be evolved. Thirdly, once we've developed our own genetic and coding scheme and seeing how it's built from scratch, when we come later in this case study to look at some state of the art systems, we're going to have a really much stronger insight into how those systems work and how the encoding schemes work, because we've built one ourselves from scratch, so we know exactly the details. I've got a little screenshot for you here, so this is just a little taster of something we're going to be seeing this week. Previously, we've seen these ready-made and simplistic robots embedded in 3D environments. You can see this is clearly more complicated, and actually, it's moving as well, but I'm going to hold that one back as a cliffhanger, you're going to get to see these random, interesting shapes, actually moving around robotically in the world at the end of this week. Welcome to week three. Good luck. There's a lot of video content this week, so you're going to have to focus to get through it, but feel free to set it on 1. 5 speed, speed it up if you need to, to get through those videos quicker because I do cover all the steps again, in the lab worksheet so you won't miss anything if you skipped on the videos if you find it's a bit too much, so that's fine. Just go to the lab worksheets, but I hope you do watch the videos because there's lots of insight sand little details in there that I put in as I work through the code."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Genetic encoding problem overview,https://learn.london.ac.uk/mod/page/view.php?id=96269&forceview=1,"[music]-In this video, we're going to have an overview of what the problem is we're trying to solve with our genetic encoding approach. Let's just look at the summary. First of all, we're going to have an overview of the problem, and then we're going to dig into the URDF file and think about joints, links, and then we're going to think about the control system. How are we going to operate those motors in the creature. Here is a diagram of the problem, and you can see that we've got various stages here. This bit here, this is our genetic encoding. That's the substrate for the genetic algorithm in a way. That is the list of data that it's going to be manipulating. That's the first thing, and it might be zeros and ones, or it might be floating points. Probably we're going to end up with floating points, but for now, just think of it as a string of values. Then the next step is that we're going to take that and we have to somehow convert it into this directed graph structure, which is the creature. Each of these represents a link and then each of the connections represents a joint and we can have multiple repeats of certain parts of the tree or the graph to give it that repeating structure that we saw in the Sims paper and so on. That's somewhat complicated. Then after that, we need to somehow get that and convert it into a URDF file. We're going to do it that way round. We're going to come up with this interim representation, which is this data structure, but then we need to bounce that down to a URDF file because that means we can easily load it into the simulation and test out the robot. Once we've got the URDF file, as I said, we can load that into the simulation, and then we'll actually get our 3D creature rendered in the environment. Now there's one other step that I didn't really mention in here, which is the control. We're going to talk about that at the end as well. How are we going to switch the motors on and off?Because there's nothing in the URDF file that explicitly says, this is what the motors do over time. It says what the capabilities are of the joints, what direction they move in and it says the maximum velocities and things like that, but it doesn't actually contain data about the control. We need some separate thing here. We're also going to need a separate thing, which is the control system. We're going to need that as well. That's probably going to come directly from there. When we're running a simulation, we're going to use a control system. This is similar to how Sims did it. He had his representation of a 3D morphology of the creature. Then he also had a separate thing, which was the control system. He actually had basic--Simple neuro networks and signal generators to do that. We'll follow along on that, but possibly in a slightly simplified way. What's next?Well, I said we're going to talk about the URDF file. If we're going to figure out how we're going to encode this directed graph structure, I figured that probably if we look at the URDF file and work out what's in there, that'll give us some really good clues about what things we're going to need in our genome. Because ultimately, it's going to be generated out into this XML format. If we're looking now, we can see, okay, what would be interesting to vary?We're going to be looking at this XML file with an eye to, ""Okay, I need to create lots of variation. I need this genetic algorithm to have a nice large search space in which there might be interesting structure sand lots of variation. When we're choosing which things we want to parameterize, we have to choose based on that idea. Let's bring up the URDF file here. The first thing I wanted to look at is the links. If we go and we grab one of these links, there's the XML for a link. What have we got in there?We have the visual geometry. That's clearly interesting. To simplify things, let's just treat division and the collision geometryas the same because that just means we can have repeated tags basically for that. If we dig into that, what's the variation that's going on there?Well, obviously you can have different primitives. This one's got a cylinder, so maybe we could enable different types of primitives, but maybe we could just stick with cylinders. That probably would be enough. Then we've got a length and a radius, so it dictates what the shape of that is. Clearly, that's something we want to be able to evolve, the size of the different components that make up the creature. That's definitely going to be a target for our genetic encoding. What else have we got in here?We've got a mass value, which it makes sense for me, the idea, the bigger the component is, the bigger the link is, probably the higher the mass should be because we don't want to have the robot able to just have whatever size it wants without any mass requirement on that. I think we can probably set the mass based on the size of the cylinder in some way. That's the links. That's the first thing. You can see we've come up with maybe one, two, three, four, four parameters or so for the links, which is pretty simple, but there's one other thing that we need to know about the link sis how many times they connect, how many repeats of that link do we have?Remember in the Sims thing enabled this recursive thing where you could have, for example, four same-shaped legs hanging off of one central body. In order to enable that kind of thing, we're going to have to think of a way of repeating because that'd be another parameter. It's probably about four or five parameters for these links. That's not too bad. The next thing you might want to look at is the joints. The joints connect the links together. Here's a joint, and this is where we'd probably get quite a few more parameters. First of all, we've got the type of the joint. Now, remember in pie bullets, we've got revolute joints, which are these ones that rotate with one degree of freedom and we also have sliding joints, these prismatic joints, and we've got fixed joints. Maybe we can simplify a bit and just think about revolving joints and the fixed joints. That's probably enough. That's one parameter, and then we've got this other thing, which is what does the joint connect?For a given object, which other links does it connect to?If we do a quick diagram of that, if we have something like a body part there, we've got a body part there, and they've got a whole bunch of other body parts, and they're all maybe connected together, and the question is maybe we want to have the ability if we mutate it. It might be connected to that one, but we might get a mutation and end up connecting it to that one. We need some way of parameterizing this factor here. What is the parent and child link on a given joint. That will allow us to potentially mutate itand have a whole piece of the creature, suddenly appearing somewhere else in the creature, but being the same of sub graph, if you like. Then we can parameterize that. Then what else have we got?We've got the access thing, which is we can potentially parameterize that--Where the one goes. Then we've got this one is really interesting. This is to do with basically which axis the motor rotates around. Also when we've got the two links, how far are they from each other and which direction are they offset from each other?It's really just really defining the structure of the creature. Those are very important parameters there. We've got probably six parameters we can mutate there. That's six, seven, and then maybe we can put some stuff into the constraints of the joint or we could just have them all being the same, might be easier. We've got about 14 parameters or so, so far, and that's pretty good going. Now, what's next is the control system. I mentioned earlier, we went through the Sims paper and we had a loo kat the control system there. Sims had a set of inputs, which were sensory inputs, which might be things like the position of the joint, so you might have sensors that tell you what angle the joints are at. We fed that into an evolvable network system with signal generators and things in there. Then that was then fed as controlled signals to effectively the motors in the creatures or the muscles if you want to call it that. Now, that's somewhat complicated. You have a separate encoding for the structure, the morphology, and for the control system. I'm going to maybe simplify it a little bit, instead of having full sensor control, sensor feedback control. What I might do is just have some signal generators that can generate motor control patterns over time. I've just shown here that when we're running it in simulation, we're going to have to be running our control somehow manually to control what the motors are doing. Just to draw a diagram of this one, what I'd like to do here is you got two link sand you're going to have a joint between them. That is going to have some sort of motor controls. What I want to do is have a signal generator may be for each motor, which is going to generate a signal, which controls the velocity. This is going to be the velocity. If I vary the signal, then I can, maybe--If I parameterize it, I can have slow ones, I can have really ones that give you a very high velocity over time. You can have all kinds of signals and you can even have different waveforms. You could have a more of a pulse train where at one point, the velocity is maximum that way, and then it switches to maximum the other way and so on. You think about how that would work, the pulse train one, that would be a thing that swings from side to to side. Velocity would be going like that and then suddenly it would switch to being like that. That would be the pulse train type of thing. Whereas the sign wave one is a bit more difficult to emulate with my own muscles but it's basically accelerating isn't it?It's accelerating and easing in and easing out. You can see--Imagine a creature with a whole bunch of limb sand it's doing these different types of movements hopefully that will allow us to have a huge variety of different ways of generating locomotive movement. What we'll do is we'll have some parameters which control this. You might have a parameter which is the multiplier on the amptitudeso we can scale it up or downand we might have a parameter which is the frequency there. How rapidly does it oscillate?We might have another parameter which is whether it's same waveor a pulse wave or maybe even we could have some linear ramps as well. We've got a whole bunch of options we can do on there. We'll do that, so we'll have this control system which is really just parameterized signal generator to send pulses to the muscles to tell them what to do. That's the overview of the parameterization plan. We've talked about that. What the overall problem is getting from some genetic representation that we can use as the substrate for the genetic algorithm and then converting that through various stages into a running physical 3D model or virtually physical 3D model inside the simulation that's moving and has controlled muscles over time. We've gone through the URDF file, joints, links, and we've thought about controllers at the end there. In this video we've just been having a bit of a brainstorm about how we might go about parameterizingthe design of a creature such that it has a huge variety of morphologies that are possible but also a variety of control systems for how it moves."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Deciding on the parameters and their ranges,https://learn.london.ac.uk/mod/page/view.php?id=96271&forceview=1,"[music]In this video, we're going to roll our sleeves up, I've already got mine rolled up. We're going to work out what the actual parameters are going to be for my GNOMEso that we know when we start programming it, we know how we're going to turn the GNOME into actual parameters for the creature. In summary, I'm going to be talking about the parameters for a link. I'm going to talk about the parameters for joints, and parameters for the control system. The Links, let's start with that. I'm going to do it all in my text editor, actually. What I want to do is just basically end up with a list of parameters. Now, the first thing I'm going to say is, I've decided that I'm going to use real values in my GNOME, so I'm going to have floating-point values in the range zero to one. The GNOME itself is going to basically just contain random numbers between zero and one. Let's start. The challenge here is to get from, say whatever it is into an appropriate parameter, the thing I'm talking about. The first step is to get a list of all the parameters I want. Then I'm going to think about what their ranges might be. Let's look at this URDF file. Now it gives us some clues about what we need here. The first thing we want to parameterize is the links. The key thing that we talked about in the links is the really the geometry, so it's this tag here. We need to parameterize the geometry of the link, which means we need to know the length, the radius, and possibly what the shape it is. Let's come up with a parameter one or zero because we're computer scientists so we start with zero. The shape parameter is going to encode, whether it's a cylinder or a box. It's basically our two values. Then we're going to have a length and a radius. We call it link-length. Let's prefix them with links so we know they're link parameters. That's really clear then. Link-length. As I said we're going to come up with ranges for these later so link the--What's the other one, link is radius. If it is a box, then maybe it has a size, so you could use that as the width and the height of the box or something, but that's okay. We'll just call it that for now, it doesn't really matter what we use it for in the end. That is more or less the link parameterized because the things going to be built out of multiple primitive objects. They might be cylinders or boxes, and they have a couple of parameters controlling the size. That's that. Then the only other thing with links is, remember they have this sort of directed graph thing, where they recurs into each other, so we might have multiple instances of a link. What I'm going to do is have another parameter which is called recurrence, which is-- Let's call it link-recurrence. We'll think about the range of these in a minute. That's really my link parameterized. Then what about the joints, well let's have a look at those. These have got a few more parameters, haven't they?Because with the joints, you can do things like--Oh, one thing I spotted that we missed is mass. Whoops, forgot about the mass. We could just have a parameter for that, but we might end up throwing it away later on. I'm not holding myself to these parameters. When we get into programming, we might throw some of them away or create new ones, that's fine. Let's put mass in there for now, because we could potentially assume that all of the material that the creatures are made out of has the same density, and therefore we can calculate the mass, based on the size and the shape. For now, let's put it in as a parameter sowe can have different densities or whatever. That's cool. Next one we want to look at the joints and how do we parameterize a joint. We have the first thing to think about with the type. What type of joint is it?I'm going to fix some fixed joints and revolving joints. I'm not going to worry about these prismatic joints because I haven't really worked with those and I don't really need them. I'm just going to say 5-joint-type. That can be either revolute or it can be fixed. Then number six is going to be the linking. We have a parameter with a join which dictates which two links it's connected together. If we have to draw that, I've got my drawings here. If we have two links, so two body parts, let's say we've got a whole lot of body part sand we're parameterizing which one's connected to which. The link would say, ""Oh, which one are you connected to?""They can connect to whichever ones they like. If we parameterize that potentially that means we could mutate it at some point, and you might end up going, ""Okay I'm going to now connect to that one instead, instead of that one. ""By parameterizing it, it becomes mutatable. Remember the objective here is to come up with--We're inventing the space here. We're inventing the space of possible morphologies and possible control systems. We're trying to make a nice rich space with lots of possible variation. If we parameterize which link is connected to, that's great. That's obviously a good idea. We'll call it joint-parent. That will be zero-- I'll talk about the range afterwards. Let's carry on. It will be one of the available other links basically. Now let's go to next one. Then we've got this axis xyz thing. It'd be good if we can choose which of those we have a one in. If we could have three options there. We could do 7-axis-xyz, which going to be three options, which are going to be 1, 0, 0 or 0, 1, 0or it's going to be for completeness 0, 0, 1. Those are the three options for that. That's three options. Next step is that we got this limit effort thing now. Have to be honest, when I was experimenting with PyBullet, I found that these things didn't really control what I was allowed to do with the motors. I assumed that a limit tag and maybe I'm using it wrong, but I assumed that the limit tag would say, ""Okay, you can't have a motor on really powerfully because we'll block it, ""but it doesn't seem to actually affect it, so I tried different values in there and still I was able tohave really fast-moving motors with high force. I'm not sure if this the limit tag is the correct one to use for doing that so I'm going to ignore that. I'm going to go straight into the origin. The origin tag has six parameters in there, so we're going to add another six parameters for rpy one, two, and three, and xyz one, two, and three. Remember, rpy is-- we got to do the rot--where the motor rotates and where it was placed relative to the road to where the link is placed relative to the parent link. We're going to have six parameters there. We'd call 8-origin and we'll call it rpy-1which is going to be zero to one, probably, or there might be zero to two pi because it's kind of a rotation, isn't it?Let's call it-- I said we weren't going to do the range yet but now we are. That's all right. To find the right one we'll come back to this. That's one, two, three. That should be 9 and 10. Then we'll do origin-xyz, so origin-xyz-1. That's the distance sort of a vector defining a difference between the position of the parent link and the child link. I think we'll set that to zero to one for now and we'll come back. That can be one, two, three. That is all the parameters my links enjoy. Let's quickly whiz through and think of ranges for those. That's obviously got a range of, I see the one or the other. It's kind of a binary value that one. That one link-length that's how long it's a piece of string, how long is the virtual 3D cylinder. Let's just 0-1 it for now, and we can come back to that. Then the recurrence is that how many times. It could either be zero, in other words or one. If in other words, it only has one instance of it, or it could go up to say four so it can have up to four connections or something like that, and that's good. Let's do mass 0-1 one as well. I'm not going to hold myself to these but we just put some numbers in for now. This is an interesting one. Now, in terms of which parent we connect to. I've built various genetic algorithm systems beforeand I quite liked the idea of a developmental process, and some of the literature seems to report that having a developmental process leads to more interesting and better evolution, basically. What do we mean by developmental process?I'll just briefly explained that. What the idea of the developmental process is rather than just instantiating the whole thing like bang there it is. What we do is we do it piece by piece. It's okay there's one link, and then we did instantiate there another link. Then when we're thinking about, which one is this going to connect to, well it's only got one option so it's always going to connect to that. Then, let's put another link out there. Now, this one because I'm doing it link by link has now two options as to which one it connects to. What I will do is this parameter here is going to dictate which of those options it chooses. This option is going to dictate which of those two does it join to that one or that one. Then when we get in another one here, so this one now has three options. We just have to dynamically scale that parameterso that it chooses from the full range of available links, and that's going to be fun to program but we'll get to that. Okay, let's just put a number of links there just so we know what that is, and then we've got axes x, y, z, three options, we done those, and I think that's is all done. The final thing is to think about the control system. Remember, I want to have these signal generators, so I'm going to say 14 control. That should be prefixed with links, strictly speaking. I'm sorry joints. Let me just put those prefixes in, o it's all correct, it's all joints. They're quite long names, but they're very descriptive, so I know exactly what they're going to be so when I get them in my code, it'll be really clear what's going on. Control, so I'm going to have a waveform. I'm going to have amplitude, then I'm going to have a frequency. The waveform will be either sine, pulse, or ramp, and the amplitude doesn't matter, it's going to be between zero and one so because remember, the amplitude is going to dictate what is the speed of the motor. If it's one, then the motors at maximum speedso it allows us to vary the speed. Yes, we could say, ""Well, we don't want the motors moving too fast, ""so we can put that fairly low for now. Then the frequency is how often it changes the polarity of the motor if you like, so when the polarity is positive, the motors going one way and when it's negative, it's going the other way, so we could have that as being zero to one as well because it depends how fast the simulation runs and how often it calls update on our control system so we can fiddle with that later. Okay, that gives us a general idea. I think that's it, so we've ended up with, what, 17 parameters. In this video, we've been going through and working out all the different parameters we need to control the morphology or to define the morphology in the control system for our creatures. We've been looking at the links, how we can parameterize the links, how we can parameterize the joints, how we can parameterize the control system. We've now got 17 parameters, which we're going to be working on when we start getting into the code. Okay, so in this video, we've just been figuring out how to parameterize our creature."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Test driven approach,https://learn.london.ac.uk/mod/page/view.php?id=96276&forceview=1,"[MUSIC]In this video, I'm going to give a bit of technical context for the programming approach I will be using to develop the code of the next few videos. Now, the approach I want to use is the test-driven development approach. I'll just say that if you have studied my modules before, so if you've already seen me talking about test-driven development, there's nothing new here. You've already seen it, you can probably just skip through this video. If you haven't seen test-driven development before, then this is what we're going to be talking about. First of all, I'm going to say what test-driven development is. I'm going to give you a nice detailed description of it. Then we're going to talk about the three laws of test-driven development, which gives us a good flavor of what the approach is. Then I'm going to quickly show you how to do it in Python using the unit test library. Okay. Here's a definition of test-driven development. Test-driven development is a practice advocated by agile software development methodologies. Agile software, so it comes from agile software, and in which tests are written in advance of source-code development. Tests are written in advance of source-code development, that's important. The idea is that you write the tests first, and then you write the code to pass the tests. The tests are intended to initially failin the absence of any substantial implementation. As we say, you haven't implemented it, you write the test and then you have some really bare bones implementation that's going to fail the test. Then you write enough implementation to pass the test. The tests effectively constitute a specification of the functionality and behavior of the software code. This is good news, people, if you haven't done test-driven development before. If you've done any software engineering, you've seen all the different ways of writing specifications, requirements, documents, user stories, all this, blah, blah, blah. All that stuff's great, but the great thing about test-driven development is you can throw it all away. Don't quote me on that, and you can just make sure that your software passes the tests. If your software passes the test, then it's good enough. If the tests aren't sufficient to test all the requirements, then you need better tests and you need to write those, but yes, that's the kind of idea. It's never that simple, but that's the idea is that by writing these tests, you're saying exactly what the code should do. You don't need a separate document that specifies what the code is supposed to do. You just say, ""Well, here's the test. If it passes these set of tests, then it's working. ""That's really good. Let's move on. These are the three laws of test-driven development. You may not write production code unless you've first written a failing unit test. What does that mean?The idea is, well, I probably should point out that what we mean by tests are, we write code which tests other code. The test itself is some code, but the actual code itself, that's the program, we're testing that with our test, but we write the test code first. You may not write any production code until you've written a test that fails. Then the next rule is you may not write more of a unit test than is sufficient to fail. You don't go and write hundreds of test sand then write the code to pass the tests. You write one test at a time. You only write a complicated enough test to make your code fail the test. It's an iterated thing. Then you may not write more production code than is sufficient to make the failing unit test pass. In the same way that we only write the minimal test to make the thing fail, and then we only write the minimum production code to pass the test. Then we write another test. The idea is you don't write these really elaborate tests, you don't write really elaborate code to pass the tests unless you need to. It's all about writing the bare minimum of code each time and you iterate. It's an iterated cycle and it's supposed to be really quick. You're supposed to quickly write a test then write the code to pass a test. That's the approach I'm going to be using. Now, in the next bit of this video, I just want to show you how to quickly write unit tests using Python and the unit test framework. Here's an example. I'm going to jump into my code editor and code this up, actually. I'm going to create a new file, save it as test. py. The first thing we do is we import the unit test module. That gives us access to built-in unit testing functionality in Python. The next thing we need to do is implement a class and it can be called whatever you want, but the important thing is that it extends on unit test. Test Case. We're basically extending on this class. If you haven't done object-oriented programming in Python before, let's do some object-oriented programming, it's not too hard. For our purposes here, we're just going to be basically thinking of a name space, so really the class is a way to store a bunch of functions in a neat way. What we're defining here is a class which contains a bunch of tests, so you'll see how that gets built up. The next thing I need to do is write a test. What I do is I write a function, which starts with the word test. Let's call it test mean. It takes as an argument self, because this is going to get past an instance of the object itself when it runs, which is object-oriented programming in Python. This is not a Python programming course, so I'm not going to go into loads of detail on that. What I want to do is I'm going to import numpy, and I'm going to use some tests on numpy as an example. When we call mean in numpy, we expect to get the mean of some values, don't we?If I was to call numpy. mean on one--Sorry about the thing that keeps popping up in the way. If I was going to call numpy. mean on 1, 2, 3. I don't like all these pop ups, I should disable them. I'll disable them so they don't keep annoying us. Anyway, so numpy. mean 1, 2, 3, you can probably guess the result of that should be two, right?What we do in order to test if that function was correct, we do self assert equal. This is why we need self as an argument. Unit test. Test Case class, which we've extended, gives us a whole bunch of functions for doing tests on results. We want to test that the result of calling np. mean is in fact two. We want mean to equal two. That says, check the result of calling np. mean on 1, 2, 3 was two. That's my first test. Then I just run it by doing unit test. main, and that would just run this thing. I press Save, and I'm going to run this straightaway in my editor here. There's a result, so you can see it basically said ran one test and how long it took. Then it says okay, which means the test was passed. Let's maybe write a failing test. Imagine I've got another function, another test called test call it test mean2. This time, I'm going to see np, just to show you a failing test, although I haven't got some failing code. Let's doing, 1, 1, 1, and I'm going to go self. assert Equal. Mean is going to equal two, so that's wrong. I've deliberately created a wrong test just to show you what happens when it fails a test. That is going to fail, obviously, because I've asked it to make sure that I get a one back, but in fact--I'm sorry, I've asked it to make sure I get a two back, but in fact, I'm going to get one back. It should produce an error message to that effect. There's an assertion error, so the assertion failed. I got a one and you asked for a two, it says it's not equal to two. One, you ran two tests and there was one failure. That's just showing you how to do that. Your test-driven cycle would be, you write a test and then you go off to your code and you fix the code so it passes the test then you come back, you run the test again. That's what we're going to be doing. Okay. In this video, we have just been seeing how test-driven development works. Like I said at the beginning, you should have covered this already, if you've done my other videos in various courses. You may be hitting this for the first time, in which case, don't worry if this is all new to you. There's plenty of documentation online about unit tests, but also, as I work through, I'm going to be explaining my test. I'm going to be writing loads of test sand you'll see exactly how it's done as we go, and hopefully, it won't be too problematic. It's definitely worth learning because it's a very good way of developing algorithms, because you can verify that the algorithm is doing what it's supposed to do at every step. In this video, I've just been introducing the test-driven methodology that I'm going to be using to develop the code over the next few videos."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Generate a genome,https://learn.london.ac.uk/mod/page/view.php?id=96278&forceview=1,"[music]In this video, we're going to start coding and start building out our genome class, which is going to allow us to generate random genomes, and also it's going to have a descriptive capability as well. In summary, what I want to do is, first of all, create a genome class. Then I'm going to generate a gene at random, so a random set of numbers. Then to generate a random genome, we're going to differentiate between those two terms clearly. Finally, how to create a genome specification, which allows us to say what the different bit sin the genome are going to be used for based on the parameters we identified in an earlier video. The genome class, now remember, we're doing test-driven development here. Let's jump over to this. What I'm going to do is create a new file, and it's going to be called test genome. Test_genome. py. Save that as tesT_genome. py. What this is going to do is it will contain tests for the genome class that I haven't written yet. Remember, I'm doing strict test-driven development hereso I'm not going to write any code until I've written a test that fails. Remember, we do class genome tests, and it needs to extend on unit test. Test Case. Then we define a function called-- let's do testClassExists. [chuckles] How about that?I'm basically going to import genome as-- so genome doesn't exist yet, but I'm going to import it anyway because it's a failing test, and that's fine. Self assert is not none. It's not none. Genome. genome. That's it, that's my failing test. You'll get the spirit of test-driven developmentas we work through it. Let's run this. That's the minimal test that I need to write for a fail, and I hit play. Import genome, no module named genome. I can resolve that. I'm going to create a new file. I""m going to save it, I'm going to call it genome. py. Great. Save that. Let's see if I can pass my test now. Oh, I forgot to actually run the tests. Unit test. main. Let's try again. My code was even too minimal. Attribute error, module genome has no attribute genome. Good. Failing tests. That's what we need. Then we write the minimal code to pass the test. It's going to have an attribute genome and that's it. Hopefully, they'll pass the test. Okay, good. I've passed my test but I haven't got anything very useful, but this is how test-driven development works. The next thing is I'm going to write another test which I can fail. I want to make sure-- remember, we're trying to basically generate a random genome here. I want to do is test Random Gene. I want to test that random gene exists. Test, assert, is not none. genome. genome. gene, get random gene. I'm going to call it get random gene. The function I need to define is called get random gene. Let's run that. You'll see now I've got a failing test because object genome has no attribute, get random gene. Let's do that. Def, get random gene. This one is going to be a static function. It doesn't rely on the class having any state so there's no object required. I can write a hint here. I can write static. I can do @staticmethod, like that. That says, this is a static method. I don't need to create a genome object before I can use it. It's just sitting there. It can just return. It's a bit of a programming lesson where we're going. Why not?I'm passing that test. I need to write another test to make sure that it actually does something. Let's do this, test random gene, not none. Okay. Test is not none, genome get random gene. I'm actually calling it now. That should give me a problem. I've now got one failed test again. I need to go back and make sure it doesn't return none. I'm going to return an array now. We're building up gradually, this is the process. I'm now passing that test. You keep going. They're really facile tests, but they basically allow youto evaluate your code as you write it. We'll get a bit deeper once we get into it, but that's fine. Get random gene not none. Get random gene has values. Assert is not none, get random gene. Let's just get a bit more complicated, gene = genome get random gene. I'm going to assert that, gene 0 is not none. Fine. I'm going to jump ahead now. I'm going to go straight for itand I'm going to do import Numpy as np. My gene is going to be return gene = np. random. random, for i in range. Let's just put-- and then we can return gene. That's a list comprehension where I'm going to iterate through this loop, which is basically going from zero up to nine. In each point in the loop, I'm going to be generatinga random float using np. random random and then I return that. That's going to give me a random gene. Hopefully, now I should pass that test. I can even just print out my random gene just to verify what it looks like. That's what I'm after basically. A set of random numbers in a gene. Now, the final thing I'm going to do is add a parameter to the function. I'll do another test. Test random gene length. I'm going to now say I want the random gene, take a length parameter, which is going to be, say 10. I want to get back the number that I asked for. I'm going to self assert = in this case. I want that to be the length of-- I'm going to assert that the length of gene-- Oops, let's just do it from scratch. Self. assert = length of gene is 10because I asked for a length of 10. Actually, let's do length of 20 because I know it'll fail that. 20. Then we run that. I'm running five tests, I've got one failing test. Get random gene takes 0 positional arguments but 1 was given. It doesn't take any positional arguments so I'm going to say, now, I want you to take a length parameter, and that's going to get passed into here. Now that test passes, but now you can see two of the other tests have failed. We need to go back and fix those other tests. In my regression testing, you can see I'm now failing. That's because I'm not passing any numbers here. I can pass those tests by just passing in numbers. Hopefully, yes. All my tests are running. I've got my random genome generator. That was a little bit over-complicated, because I was doing my unit test approach, but it does mean I've got a nice test re-developing here, which is great. The next thing I need to do is, I've got random gene, I've got a genome class, I need to do random genome. Just to be clear, the difference between a gene and a genome is that a genome is made up of multiple genes. The concept we're going to use here, remember that list of parameters that we designed earlier, which is over here, remember that list of parameters, the idea is that a geneis going to encode this set of parameters. Effectively, each gene represents a linkor if it's got a higher recursion, multiple links. Then the genome is a set of genes. We have multiple genes in our actual creature. That's the idea. What we're doing now is we're making a genome which is essentially-- has multiple genes in an array, but they're really just NumPy arrays underneath the hood. What I'm going to do is, first of all, I'm going to write a test to verify that I'm getting NumPy arrays so testRandGene is NumPy array because I want everything to be NumPy arrays. I'm just going to pull out gene and self assert I'm going to say type of gene. Now, I can't remember what the type of NumPy arrayis so I'll just quickly check it here, as np, and a = np. array 1 2, 3. Type of a. I need to verify that that's what I'm looking for. I need to make sure that I'm getting NumPy arrays back. I'm just showing you my practices, how I do stuff. You can't remember every possible thing. Gene equals self is type of gene is going to be equal to numpy. ndarray like that but we can actually np because we imported it as np. Let's run that and put self in. Let's run that. Name np is not defined because I have an imported NumPy, so let's just important NumPy to the tests so we can access it. Let's run that again. One test failed ""list, "" so at assert error, we've got a list but we wanted a NumPy array. Let's convert that to a NumPy array, and that should be easy. We can just do np. array, wrap that, like that. Done. Minimum code to pass the test, done. We'll test the passing. Now, next step is to get that genome. I'm going to say def testRandomGenomeExists. I just do genome = genome I can't call it genome because that's the name of the importso let's call it data because genome. Genome. get_random_genome. Let's say, you have five genes. Now, let's just assert that self. assertIsNotNone (data) basically. Let's run that. It crashes because genome doesn't haveget_random_genome yet so we're going to just do that, gene_count. For i in gene_count. We could actually do this with less comprehension as well. Let's just do that. We can just do genome= get_random_gene. Let's do gene_length, gene_count. get_random_gene on gene_length for i in range(gene_count). Okay, good. I'm going to put this as a static method as well. That can be a Genome. genome get_random_gene, gene_length, da, da, da, and then return genome. Hopefully, we can pass that test now. Type Error, get_random_genome, missing one required positional argument, gene_length because I need to pass it a gene_length. Let's say 20. I'm passing that test. I can now generate a random genome, so we're making good progress here. This is really that test-driven development approach. Moving on. What's next?I've got random genomes. What about genome spec?Now, this is where it gets a bit trickier. What I want to do, I'm going to be a bit naughty, I'm going to break out from my test-driven developmenta little bit here because I want to just write out what a spec is. Let's put a little test in so we can just say, def testGeneSpecExists. I'm going to do spec = genome. Genome. get_genome_spec-- gene_spec actually because it's really a gene, not as genome. spec and self. assertIsNotNone, just copy that, spec. We can run that just to verify. Failing test, def test I can't even What's the problem?Invalid syntax because I forgot my colon again. There we go, so type object Genome has no attributeget_gene_spec so let's write that. @staticmethod, def get_gene_spec, and return a dictionary. That's now passing that test. I'm going to go into a bit more depth here. I think I'm going to skip to the next video to do this because I've been rattling through just making basically, a list of random numbers and could have done it in one line, but I wanted to show you that test-driven development workflow. In the next video, we're going to come back to this and do the gene spec. Let me just jump to the summary just for completeness. In this video, we've just been working through building up some of the code, the underlying algorithm to generate that random genome and random genes, and doing it in a test-driven way. That's it, see you in the next video where I'm going to be doing the gene specification."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Genome spec,https://learn.london.ac.uk/mod/page/view.php?id=96279&forceview=1,"[music][music]-In this video, we're going to be building out our genome spec, which is going to be a dictionary that helps us to work with the genome data. First of all, we're going to look at what the genome spec is going to be doing. Then we're going to implement this idea of getting the genome, values in the genome and scaling them into the range that we need. First of all, it's a little bit of planning here. What we want is some descriptor of the genomeso that we can-- You see that first line there, so gene[spec[""link-length""] [""ind""]]. The idea is that if I've got a gene with random numbers in that I can pull out the number that I need for the link-length. Remember, link-length is the size of one of these parts that makes up the creature. If I want to find out when I'm building the creature from the genome, I want to find out what is the length of the link. Then this syntax here, you can see it's going to be quite helpful. I can get my gene and I can pull out the data. This is the idea. Then we'll have the secondary, we want to be able to get a scaled genome where we can basically parse those raw numbers between zero and one that we saw earlier, and we can get back. You can automatically scale all of them by their right amounts. We need some kind of descriptor dictionary, which has all of these different parameter sand maps them to scaling factor sand gene positions. We're going to need this index parameter for every one of our genomic parameters, and we're going to need some scaler parameters as well. Let's see how we go about doing that. First of all, I'm just going to look at the parameters to remind ourselves what they look like. We went through and we came up with this list of parameters. What I'm going to do is, basically, just turn that into a Python dictionary in the format that I want. Let's just take that and copy it into the genome object here. We've got this get_gene_spec here. How do I convert this into a dictionary?I'm not doing this test-driven style. Sorry. I just need to build this back up first. What we need to do is I'm going to dump these indexes first because I might want to insert more parameters later so I don't want to hard code all the indexes. The index, in theory, that would be the position in the gene where that parameter exists. At the moment, I just want to have--That's new genes_spec equals--We can just put it all in a big dictionary to find it there. Then what we're going to do is I'm breaking the laws of test-driven development here. I should've written a test first which is failing. All right. Call me red-handed, I'm going to do it. There we go. Let's do testGenomeSpecHas an index. testGenomeSpecHas--What's one of those parameters, link length, HasLinkLength. There you go. A failing test for you. HasLinkLength genome spec. test assertIsNotNone(spec['link-length'])How about that? There we go. We've got a test that will fail now. It's going to just crush out terribly because the code doesn't work at all. Key Error link-length. Great. I've no idea how I managed to process that, but anyway. We got gene_spec. link-shape is going to be--What I want to do is I need to have the link-shape and then it's going to have some--That's the key name, so I need to put that in quotes. It's going to take a little while, we can probably fast forward to the point where I've actually done this, but let's do it anyway. You can always put the video on to double speed to watch me doing this. There we go. That's all the parameters there, I don't need the numbers. I'm going to put all those in. Disappear off the screen, so you don't need to see me staring down at my code editor here. Those are all the parameters. What I want to do is just prefix them all with a quote. Let's just do that. Oops, not a quote and a bracket, but just a quote. It takes me back to the days of programming trackers on [?]when we used to program in endless drum brake sand have this really fast patterning of keyboard, shortcuts that you do. There we go, so we do that. Let's do that. These are all the properties in my dictionary, and they're going to have one for each of the parameters we figured out earlier. That would allow us to basically have a bunch of these. Oops, the code goes there. Then the next question is, what am I going to put into this?I need to have basically the range. We can do another dictionary which will be scale factor, which is going to be one, so that one is not going to get scaled effectively. We can edit these as we go through later, can't we, to tweak the encoding. It's going to have a scale of one, that scale of one. Now, that one's going one to four, so that's going to have scale of four. Oops, forgot their commas. This one is scale of one. That's going to not scale. All of these are all scale of one. Like I say, feel free to fast forward. Now, this is an interesting one. This is the one thatis controlled by the number of links. Remember our developmental process. Then this one doesn't need to scale. Oh, that's up to two-pi, so let's do up to an np. pi * 2. These are all two-pi because their rotations and radians. We're building up our gene spec care here, we're saying explicitly what it's going to be doing. Oh, that's not a rotation, that can be a scalar of--That's how far is away from its parent link. That's going to be just up to one at the moment. Getting there. This one, that's going to be a selector inside an array, but we will deal with that later. That's going to be 0. 25, so we don't want to go--That's the maximum speed of the motor. Then that is the maximum frequency of a motor. I think that is it. That's my dictionary. Let's get all of that, and do there. Let's tab it in. That should do it, and returngene_spec. Let's save that, run the test, see if it works. Brilliant. I'm parsing my tests now, I'm back. I've now got my basic gene spec which now--Remember, this is what I'm aiming for. I want to have a gene spec which I can then use to index into the gene sand also do scaling. At the moment, it doesn't have an index because remember, I moved all those indexes, but because I don't want to have to hard code all the indexes because that's a bit of a pain. What I want to do is I'm going to write another test to verify that both that we have a link. The link-length has an index. The index is going to be the position in the gene array where that exists. I'm just going to verify that. It doesn't work. Key Error ind because there is no ind, there's a scale, there is no ind. What I'll do is I can just iterate over this for key in gene_spec. keys. What I'm going to do, is keys--Because I switched languages all the time, I can never remember how I pull things off of where. Let's do a quick dictionary. This is what I do. Whoops, test equals hello. Then if I do--Sorry, that's a list, test equals hello. x. keys, is it a function? I can't remember. Yes, it's a function so you have to call it. It's a function x. keys. Then what we're going to do is we're going to say, we're going to have this index, so index = 0, index = index + 1. I'm going to say that gene_spec, its keys for key equals index. I'm going to add a new key to it called index equals ind. That should work, and I'll parse my test now. Good. We can even print out the gene_spec just to verify that it's not completely crackers because that is a problem with these unit tests. Sometimes you're wondering, is it actually doing anything sensible?Right, so there's our gene spec there. You can see we've got all of the different parameter sand each of them has it's own scaler. In theory, so I can now use this in combination with the gene generator to pull out properties. For example, I could do a test that says testGenSpecScale. I'll get the gene spec and then I get a random gene. Let's do that. Genome is random gene, right?Then I can say self. assert Greater than--I want to insert the gene, then I can do spec. There's exactly that code that we saw on the slide basically, [spec[""link-length""][""ind""]]. Let me make it bigger for you, link-length, ind. I'm changing my quotes all the time. Okay. What I want to do is insert that I can actually pull out a gene that's greater than zero, because it should be. What I'm doing is I'm now using my gene spec to pull a value out of my gene. That means that is more readable because otherwise if I pull gene number index three, what is that? We don't know, right?If we say pull gene link-length, that's giving me the gene length, the link-length from the gene. That's the idea. It's all about trying to have a more readable code. Okay, that is parsing. I was able to pull out something from the gene and it's able to verify that it's greater than zero. Okay. That's the basic thing for my gene spec. We're going to be using that gene spec in lots of different places as we develop the genetic algorithm. Let's go to the summary. We've just been building out the genome spec object which allows us to--It's a dictionary really, which is an object. It allows us to index into the gene sand pull out what we need for a given purpose. In the latter videos, we're going to be then using that to convert the genes into sets of links and the joints. Yes, we've just be working on our genes specification dictionary."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Converting from flat genes to a graph: algorithm,https://learn.london.ac.uk/mod/page/view.php?id=96282&forceview=1,"[music]In this video, I'm going to explain how we're going to go about taking a flat genome architecture and converting that into this expanded graph structure. In summary, I'm going to start by reminding you what the gene structure is, if that isn't clear, and then we're going to look at the graph structure to see what the target is. Also talk about the fully expanded graph structure, which is the ultimate target here. Finally, how we then get that expanded graph structure and convert it back into a URDF, and what we'll see is the pseudo code algorithm that we're going to build actually does that for free as a result of how I've designed it. Let's get into that. This is the structure of a gene. You'll remember from the previous videos that what we're doingis essentially generating lists of random numbers, and there'd be a certain number of these and say 14, I think it was. Each of those numbers represents a parameter on a link effectively, which defines how that link connects to other links what size it is, how the joint works, all that stuff. We define all of that as a list of numbers. We have several sets of these lists of numbers, each of which we might refer to as a gene. What we need to do is take that flat array of arrays if you like, or list of list sand convert that into something that looks like this and ultimately something which looks like that. Now, let me just get into that and explain what I mean. The first thing to talk about is the basics of genome structure. We have these lists of numbers that look like that. Each of those is a random number, and each of those is a gene, and what we're saying is each of those genes converts into a link, which is a body part, if you like, for the robot. There was a complexity here, which is that when we link from say body part B to body part A, we have a recurrent relationship, which says we can actually make repeats of that. If you saw in the structure, we were looking at on the slide there, we actually had-- when we defined our C we had two links going from C to B. Then we had another thing which was D, and that had one link going back to C. What that means-- whilst each of those would be your gene. We'd have another gene that defines that one and another gene that defines that one. It's four genes and there's four different types, four different nodes there if you like. What it means if you've got that structure where you have the two links here, is that it basically is recurrent. We'd actually end up with the expanded graph. We'd have an A, and it would have a B, which is easy enough, but then for the C, it would have two. You'd have two Cs like that, and you'd have a D, for each of those two Cs, you'd have to do that. What the trick is, we need to get from this flat structure into this structure first with the recurrent relationships. Then we need to take that and expand it out into this full structure here, which is what I'd call the expanded graph structure. That's what we're going to have to do. Then eventually we need to pack that down into a URDF file and do that. Let's go through that. I'm going to do it in two phases. Phase one is going to be this part where we get from the--sorry about the messy diagram, from the genes into the list of core node types or link types. Then the second phase is when we are going to take those core link types, and we're going to expand them into the actual total number of links that we're going to need in the eventual robot body. I'm going to mention one more thing here, which is the developmental process. Remember that as we are building out our robot, we have this developmental process. We start with the root node, and then we add another one. That's the first gene. Then the next gene would be B, and the idea is that this one can link to, or can be connected to-- got to be careful with this word link. Each of these is a link, a link is a body part, a joint connects two links together. That means that B is going to have a joint connecting it to A, and because of the developmental process, B only has one option as to what it's going to connect to. That's fine. It has a parameter in its genome, which says, which parent do you connect to from zero to one. In this case, it doesn't matter what that parameter is, it can only connect to A because there's no other things left because we build it one by one. Now, when we get to C, when we add our C in there, depending what the genome parameter is, it could either connect to B like that, or it could connect to A. If the genome parameter it says between zero and one, if it was 0. 3, then it would connect to A, if it was 0. 7, then it would connect to B. That's one of the things we can evolve is which ones does it connect to. For now what we know is that actually, that that value must be high because it actually connects to Baccording to the graph we've been working on. That's fine. That connects there. Then when we get to D has even more options. D could connect to B, C, or A. Depending what its value is for that parameter, that's what decides what it's going to be. It's obviously got a high value for that because it's connecting to C. That's what we mean by developmental process. What I want to do now having laid that all out, I'm going to just write the two phases algorithm. Remember phase one is going from the genes to the flat list of links types, and then phase two is going from the flat list of link types, and then processing this recurrence relationship where we have multiple links sorry, multiple connections from one link to another. We basically expand that out. How are we going to do it?I'm going to just bring up my text editor here and start coding. Phase one pseudo code. It's basically going to iterate over genes, and it's going to start with a--I'm going to have an array called flat links. The flat links array, its going to be an empty array. Iterate over genes for each gene. Create a link and add it to flat link this array. Let's call it an array because it's an empty array. That's it. You end up with-- that's it, that's easy. The other thing is we need to just decide what the parent is. Decide parent link name. We might actually assign each link a unique name, decide parent link name. That's it and actually, probably we want to for each genome create a link, then let's just rearrange that a bit into a sequence of steps. That's more like it, isn't it?That's our pseudo code algorithm for phase one. We basically iterating over the gene. That is going to give us-- if we have to draw that out, that would look like-- the output of that would be basically this kind of thing with A, B-- sorry. My shape recognizer thought that my square was a circle, A, B, C, and D. That's going to give us that. It's just going to give us that flat link structure. Great. Now, what's next? Phase two. This is where it gets tricky. Let me get out of the way. Phase two. Let's bring that up a bit. Phase two we're going to have another empty expand links equals empty array. Now we're going to define a function called phase two and phase two is going to take a parent link. It's going to take the flat link sand it's going to take the expanded links as arguments. Just to say, if you've followed any of my software engineering videos before then you'll know that I don't like global variable sand so what I'm going to do is I'm going to pass into this function everything it needs to do, what it needs to do. It's not going to pull out some magical global variables. That's that. Basically, I've got my phase and it takes those three argument snow what's it going to do with them?Well, it's going to say to get say children equals get and imagine there's a function called get children for parent link. We know how to do that. Basically, how do we get the children for a parent link?We go here and we say, ""Okay, that's my parent link give me all the children, ""which in this case would just be B that would just give me B in the first instance. Then what we're going to dois going to iterate over the children for child in children. Okay, what we are going to do with it?We've now got our B. What we are going to do with B?We are going to, first of all, we're going to add child to expanded links. Wait, let's make it like this. Let's go a little bit pythony. Append child. Then we're going to say for recur in child. recur. Actually, we're going to do that here. Sorry. I forgot so I need to do that recurrence thing first. Yes, so I'm going to say, how many recurrences does this child have?For each recurrence, I'm going to add one to the expanded links array. Then I'm going to call phase two. This is the trick. This is where it gets recurrent. Now, just to say-- just getting grokking, as they say, getting your head around some sort of recurrent function can be a bit of a mind-expanding experience, which makes it great. Yes, it's not as easy to always get it the first time around. Some people can. For me, it takes a little bit of screwing on it to get it into my head what a recurrent function is doing. Let's go for it. What it's going to do, it's going to iterate over the number of recurrences that child has. Let's just assign that number here. Remember, this is going to be one, one, two, and one. How do I know those?Because remember going back here, we had two links going from C to B. Then only one link going from D to C. Those are the recurrence values which should be genetically encoded. Remember these are mutatable things that are genetically encoded. We figure out the recurrence value and we created one child. We added copy of the child. Copy of child, because we're going to be in python. We're going to make a copy of the child, we're going to add it to the expanded links. Then we're going to call phase two, sending it child and, obviously, flats and expanded links. What's it going to do?What's going to happen there?It's just going to call phase two again. What is phase two doing?Gets the children for that one, so if we're on-- Let's just go through this. We start off with A, and we get the children of A, which is just B and we count the number of recurrences for each recurrence. This is our expanded links. For each recurrence, we're going to add the two expanded links. Okay, fine. That's B. Then we're going to get the children of B. That would give us C. Children of B is one C, and then for each recurrence, we're going to add it. We add it, that gives us a C. We add it and then we recur in. On the first child, we're recurring in, if you like, we're digging in, and we're getting D, and we're going to add that onto the array, because it's got recurrence one, so that adds D on, and then we're done. We're done with that branch because there's no more children of D, so we're done with that and we go back up to C again. We're going to add another C onto expanded links, so we're going to end up with the C. That's all right, a circle is okay. We ended up with a C there. Then we're going to go from C down to D again because we get the children of C, remember?We're going to add- swipe that, that's a C. Then we're going to get the children of C, we're going to add that onto there and so on. Eventually expanded link is going to look like this. It's going to have an A, a B, a C, a D, a C, and then a D. That's what the algorithm is going to do. It's very simple, isn't it?Really not that much going on. That's how we dig in and we expand it into this array. To do the final stage, remember the final stage was--If I go back to my slides--The final stage was this. We need to get out from our expanded graph description into the URDF. You can see behind me there. URDF there. You want to get that back out, then it's actually quite simple because we've just got this-- We've already got our--These are all our links, so each of these things in expanded links is going to convert into a link tag, and each of them is also going to have a joint tag, which dictates which parent it connects to. We know that because we do that in phase one, we figure out what the parent is. Each of these in expanded links over there has a parent already, we just need to generate a joint tag that does that. In the next video, we're going to be actually doing this in code. We're going to check what comes out, verify, and see if we can turn that into valid URDF, which we can load into the PyBullet environment. Just see what it looks like from, say, a random genome into a random graph structure for a creature. Okay, let's finish there then. That's it. We've just been through the process of converting from a flat gene structure into a flat graph structure and then into a proper expanded graph structure using a recurrent function to keep digging into each branch of the graph and doing the recurrence, to make sure we repeat the branches where necessary. Then, finally, we talked about how we can convert that intoa URDF file or XML data at the end. Okay, so in this video we've just been converting froma genome into our expanded graph structure."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Gene to graph - implementation,https://learn.london.ac.uk/mod/page/view.php?id=96285&forceview=1,"[music][music]-In this video, I'm going to take the pseudo code algorithm that we saw in the previous video, and I'm going to convert it into Python code. In summary, we're going to talk about the flat links part of that algorithm and then we're going to get into the expanded links part where we dig in and really expand out that graph. We're going to deal with the unique link names problemas we go along, and we're going to be writing tests as we go too. First of all the flat links. Now, what I want to do is I'm going to use a test to say what it is, I'd like to be able to do. Here, I've got my test genome file here, and I'm going to write some new tests. I'm going to do test flat links. This is what I'd like to be able to write. That's one I'm doing with this test. I'd like to be able to say that links is a set of things which I'm imagining I've managed to pull out of a genome somehow. I'll do genome. genome. URDFlink. It's going to have a name, it's going to have a name variable, and that's going to be A for the first one, it's going to have a parent name, which is going to be none. Let's make that actually none for the first one because it's the root. Then it's going to have a recurrence, which is going to be one for that one. This is what I'd like to be able to write. It's not going to work yet, because I haven't defined that object yet. I'm going to have the rest of them so B, C, and D. That's going to have parent A, that's going to have parent B, C, and then that remember, C has a recurrence of two because we repeat that part of that graph, and its sub graph twice. There we have it. I'm just going to assert that self. assertisnotnonelinks. That should do it. I'm going to run that test. I get an error, which is, genome has no attribute URDF link. That makes sense. I haven't defined that class yet. I'm going to go ahead and define that class and then we should be able to pass this test. Let's write the minimum code to pass that test, which is to write a urdf link class. Here's my genome means genome. py now, and I'm going to put a new class into genome. The same level as the genome class, gn is going to be called URDF link. It's going to have a constructor, which takes as its argument itself, the instance of the object plus the parent name, and recurrence level, and it's just going to assign those. It's going to say self. name = name, self. parent name = parent name. self. recur = recur. That's the basics. If you're not familiar with object-oriented programming, basically, what I'm doing is I'm creating a wrapper, which I can use to contain all the data, I need to represent a link. The idea is, eventually I'll get that genome, which is the list of floating points. I'll pass it through the genome spec to convert it into numbers in the correct ranges, and then I'll extract out these URDF objects. Now I don't want to do that. But yet, I just want to write the algorithm that basically expands the graph for now. I'm going to assume that I can already do that. I've ended up with these links. That's what I'm doing here. I've got this wrapper that can contain the relevant parts for now of what a link is. It's going to have a parent name, and a recurrence. I should be able to now run this code where I essentially use that class to create a bunch of links to represent the graph we've been looking at. Type object genome has no attribute. URDF have link, so it's not inside genome, is it?It's actually just inside the module. That code where it says genome. genome that's digging into the genome class inside the genome module, but only to do that because it's at the top-level inside the genome module, isn't it?Because I just put it at the same level as the genome class. Hopefully, that'll work. Great. I'm passing that test now. My next step is phase two, which is where I actually do the recurrence, and I expand these basics, flat links into the expanded links array. I've got my flat links, which is if we remember back to the drawing, so that's my flat links up there. I need to figure out how to expand it into the expanded links which is coming down on the left there. How do I get it into that?This is where the recursive algorithm comes in. I'm going to write a new test for that. Def test expand links. This one is going to start with the same links here. It's going to create an expanded links array. I'm going to just say, genome. genome, expand links. I'm going to pass it the parent, which is going to be what link zero, so it's going to pass it the top-level one. I'm going to pass it, the flat links, and I'm going to pass it the expanded links because that's what we need. That's the basic function signature that I had in my pseudo code. What does that function do?Well, let's go into genome and start writing it. That's genome. genome. It needs to be in the genome class. Go up here. This is the genome class. I'm inside here. It's going to be another static function, because it doesn't depend on the state of the genome static method, and it's called def expand links. It takes as a parameter, firstly, the parent link, the flat links, and the expanded links. Now, the first thing it does if we go back to the pseudo code is this. It basically finds the children for that parent because we want to know, okay, what are all the links that connect to this one?What we can do is we can just use a list comprehension to do this. We do children = l for lin flat links, if l. parent name is parent link. name. Why does that work?Well, so remember flat links contains flat links contains all the link types, the sort of A, B, C, and D. What I want to do is I want to go through all of that and say, give me all of the links from there that have the parent a, in the first instance. I'm filtering that list of all the links down to the ones that have the parent that's coming in. The incoming parent is called parent link. I'm doing filtering for things that have the name of parent name of parent link. name. That gives me the children of that parent. Great. The next thing I need to do, according to the pseudo code, is I need to iterate of the children. Inside there iterate the recurrence level, and then add copies of those children to the expanded links array, and then dig in. Wow, okay. Let's implement that. Hopefully, actually, let's just see if I can write test here. What I want to do is assert that at the end of this, I'm going to have six links, so length of expanded links, I'm going to say that it's six because if you remember that we need to end up with six because that's what it looks in the diagram that I worked out here. Up there, you can see I've got A, B, two C's, and two Ds, that's wrong so there's six links in the expanded links array. That's what I'm going to do with this test. Let's run it. Zero is not equal to six. There's nothing in the expanded links array yet. I need to go ahead and implement that. Let's do that. For C in children, for A in range, c. recur. Iterative of the children iterate of the recurrences of each child, make a copy of the child. Now, in order to copy an object, what I'm going to do is import the copy module, import copies. I do that at the top of the script there. Back down again, and I do so C copy=copy. copyc. Remember, C is the child that I've got. I'm making a copy of it. I'm going to expand it links. Append c copy. Then I dig into the tree. I dig into the graph. I'm going to say, expand links, and I'm going to pass it C. I'm going to pass it the flat links, and I'm going to pass it the expanded links. Now, I don't pass it the copy of C because it's fine. It just needs the original flat link version. That's going to help us out in a minute as well when we get to this unique link lame problem, which I'm going to deal with in a sec. Expand link snow saying that it's going to give me an error thereso I need to just put the name space inside the genome class, cool, expand links on that. Let's save that. That is the basic implementation, and see what we get. It's now saying, I've got an error which is five is not equal to six. If for some reason I've only got five link sin my expanded links array, after I call that function, it should be six. Now, why is it only five?If we look at the code, we never actually add the top-level parent. We send in this quarter, we start off by bootstrapping it by sending it the parent, the root node, which is A, but it never actually adds that to the array because it only ever adds children. The only things that ever gets added to the expanded links array are the children. The original parent that comes in, never gets added. What I can do is I can just do that up here. I just put it in, started off with the parent of the root node there. Cool. There we have it. That's fine and no problem. The next thing is, the problem is that I've got this thing with six links, and let's just check what the names are before we carry on, and then we can see what the problem is. I'm going to just going to donames = (l. name for l in exp_links). I'm just going to print the names out, just so we can see them. Let's just run that. Where are they? Yes, there we go. I'm just going to get rid of the other printout because I don't want that. That's there. That's confusing me. Good. Let me get out of the way. What have we got?These are the names of the links that I've created, and that's what we expect, that looks good, because we wanted that. We wanted A, B, C, D, C, D and that's what we got. Great. It's working, but the problem is in the URDF file, every link has to have a unique name. I can't have two links with the same name, because I think it would be an invalid file, so it probably won't load it. Also, it means then I can't then link to it, because if you look at this structure here, you can see whilst they're both kind of seem to be called C, if I'm building that structure, how would I know which C I'm going to be linking toif they're both called C?Actually, they're going to have to have unique names, and that means I'm going to need to pass that unique name through, so I'll correctly structure my URDF link objects. I'm going to have to expand on this function a little bit. All I'm going to have to do is, I need to have a parent link, and I need to have the parent link unique parent name. I'm going to add that. What it's gonna do is, yes, you're going to know which link it is but you're going to use the unique name to link to instead of the original one, and that's going to make it work. First of all, I see you need to douniq_name =c_copy. name +and then I'm going to use the length of the expanded links array to figure out a unique name, expanded links. Now it can't be an intercom, add an [?] to a string, so I've turned that into a string. That'll give me a unique name. That's great. I'm going to do c_copy. name = uniq name. The problem is, if I'm now digging into the tree, and I'm linking to the original name, which is like A, B, or whatever, but I've now renamed it a zero, that means I'm going to lose that link, or I'm not going to know what I'm going to link to it. I need to pass this unique name in to the expand links function. I'm going to do unique name, so it's going to get the unique name. I need to fix it so that the child-parent name, so c_copy needs to know what its unique parent is. This is a little bit complicated, but trust me, we're nearly there. So c_copy_parent_nameis actually going to be this one coming in, uniq_parent_name. That way, it allows me to pass those through. Let's just save that and then run it, and you'll see what I mean. Excuse me, I did make an error, expand Links missing 1 required positional argument. Oh yes, now in the test, I need to pass it in that uniq_parent_name as well. That's fine. Let's just pass it a uniq_parent_name. Because it's the first one, it can be just that. That's fine. The first time I call it, it's just going to be the root node of which there's only going to be one, it should be fine. Let's have a look. Now, you can see I've got A, B1, C2, and then importantly C2 does not have the same name as C4. I've got two C prefix names. The question is did my little trickery to make sure that that unique name gets passed through is the linker name, is that working as well?Let's see what we can do there. I'm going to make a string here. I'm going to do, (l. name+""-parent-is-, ""and then I'm going to add, +l. parent_name, so we can just verify that the parent name looks correct as well. Let's run that. Let's have a look. Oh, can only concatenate str (not ""NoneType"") to str. It's saying that somehow, parent name is none, because, yes, that makes sense. If we look up here, we can see the parent name is none. When you're making videos, I don't always like to leave them perfect. I'm going to leave that error in and let you think about that. You can see when I run this, I'm getting this problem. It's saying, can only concatenate str (not ""NoneType"") to str. I'm trying to stick together. I'm trying to make a list that tells me what the original name is of the link, and what its parent name is. The problem is, you can see here, the first parent name, I'm setting to None, so I try to concatenate None, and then a parent name. I'm going to just set that to current name, let's just put None as a stringso that it's really clear that that is what it is. Let's see what we got now. 'A-parent-is-None', that makes sense, 'B1-parent-is-A', that makes sense, because B1 is linked to A. 'C2-parent-is-B1', so that makes sense because C2, our first C that we've create dis connected to B1. Then it says 'B3-parent-is-C2', 'D3 connects to C2', that's good, and then 'C4 connects to B1', that's good, because we've got a unique C there, so C4, I've got another C, and that's also connected to B1, hasn't come up with a weird name. 'D5-parent-is-C4'. That's the other one. If we were to draw this out, if I just pull out my pen and we draw this out, we can now show what the actual names are on our graph here. We've got one, two, three, four, five, six. What I've done with these unique names, is I've got my A, and I've got B1and then I've got D3, C2, C4, and is it D5 the last one, yes, D5. What I've done is I've connected them up like this. You can see, 'A-parent-is-None', so that doesn't have a parent, that's fine. 'B1-parent-is-A', so that's there, 'C2-parent-is-B1', that's that one, and then it goes, 'D3 is C2', 'C4 is B1' yes, good, and then ""D5 to C4'. Perfect. Okay, I'm happy with that. My little trick to create unique names, then pass them through, the recurrence is working. Great. What I want you to do, I'm going to set you the task of run some more tests for this and just verify that it's definitely working by passing in some different trees, so by hand, you can pass the trees. Have a look at what comes out from this algorithm, and see if it's correct. Let's jump back to the slides and do a quick summary. Where is it? There it is. What we've been doing in this video, we've just been working through and putting in a placeholder for the flat links generator at the moment, so we just manually created these URDF link objects. We've been testing out the expansion function, which takes that flat graph and expands it out using the recurrence relationships into an expanded graph. We encountered this problem where we need unique IDs. I showed a little trick to pass those through, and we did a bunch of tests, and we verified the name sand looked at that in great detail. Hopefully, that's really clear to you now. I want you to go and write your own tests now, so you can test it with some different laboratories and see if it works. In this video, we've just been working on implementing this recursive algorithm to expand our graph."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Interpret the genome spec,https://learn.london.ac.uk/mod/page/view.php?id=96290&forceview=1,"[music]-In this video, we're going to start building outour genome to URDF code. In summary, we're going to be looking at the genome spec part one, so we're looking at parental and recursion parameters which are the hard ones, really. Well, the parent one is. Then we're going to be looking at how we can then pull in the extra link and joint properties. The first stepis to think about getting a genome dictionary. What we want is a dictionary that describes the genome because at the moment, remember, the genome is in the form of a big list of random numbers. That's not very easy to work with, and when we start reading our code and this is dealing with hard-coded indices in that, it's not going to work. What we've done is, we've already created this genome spec thing. If I jump over here, I can remind you of that. Where are we?Genome. In the genome file, we've got this genome_spec thing. The genome_spec, it basically tells us, for each index in a gene, in a genome, what's the name of that parameter that it's encoding and what's the range of it. The idea is that this will make our code easier to read later onby having this spec. It also makes it easier to tweak it. Let's say I suddenly decide, ""Oh, I don't like that link recursion. I want to have more, a higher level of available recursion. ""I can just go in and edit that number there and that will scale up and every creature I create from that point on will have a higher recursion level, or if I want to change the range of the length or whatever, it's super easy. It's very easy to read and you can interpret it. That's the reason for using this. It does make this interpreting the genome a little bit more complicated. What we're going to do is I'm going to just show you in here briefly. I've got this genome that's got random numbers in like these. What I would like to have is a genome dictionary which is something like link-recursion. That will be whatever, a sensible number for that, right?3. 4 or whatever. Anyway, it has to be a whole number, doesn't it?Then, with the others as well, so with the other parameters. That's what I want. I want to get from that to, then, my nice lookup dictionary for my genomes. How are we going to do that?Well, what we're going to do is we're going to write a function called genome to genome dict, and then we're going to do that. Let's write a test first. The test is going to be called def testGeneToGeneDictand we're going to start at the gene level, right?Remember, a genome is made up of multiple genes. The inner function hereis the one that takes a single gene and the gene_specand uses them to convert into this nice scaled genome dictionary. How am I going to do that?First of all, I'm going to take my spec equals genome. Genome. get_genome_spec, and gene equals genome. Genome. get_random_gene. The length of the gene is going to be the length of the spec because the gene should have the same number of numbers in it as there are keys in the spec. Then what I want to do is say, gene_dict equals genome. Genome. It'll be gene_get_gene_dict. I'm going to parse it to the gene and I'm going to parse it to the spec. Then I'm going to do self. assert and that is in. [silence]Okay, yes, self. assert. I want to make sure, with this, --Sorry, it took me a little while to find that one. -I want to assert that in my gene_dict that I have a key, maybe somebody's key. Just verify, maybe, that link-recursion is there. I can just check, have you got a key, a sensible key, in there. That's just a quick way of verifying that the whole process is working with a simple test. I can run that, and we'll find that, of course, object genome has no attribute get_gene_dict. Okay, because that's what we're supposed to be writing. Let's go ahead and do that. We need to add a function called get_gene_dict to this. I'll put it up here. Maybe up here is good. It's going to be a static function again because it isn't stateful in the sense of requiring a built-up genome object. Get_gene_dict, and it's going to take as its input a gene, and a spec. What it's going to do is going to go for key. Let's, first of all, create a value called gdict equals that. Create an empty dictionary for gdict and for key in get, -Whoops, sorry, -key in spec. The index is going to be spec key index. That's which element in the gene array I'm going to be using, and the scale is spec key scale. Just to remind you, you've got the scale value there, and then we assigned inds to each one as well. That will allow me to find the appropriate item from the gene. Gdict[key]equals gene[ind] times scale. There's return gdict like that. That's what I wanted to do. I wanted to, basically, use that spec to make a more usable data structure for my gene processing. Type error in the string requires string as left operand, not dict. Whoops, that's interesting. Assert is in gene_dict. That's interesting. Keys. Let's try that. In assert In, if member not in container, Type Error in <string>requires string as left operand, not dict_keys. Oh, okay, sorry, sorry. The first thing I need to parse is the thing I'm looking for, and then I parse the thing I'm looking for in. I don't need keys. Okay, good, it's passing that test now, so yes, yes. Just to explain, the assert In function, the first argument, is the that thing you're looking for, and the second argument is the thing that you're looking for it in. You're going to say, ""Is this in this?""It makes sense for them to be the first and second. Okay, that's fine. I've successfully created my gene dictionary by the looks of it. I could do some more tests to verify that those things are being scaled properly but I'm pretty sure they are, but yes, you might want to be, for completeness, you might want to do that yourself. The next step is to do the genome dictionary. Next, I would, basically, just write a function called static. No, we write a test first, don't we?Naughty me. Def testGenomeToDict. This is basically doing the same thing but it's going to operate over the whole genome. Spec equals--Let's just copy all of this like that. Spec is get the spec and then the genome_dicts-Because, actually, it's going to be a list of dictionaries because each dictionary is going to be for each gene, so it's going to have a list of them. -equals genome. Genome. get_genome_dict, and I needed to parse it. Let's make a random genome. genome. Genome. get_random_genome. It's is going to be length of spec for the--Getrandomgenome requires the length of the gene and the number of genes and one gene count. Whoops, sorry. The gene count. That is, the length of the genie is, length of spec again, and the gene count so let's do three for the sake of it, and we can verify that we get three back out. Gene_dicts, your genome, get_genome_dict, and I'm going to parse it. It's plural because it's going to give us a list of them. I'm going to pass it as spec and I'm going to pass it to dna. Oh, it's the other way around on that one. I'll do it the other way around. Let do it, let's be consistent. Dna and then spec. I need to now run that, see that it fails. Okay, genome does not have get_genome_dict. That's fine, I can write it. Def @staticmethod, defget_genome_dicts, and it receives the genome and the spec. For gene in genome, gdicts equals a li stand gdicts. append Genome. get_gene_dict on gene and spec. That is that. Just calling my other function that I just wrote and I shall return gdicts. Just to complete that test, I would assert that the length of the dictionary, the list that I get back is, in fact, what I want. I'm going to do self assert equal length of genome_dicts is three, and then I should be good to go. Let's press play. Okay, all tests running. Excellent. Good progress. What do we do next?We've done our get_genome_dicts, we've done that bit. The next thing is to do the interpretation, the developmental process. Just to remind you, the developmental process, the concept here is that we place one linkin this imaginary world and then as we place subsequent links, it then uses one of its parameters on the genome to decide which other link it's going to connect to. Only ever connects to one. If your link is placed in the space right at the end, it has loads of options. It's a developmental thing and that gives us more flexibility in the evolution as well. How are we going to do this?Well, what we're going to do is, basically, have a function which is going to takea genome and turn it into a list of links. Do def testGetLinksand let's do it again so this time we do this. That's my first few steps. I've got my dictionary of genes, my list of gene dictionaries, and then I can then turn those into links. Links equals genome. Genome, get Links. Maybe it can be more explicit. Genome_to_links. I'm just going to simply parse in my genome_dict, because I don't need the other stuff anymore. That's it. Oops. Thats dicts plural. Then I'm just going to assert self. assert equal. The length of links is going to be three. I should have three links because even though we've got the recursion and all that, we're not doing the expansion yet. What we're doing here is just generating the flat links array if you remember back to that video. We create the flat links array, which we're going to then parse into the expander to expand out using the recurrence. For now, we just want to get flat links with all the correct properties. Let's go for that. That test should fail. We run that and the test fails. Genome has no attribute genome_to_links so we need to define that function. It's another one. Static method, def genome_to_links, and it takes the genome dictionaries. I could put a comment in to make that clearer but that's fine. What I need to do is, to do the developmental process, I'm going to have to have an array of parents so I remember all the parents that I've placed on there, and I have to give them names so I know who I'm going to connect to. When I then create my links, I have the appropriate names available. How am I going to do that?Let's have a list of parent names. We're going to start it off with--I'm going to have a parent index, which will allow me to generate the names. Let's just start off with the first name, which is going to be parent index converted into a string, basically. That'd be the first link that I'm going to create. The first link is going to be--I will have an array of links. Sorry, links equal that. Then I'm going to go through and I'm going to add all of my links in there. Let me get that out of the way. For gdict in genome dicts. For all of the genome dictionaries, I'm going to pull out each gene in turn and that's going to be converted into a link. First of all, I need to know what the properties areso the parent_index--Oops, sorry. I'm going to call that link_ind, actually. That is link_ind. Now, link_name is going to be, basically, the string version of link_ind. That's okay. Then I'm going to add one to link_ind every time. So ind plus one. Now, remember how we create these links. We need a name, we need a parent name, and we need a recursion value. We've got the link name there. What about the parent name?The parent_indexis going to be pulled out of the genome dictionary, right?Gdict. If we go and look at the spec we can find out what the name of it is. The spec is joint-parent. Joint-parent, and it's scaled in the range zero to one so that's fine. We're just going to fix it up for ourselves. The parent index is this property scaled by the length of the current available parent names. That means that the index, as we build out this thing, we'll get more and more parent names, and we'll be able to index into that, but the beginning, it starts off parent name. The parent name is going to be oneso it's guaranteed to link to the parent that's called zero, which is the first parent. That's great. We're going to link to that one. Then what we do is we now need it to go, the recurrence is gdict. Let's find the value for that. That is called link-recurrence. That's that property, link-recurrence. That's fine. Now I can create my link. Link equals URDFLink, and the name is going to be link_name. The parent_name is going to be parent. Oh, I haven't got the parent name yet. Oops. Parent_name equals parent_names int cast of parent_ind. It's basically going to use that index, which is a scaled version of the parameter to index into the parent names and select me a parent name. I've got my parent name. Great. Then recur is going to be that other property I just pulled out of the genome. That's easy. That's created my link, and I add my link to the end of the links. Links. append(link). Then finally, what I need to do is add the name of this link to the parent names, right?Parent_names. append link name, because now it becomes one of the available parent names. Finally, I return the links. I think that is it. Let's try to test that one. I'm passing that test. Great. I'm pretty happy with that. I'm now passing my test where I can generate a set of links. What we do next is we go back to the slides. The next thing is to get all the other properties and put them into the link. I need to update the URDFLinkclass so that it can take some more properties inand I'm going to parse those in from the genome_dict. I'm not going to do the whole thing for you because this video would be too longand boring otherwise. I'm just going to show you how I'm going to add one property from the genome into the URDFLink object and I'll then post you the code with the complete set so you can see that. Let's just add one, for the sake of brevity, because it's basically repetitive. What I need to do is, first off, I'm going to add extra properties to this, which is my URDFLink class. If we look at the spec, let's take one example. Let's start with link-shape. That's a little bit of a tricky one, actually, because that's a non-numerical one. Let's do link-length first. How do I add a link length?I'm going to just say, okay, you're going to have a link length, and I'll make it an underscore, and I'll give it a default value. I'll give it a default value because that means that the other code that's dependent on this having only three inputs is not going to break. Let's drop that onto a new line so it's a bit neater. I've added an extra property to that. Now, when I create the link here, I can add that property. I could actually make this a bit easier to read as well. We're going to parse all the properties in a line like that. Then the next one is link_length, and that's going to be, basically, this. I can just pull it straight out the dictionary. I do link-length like that and then that's it. That should work just fine. Let's just go back here. No crashing. Good, so that's basically it for now. What I've done thereis I've shown you how I've done the developments process where we get this gradually growing list of parent name sand we index into that list with each subsequent link that we create. Then I've shown you how to add the other properties, so the recursion, and we added an extra one, link_length. What I'm going to do now while you are getting onto the next videois I'm going to complete the rest of those properties and fill them in. Then we'll come back in the next video and look at the XML. Jumping back to the summary. What I've been doing is we've been working on converting our low-level, raw dna data, the floating-point values, into proper properties of a URDFLink objectso that we ready to later then convert that into XML. This is what we've been doing. In this video, we've just been building out our genome parsing code."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Creature class,https://learn.london.ac.uk/mod/page/view.php?id=96292&forceview=1,"[music]-In this video, we're going to be starting out on the part of the code where we convert this expanded genome into an XML file. In summary, we're going to, first of all, look at some tweaks that I've had to make to the genome code just to fine-tune it a little bit. Then we're going to dig into the creature class, which is the bit that's going to be doing the XML conversion by basically building a bigger wrapper around all the DNA and all the processes that are needed to convert that into XML. First of all, tweaks to the genome. Now, first of all, I think I mentioned at the end of the previous video, we had to add extra genome parameters to URDFLink. Let's have a look at those. I've got my code editor here. This is the updated version of the URDF link constructor. You can see that now it has all of the genome parameters from genome spec in there. I just added them all. You may have a different Python style, and that's totally fine, you might prefer to have a Kwargs dictionary coming in, and then to use a dictionary to store those. I prefer explicitly specifying every argument to the constructor just because I find that clearer and more straightforward. I've done that, and then I assigned them all as variables inside the URDFLink. It's got all the data it needs for when we're going to then convert it into XML later. Then in terms of the code that actually creates that, here's my genome to links code. You can see it's just adding more parameters to the constructor. Instead of just sending the parent in, and the name, and the recursion, it sends in all the other ones as well. That's what I've done there. That's the first tweak. The next tweak is setting the parent name on the root node to 'None'and a little bit of adjustment I've done on the code there. That's in the genome to links code. What you'll notice is how similarly, I've started out with empty links array, set the link index to 0, and set the parent names to that. The parent name starts out with just number 0 in, and then I set the link name using the link ID. I don't increment it yet. Then I select the parent, and then I set the parent name. What this means is the first time this loop iteration runs, they'll only be one parent available and it will also be the only link available. It means the link is linking to itself, and that's a problem because we can't have links linking to themselves. They always have to link to a different link. That's the way it works. What I've had to do is instead of adding every parent--Because I've already added the first parent ID, ID0 into the parent names. I only add them once the value of the ID is greater than 0. The first one is already in there, that means I don't get two 0parents in there. Because if I had two 0 parents in my available parents, it just increases the chance in a way that I don't want of the subsequent links connecting to that parent. Just because there's two of them in there says more likely to choose that one. That ensures there's only one instance of the root parent link in there. Then this code here allows me to set up the first link, which remember, it has to be the root link that doesn't link to anything. It's just going to be itself. That one is now having its parent name set to none so it won't link to anything else. That's how I've tweaked that code just to make sure that I don't get 00, two 0 parents in there and y first link is always linked to noneso that it ensures that it works. I need you to do this because once we plug all this code into the genome, once we're actually pulling those numbers out of the genome, suddenly it becomes more complicated than what we had before where we were parsing in test code like we were hard coding what the names of the links were, and therefore it was all working. Once you pull them out of the genome, it becomes a little bit more complicated. Those two tweaks. The other tweak I've done is that I realized that I wasn't getting the correct number. When I got on to the next step, which is doing the expanded links, I wasn't getting the correct number of links in my expanded links array. I'll show you that test in a minute. The trick was to set the recursion to greater than 0. The recursion has to be at least 1 because I found that I was basically scaling my recursion from 0 up to the maximum recursion allowed which meant that sometimes the recursion will be actually 0. That means if the recursion is 0, it would not even generate any children links off of that link, so it doesn't work properly. You have to have a recursion of at least one for the code to work. These are all things that got revealed to meas I went to the next step that we're going to go through no wand started coding that up. Let's just see where that is. You'll see that in the code where I create the link, the recursion level is now set to recursion +1. If it's 0 it will go up to 1. Is always at least 1 basically. That was the trick there. That's that. Next up, the creature class. We've got our genome class, which gives us all these capabilities. Most of them are static functions. So it's stateless, it allows us to work with DNA, basically but we need a higher-level stateful wrapper which is going to be the subject of our evolution if you like. We're going to call this a creature. It's basically going to be a wrapper class that stores some DNA and is able to call lower-level functions on the genome to turn that into URDF and so on. Let's start out building the creature class. The first thing I want to do is I'm going to create two new files. One is going to be called test_creature. Test_creature. I'm going to create another one, and this one's going to be called creature because I'm going to be coding up the creatureas its own thing. You see, I've now got test_creature, and I've got creature. Test_creature is going to be one of these unit tests. Import unit test and I define a class called Test Creature, which is going to test if she's expanding from unit test. Test. Oops, I didn't spell unit test right. unit test. Test Case like that. Let's define our first test. Creature Exists. All right. Just verify that we have a creature class. Let's import the creature file. The module rather. Self. assertIsNotNone(creature. Creature). We're just going to verify that this kind of creature constructor is Not None. If I run that test, then it is going to fail. It's not going to do anything because I haven't called unit test. main. Let's just do that. Unit test. main. That will cause the test to run, run the code. Oh, interestingly, creature. Creature is not actually None. That's interesting. Is not None. Creature. Creature. Oh, because I forgot to prefix my test with the word test. Therefore, it didn't run that test. Let's run it again. Now it's failing. When you're using unit test, it basically looks at all of the functions you define. If they begin with the word test, then it will consider it to be a test and it will run it, but because I didn't prefix the world test it didn't work. Attribute module creature has no attribute creature. That's true because I haven't created it yet. Let's go ahead and instantiate that. I've just noticed that creature is there. There we go. creature. py. Let's define our class creature, and let's define that special constructor function, and it can just do nothing for now. Remember, minimum code to parse the test even if it's useless. We're parsing the test now. Now we write another test. What I want my creature to be able to do, this is where we get into actually specifying what my creature is going to do. I want to basically be able to create a creature and then it's going to generate a random genome for itself, and then it's going to be able to give me back a set of links from that. A set of the unexpanded links and the expanded links. Let's do that. I'm just going to kill my teams. I could potentially edit that bit out, but that's okay. def testCreatureGetFlatLinks. Let's just verify. This is a basic function I want to create. I want to basically create a creature and ask it to give me its flat_links. That means in order for it to do that, it's going to talk to the genome class, it's going to do bits and bobs and generate a random genome, and everything else. That's a way of just giving me a high-level thing that I want to do. c = Creature, and I'm going to give it a size. I'm going to say gene_count=4 because I want my creature to have parameter where I can tell it how big the genome os going to be, so creature. Creature. Then one thing to do is verify that self, links = creature. get_flat_links. self. assert Greater--Let's get the auto the word assert. I want greater than, the length of the links must be greater than zero. In fact it should be equal. Let's do an assertion on equal. Because if I told it to have a gene count of four, the length of that should be four. Let's run that. You get an unexpected keyword argument gene count. That's the first error I need to fix. Which is, let's get rid of that and that, I don't want that or that, let's trim it down a bit. It's going to have to take a keyword argument first which is gene count. Here is where it gets interesting. We actually have to do some work now. Let's import the genome module. I'm going to say self. dna = genome. Genome. get_random_genome(). Get random genome takes two parameters, gene_length which is a number of parameters on each gene which I don't know yet. What I need to do is also get a spec, so self. spec = genome. Genome. get_gene_spec. I've got spec and the gene length is length of self. spec. Next up I need to code up. Let's see if we can-- yes that's okay, self. spec and then the number of genes is gene_count. I'm still not going to parse the test. Then the next thing is I need the function code get_flat_links. What get_flat_links is going to do, is it's going to convert the DNA into a set of flat_linksusing the genome functions. self. flat_links = genome. Genome. genome_to_links(). genome_to_links just takes as it's arguments the gdicts. How do I get gdicts? gdicts = genome. Genome. genome_dicts(). Then what does that take as an arguments? Hopefully just the DNA. Genome specs, so self. dna, self. spec. That should give me the genome dicts and then I can parse that to the function here, gdicts. Really, it's just using all the stuff we've already built but just kind of giving me higher level of functions on itso that all the sort of house-keeping stuff we've to create DNA and all that's been done for me. Then eventually I can just return self. flat_links. Let's go over to the test code and see if that's working now. Okay, module of creature has no attribute get_flat_links. Does it really?Because it's c. get_flat_links, not creature. That test is parsing, good. It's successfully converting it's links into flat_links. Because I've written all these unit tests on genome, I can be reasonably confident it's doing what I think, it's not just doing some weird random stuff. That's the idea of building on these tests. You build up and up and up and if you're not sure if they're working because you made some changes, you can always go back and rerun them. Regression testing. Next up, I've got my flat_links, what about expanded links?Okay, let's do a test for expanded link sand this is going to be a def testExpLinks. This one is going to do all the same stuff. Then I'm going to do exp_links = c. get_expanded_links. What I'm going to do this time is assert that, the length of expanded links. The length of expanded links needs to be equal or greater than the length of the regular links, which makes sense. If you remember how it works, we basically start out with these flat_linkswhere you've got one for every unique link and then we run the recursive expander which then would at least generate the same number of links again. If the recursion of all of them is one, for example, but the expanded links will have the same numberas in the un-expanded links, but it's likely that one of the mat least is going to have a recursion of greater than one and therefore we'll end up with greater than. That comes down to the test which is that we need to self assert, it should be greater than or equal to the length of link sand length of expanded links, okay. Actually, if you remember earlier, I was saying how to make some tweaks. This is one of the tests that I failed earlier and had to go backand rework the genome code a little bit. Next I need to run that and obviously it fails because it doesn't have an actual record get expanded links. Let's go in and write get expanded links and look how clean my code is here because I'm writing these tests, I'm writing the minimum code to do that. get_expanded_links. First of all, I'm going to do self. get_flat_links, and then I'm going to do self. exp_links = genome. Genome. expandLinksand let's see what it takes. This is where it gets a little bit tricky. The expand links function needs a parent link, a unique parent name, and flat_links and expanded links. Actually, remember that that function doesn't really return, it just builds it out in this array. Expanded links is there and then the parent linkis going to be the first link of the flat_links, self. flat_links zero. That's the first parent. Then the parent name is going to be the name of that in the first instance, . name. Next up we have the, flat-links array, so self. flat_links and then we have the expanded links array which is exp_links. Then at the end what we can do is we can do self. exp _links = exp_linksand I can also return self. exp_links as well. So we got it. That way, by coding this function I generate those expanded links and store them into the creature, so the creature is now stateful, so I'm not using those static method things anymore. The creature when we call get_flat_links and give you these flat_linksbut also generate them if it needs to. Which is possibly a bit naughty because then that means I've got functions that are doing two things. Maybe I should separate it and have one code generate flat_linksand get them or something like that. For now let's just leave at two, that's okay, let's not get super engineering on it. Assertion Error, 4 is not greater than or equal to 10. Now I've got a problem there because actually--Sorry, I've got it the wrong way around. The expanded links should be at least as big as the regular links. I've got it the wrong way around, so let's run that. Great. Because of the random nature of this, when we're testing things that are randomly generated genomes, the test might parse but then next time round it generates different random numbers and it doesn't parse that time. What I sometimes do is I'll just run the test a lot of times, okay. For i in range (100), just run it 100 times. Literally, I just chucked it into a loop to run 100 times and then we can verify. Great, even if we run it 100 times it's not going wrong and I can even go really crazy and set the gene count to 25to maybe just verify even with a massive tier. It didn't even have to think about that one. That's 25 genes and we could even print out. I wonder how big these things are going to be. That's with a 25 genome so that means there's 25 different unique links but then we can have recursive parts of that tree. Many parts of the tree or graph could be recusing out into massive sub branches. Let's see how big these things actually are, right. If we run it a 100 times and I'm just going to print out the length of the expanded links just out of interest, like that. I've printed out the length of the expanded link sand then that would just give us an insight into this. You can see that from those numbers there, for example 1, 710, you can see that I'm ending up with some pretty large robots here because remember it's recusing in and giving you that big sub tree. You can get really complicated with recursion, you can get really complicated elaborate creatures very quickly with very few genes in the genome. That's what's cool about this, but we probably don't want to be simulating creatures of that size because it's just going to take too long, but we can have a look at them, certainly. That's good, right, where are we?We've just been preparing ourselves to actually do the big pus hand generate the XML. We've created a creature class and we've added a couple of functions to it. Before we did that I had to do a few tweaks on the genome code, on that links code and the expanded links just to make sure that I was getting the correct names of thing sand everything was generating in the right sizes. Good we're now ready to get on with the XML generation. In this video we've just been preparing to generate XML."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: toXML,https://learn.london.ac.uk/mod/page/view.php?id=96294&forceview=1,"[MUSIC]In this video, we're going to push ahead with the implementation of our creature parsing code and hopefully get pretty much to the end and have, at the end of this, a complete export to XML, which we can then load into PyBulletto see what the creatures actually look like. Let's get straight into it. My plan is, I'm going to start out by just thinking about how we can actually generate XML tree structures in our code. We've got some code up on the screen here and I've actually got that in my Text Editor as well. What we're going to do, and I'll just show you this inside an I Python environment, because that's probably the easiest way to do it. Let me just clear that. We go into IPythonand we're just going to play around with the XML code for now. We want to do is import this function from the XML library. If you don't have a miniDOM, you can PIP install it. I've got miniDOM installed, and that gives me a function, from which I can then do everything I want to do with XML, which is cool. Let's go in. The first thing is to create a DOMImplementation. It's a generic way of working with XML. That's going to give me an implementation of an XML DOM. Just to be clear, if you haven't worked with DOMs before, then a DOM is a document object model. It's basically a hierarchical data structure that we can use to represent XML. If you think about the XML documents we've been looking at, they have tags inside tags inside tags. That's what we mean by hierarchical. Basically, a DOM is a kind of data structure that represents that eventual text file that we see. Next up, we're going to actually create ourselves a new DOM like this. What we're going to do is actually call it starter. Now we call it starter. Here we go. Let's call it starter. That's going to give usa basically an actual data structure that we can work with. We've got the implementation, which is the domimpl, and from that we can then generate a data structure. We've now got a real data structure that we can work with. Let's say we wanted to create a tag. If we wanted to create, say, a geometry tag, so I could do g_tag = adom. create Element, and then I'll just give it a name, which would be, say, geometry because that was one of the tags that we have in the robot. What can I do with g_tag?Well, I can do g_tag. toprettyxml, and that gives me an XML representation of it. It's a workable data structure and a manipulable data structure. I can then turn it into a string of XML at any point, which is great. What about if I want to add an attribute to that tag?Well, I can do g_tag. set Attribute. I don't know. Let's say we want to set the length to 10 or something and always have to pass it strings as the attribute values, even though it's a float in there. That's okay. I can try that again. You see now it's added the attribute into the geometry tag. Let's say I wanted to add a child tag. Let's say I had a weight tag. Just making this up now, equals adom. createElement(""weight""). I can do g_tag. appendChild(w_tag). Like that. If I do it one more time, I can do g_tag. toprettyxml. If I print that out, then it should do it properly. Now you can see that I can actually spirit out in a human readable way. If you don't use the pretty one, I think it's just toXML, you can see it comes out compact, a compact version. That's great as well if you want minimum file size for your XML. That's the quick intro to the XML code. What we're going to do now is attempt to adapt our URDF link class, which represents one of the links in the robot. We're going to try and adapt that and get it to generate XML. Let's go back to the code over here. I'm going to do it all from test genome because the URDF link class, which we'll just have a quick look at before we carry on, URDF link is in the genome file here. There it is. What we're going to do is we're going to add toXML functions on this and I'm going to build out some of the code to convert that into a link tag. I'll show you how that works. First of all, let's write a test, testLinkToXML. The first one is going to be a really simple test and I'm just going to create a link. I've got some code up here that creates a link. link equals-- end. You'll notice that earlier, luckily when when I was adding all the parameters to the link class, I gave them default value. I won't have to pass in all those gazillions of parameters because you see, they have a default value, all of those ones. That's fine. I can just use that for now, and that makes things a bit easier. I only need to pass in the name, the parent name, and the recursion level. That's okay. I've done that. It's got a name, parent name, recursion level, and then I just want to see, does it generate XML?xml_str = link. to_link_xml. Now, why to link XML?Why not to XML?Let's just remind ourselves. Here's my robot XML file. You see, it starts off with a robot tag, and then in that I have for every robot part, there's a link tag and then for every link, there's a joint except for the first link. The first link, remember, it doesn't link to anything, it's just the root link and everything else joins onto that. Basically, the number of joint tags that I need isthe number of link tags minus one. Anyway, we're going to do the first bit where we generate the link tags for now. Let's go into that. Back to genome, and there it is. What I want to do is look at my test link and I just need to self. assertNotNone(xml_str), like that. I can just run that test and it should fail because you have no attribute to link XML. I need to add a function to that code to link XML. Let's do that. def to_link_xmland it's stateful because it's dependent on its own self. whatever parameters. The URDF link has to have state. That's why we don't use a static method there. I'm just going to earn a string for now because that should pass that test. According to the rules of test-driven development, that's enough. Okay, great. Now, I need to write a better test. The problem is at this point, I don't want to have to write a test for every little tiny piece of the XML. I'm just going to eyeball itand I'm going to load it into PyBullet and make sure it works. What I want to now is just ignore the testing for a secondand just write a bunch of code that's going to generate the XML tag structure. I'll just show you how that works. What I'm going to do is I'm going to jump out of the video and then come back in again. At that point, I'll have written all of the code, which is a bit repetitive, that does all the tags, and then we'll be able to see the XML that comes out at the end. Let's just do a little bit of that code now. If I want to generate links, remember, I need to have this at the top of my file that I want use in. Because it's in the genome, I need to put that at the top. Let's put that all near the top in the imports. I need these two lines to create a DOM. Now the thing is, I don't necessarily want to have to create a DOM for every time I call this. I might actually create the DOM, because the creature's going to be the master of all of this. The creature is going to basically generate all the link sand call out to them, pull their XML and put it together. Maybe the creature just needs to create one DOM and then it can do it. What I can do is, say, well, actually, you need to pass me a DOM here. That means I don't need that line there because it's going to get passed in a ready made DOM. That means the test code needs to have that line because it's going to be doing it. Eventually, the creature will need that. For now, let's just put in the test code. The test code also needs to generate the DOM like this. Let's put that in over there. I just need to do that, and I'm going to pass it when I do that, right. I need to pass it the argument of a DOM. Now, the genome URDF link class has access to a DOM. What it has to do is it's just going to create those tags. We're going to refer back to the 101 document which is over here. There's our 101. Basically, we need a robot. To create one of these links, the top level thing is called a link tag. We need a link tag with an attribute name and then we need visual, geometry, and cylinder. Let's just do a few of those just to see how it works. link_tag =adom. createElement(""link""). I can do link_tag. set Attribute, and it has a name. It's going to be self. name. There we go. That's where I'm glad I've actually got all of these variables inside self because if it was all tucked away in a dictionary, then my code editor might not be able to find all those things so easily. I'm glad I've done that at this point, so link_tag. setAttribute(""name""). Then, what's next?Visual tag, so vis_tag = adom. create Element. Let's call it visual. Then next up, we need a geometry tag, geom_tag =adom. createElement(""geometry""). Then, I need to add a cylinder tag, so let's do that one while we're here. cyl_tag = adom. createElement(""cylinder""). I'm actually going to add a couple of attributes on that. Cylinder has a length and a radius attribute, so I can do that. I can do cyl_tag. setAttribute(""length""), and that is self. length. Oops, self. link_length, is it?Let's have a look. Oh yes, self. link_length. Remember, it has to be a string, so I just turn it into a string like that. Then it's going to have a radius, which is self. link_radius, I think. That's enough for that. Well, let's just return it, then we can build the tree. I'm going to just quickly--I need to add cylinder to geometry, geometry to visual, visual to link. I'll do geom_tag. appendChild(cyl_tag). Then it's vis_tag. appendChild(geom_tag). Then it's link_tag. append Child, and that is (vis_tag). Then, I can do return link_tag. toprettyxml(). Like that. Let's just see what we get backand print just to see that it looks good. Sorry, str. Run the test. Nothing went wrong. Name amom is not defined. I've done something silly. You probably spotted it if you're watching my video. Where did I do it?adom. Oh yes, there we go. I made up a variable name. adom. Back to the test, run again. Let's have a look at it. Link tag with a name. Visual tag, geometry tag, cylinder tag with a length and a radius. Let's check that against the actual 101. urdf. Looks kind of similar. I'm happy with that. I'm going to cut out here and I'm going to jump back inwhere I've completed that for the link tag, and I'm then also going to go ahead and write one for the joints. I'm going to iterate over the links and create joint tags for them as well. I'll give you instructions for this in the worksheet for how I've done it, but it's quite repetitive. It's just lots of link_tag. append Child, link_tag. set Attribute. All that stuff over and over again. I'm just going to go off and write that. I'll come back, and we'll have the XML that we want generating from that function. I've gone ahead and written all of the XML generating code now. We've got two parts, really. We've got toXML on the links, and we've got toXML on the joint. This is a list of all the tags that I had to write and the various attributes. The link tags were easier really because they've just got geometry tags and collision tags. Then the joint tags had a whole bunch of different tags. Let's just review that code to see what's going on there. I did it all in the URDF link class. Here it is. You see why I didn't want to have to type all that by hand. It's quite tedious. Basically, it's the same stuff. We've got the link tags that we saw before, and then there's the geometry, cylinder tag for the geometry using--these are those parameters coming in from the genome. That's where the variation comes in. Then, we had to create the inertia tag, so that's all in there as well. I messed around with that, but you can experiment with these inertia values to see what it does. Yes, you can see the genome parameters are coming in on the mass. Just earlier, I was saying how it doesn't matter about the mass, we've just said it, but I actually decided to do it based on the volume of the cylinderso it was actually realistic, so it's properly calculated there. That's good. Then in the joint tag, obviously, that gets a bit trickier. In the joint tag, we had the name for the joint. We had to come up with names, and that was based on its name and the parent that it's joining to. Then we had to set the type. We've got revolute and fixed joint. I allowed both joint types for now so that some joints won't move, some of them will. Then, we had the parent and child. Again, that's genetically encoded remember or calculated based on who it's going to link to. Then, we had the idea of the placement of this particular link relative to its parent. This is where the axis xyz's and so on come in, and also the other things which are the rpy and xyz. That's to do with the rotations and the different things. I think this axis is dictating how it rotates if it's a revolute joint, and then this stuff dictates where it's placed relative to the parent. Does it rotate and place that way?That's the basics. I think we're ready now to actually generate a full XML and push it into the PyBullet to see what it looks like. Let's do it. Just finally, before we get to that, sorry, in the creature class, I just quickly wrote a toXML function in that, which basically just pulls up the link and joint tag and iterate for each of the link sand apart from the parent, there's no joint on the parent, and then smashes that all together, adds it onto a robot tag, and finally outputs it as XML. We're ready to go. I've got a test creature function up and running here. If you look in here, this is it. This is all the code that we need. Remember, I was talking about it being a high-level class earlier. This is how it works. You basically create a creature, tell it how many genes you want up there, so gene count. Then, you expand the links, and then you just call to_xml on the creature, and it will do the whole work of iterating of all the link sand generating all the joints and everything, and eventually give you back an XML string, which we then write out to 102 up there. With the open and f. write, we stick XML at the beginning and check that it's not empty. Let's just run that, and hopefully, nothing will go wrong. We should be able to see. The test works, so that should've worked. Now, I'm ready to go over to PyBullet and load that into my code. Where is my PyBullet environment?I do ipython. Let's put that up there. What we're going to do, we got that start to code, so we should run starter there. There's my PyBullet environment. What I want to do is just load in that URDF file that I just created. Let's do p. load. We'll assign it c =. Let me just pull that up in case you're reading the subtitles. c = p. loadURDFand the file name is ('102. urdf'). Watch above me and let me get out of the way. There it is. It generated it. No complaints because I've got all the right tags. Therefore, I set the simulation to run in real time. I set that to one. Let's see what it does. There it is. There's our robot thing. We can pull it around. There it is. That is our first randomly generated creature. That's pretty interesting-looking, isn't it?You can see it's got a few different parts there. That's all very exciting. We've got a creature, and we could now go through and we can start running the engines on it. We can turn the motors and see if it moves. Let's try doing that next just while we're having fun. Let me get out of the way again. I'll zoom in on it a bit. What we can do, setJointMotorControl2. We can do things like setting the position. If we do, say, c and joint 1, I'm going to do, set the position to mode=p. POSITION_CONTROL. I think I can just pass in a position like this. Let's see what happens. Positional argument follows keyword-- Okay, all right. I think you probably saw that moving there. Let's try that again. If I set it to-- Whoa, okay. Remember, when we do this position control thing, it's rather rapid, isn't it?We can't get a really good view on it. Let's try another one. Okay, 2. 5. There it goes. Yes, so we can move the joints around. That's pretty cool, isn't it?We've got a robot that we can interact with, and we can move the joints around. It was all generated from those raw random numbers, and of course, I can generate another one. Of course, let's do that. If I run the test again, I'll get another file. It'll overwrite my original one, but that's okay. Let's get that over there. I'm going to get out of the way. Let's see if we can load up another robot. This time, we're going to do c2 = p. loadURDF, 102 again. Okay, well, there's another one. Let's see what that one's going to do. You can see that one's obviously got a different shape. Somehow similar, but different. That's just showing you loading another robot. I want you to run this code now and play around with it. Generate some random robots, and try varying the number of genes, because this is only with a low number of genes. You can also mess around with the parameters. I'm going to come back in the next video, and we're going to see how we can improve this a little bit because there are some limitations that I wanted to talk about. There's my summary. We've basically been looking at this toXML processing code, essentially. We've gone through, we've seen how we convert all those genome parameters so that they can be generated into tags, and we built up gradually from all the parts. Now, we have the ability to actually render out XML which we can load in without error, importantly, into PyBullet. In this video, we've just completed the whole process from random numbers in the genome into XML that we can load into our physics engine. Now, we need to just tweak that a little bit, which we're going to do in the next video. Then, we're going to figure out how to get the motors working in the robot."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Fixing positions of children,https://learn.london.ac.uk/mod/page/view.php?id=96299&forceview=1,"[music]In this video, I'm going to identifya problem with the geometry of our robot that might not have been apparent when we're generating random robots. In summary, I'm going to talk about the problem, which is overlapping parts, and then I'm going to give you a quick solution, which is a bit hacky, but it works, so I'm happy with it for no wand we'll proceed from there. Here's the problem, actually, I'm going to draw this one out. Say we have a flat link structure like this, where we have A, and we have B, and basically, B has got a recurrence of two, so it's connecting twice to A, so we end up with two B. When we expand that graph, as you'd expect, we end up with an A, connected to two Bs, like that. That's great, that's all very well, but the problem with that is, it's not how it's reflected in the geometry of the robot. In an abstract sense, the graph does look like this, but in terms of the physical geometry that will generate, it doesn't actually look like this. This is what it does look like, it will actually be like this. We'll have our A, and we'll have our B, we'll have A and Bwe'll connect that, and then we'll just create another Bin exactly the same place, and we'll stick it there. It'll be correct, there will be two Bs, and there will be one A, but the Bs will be in exactly the same place because we're not taking account of the fact that in our geometry generation, that there might be more than one child or sibling, if you like. How are we going to solve it?We're going to go into our code and we're going to basically implement some stuff. Before we do, I want to show you what it looks like, what I mean, exactly. I've got some test code hereif I can just dig through my errors. Here we go. This is my test code. What it does is it just hard codes a couple of links, so we got your URDF link A and B, and then B connects to A with a recurrence of two. I've set it up so that it's really obvious. It should have a rotation of 0. 75 radian son the X-axis, I think that's X-axis, and it should have a length of 1. 0. The default length will be 0. 1because it's a default parameter on the URDF link. The B link should be really big. What we want to see is our two big long links running. If I run that code, and render out the URDF file, and then I fire it up in i Python, so I do run starter, and push that over there, and then-- get out of the way, and then we do C = p. load. Yes, there we go. I'll do p. setRealTimesSimulation(1). There it is. You can see pretty obviously, that it doesn't look like it has two links, does it?You can just see that one little blob, and then the two little--It looks like one bar sitting next to it. Let me drag it. Let me see if I can pull it a bit closer. There we go. That's that, that's the problem, is you've got this one blob, and then one bar. One is this sort of--I want the bars to fan out because there's two of them, so that they fan out like that. Let's see if we can convince it to do that and I'll show you how I'm going to do it. I'm going to go back into my genome class. First thing I need to do, is I need to go into the URDF class actually and tell it that it needs a new parameter. In the constructor of URDF, I'm going to put a parameter which is going to be called sibling index, self. sibling_index = 1. It's going to default to one, but that just allows me to know that it's there. It's going to have a property and then how am I going to use that property?The higher that number is, the more rotation I'm going to give myself, but proportional to what the rotation is encoded in the genome, so it's still parameterized for evolution, but it's still going to be proportional. To do that, I'm going to whiz down hereinto the nasty XML code. I'm looking at this line here, which is where I calculate the origin tag, which dictates where the thing is relative to the parent. I'm going to pull this value out and calculate it separately. I'm going to do rpy1 =self. joint_origin_rpy_1* self. sibling_ind. That line there means that the rotation will increase depending on which of the siblings I am so if I've got multiple siblings, it should increase. I'll just put that parameter in here instead of the original one, (rpy1). The next step is I need to assigna correct sibling index to the URDF links as I'm generating this. I'll only know which sibling of the siblings it is, I only know that when I do the expanding links code. That is in expand Links, which is up here somewhere. Where is it? There we go. Expand Links, there it is. It's this function here, I need to update this. This is the one that iterates over the children and iterates through the recursion and knows which sibling it is. What I'm going to do is do something simple. I'm going to do sibling_ind = 1and then I'm going to dosibling_ind = +1. That way, it's going to increase every time we recreate a new sibling. I just need to then assign it to the copy. C_copy remember, is the URDF link that I'm generating as I go through the expanded thing and it's sibling_index = sibling_ind. That means they will now have a proper sibling index to rotate themselves. That should be more or less it. I've written the bit of the code that generates the origin tag, so it takes account of which sibling it is, and I've updated this codeso that it correctly assigns a sibling or at least it assigns an increasing sibling number to that. Hopefully, I can now run this and generate a new--no crash is good. Now I can generate a new URDF file, which I have, and have a look at it. Let's go back over here again, and load a new URDF file. I'll get out of the way so you can see it. Hopefully, now you can see that the two sticks that make up the robot are nowfound out, they've rotated. If I was to increase the recursion to say, recur=6, hopefully, I'll get 6 found out rotated and I reload it again. That one, you can seeas we increase the recursion, they fan out. That's giving me the geometry that I want, as well as the abstract graph structure I want. I've now got the-- that's now being applied to the geometry as well. Imagine if I now have another link that hangs off of B, I could then fan that one out as well. That's all good. I'm now finding outand it's parameterized in the genome still, although it looks like I've hard coded it. It's based on that parameter, on that parameter that comes in from the genome. If the parameter is smaller, then you'll get a small fan, if it's higher, we'll get a big fan. That will allow it to come up with all weird combinations, and so on. That is the endof this bit of coding. In summary, we've just been looking at the problem of overlapping robot parts which is caused by recursion and not changing the angleas we create them. We looked at a solution which is to have a sibling number, sibling X of Y children so we can now work out how much to rotate it by, based on which sibling we are. In this video, we've just been solving a little problem with the geometry of the robot."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Adding motors: creature.update,https://learn.london.ac.uk/mod/page/view.php?id=96301&forceview=1,"[music]In this video, we're going to build the motors for the robot so it can start moving. First of all, we're going to have a quick reminder of how motors are going to work hereso that we know what we're aiming fo rand then we're going to write a motor type enum. Then we're going to do a motor constructor, which allows us to create a motor, so we're going to have a motor class to represent these things. Then we're going to write the get_output function on that. Then we're going to write a test scriptso that we can evaluate these things and see them running in real-time. First of all, reminder, how do motors work?Well, I don't mean motors in general, I mean the motors that we're going to have in here. Remember that we can use these functions that are available in PyBulletto set up the controls on the motor, and we're going to be using this velocity control, which means that essentially, you've got a motor, which can rotate in one dimension and it's on one plane. What we're going to be doing is we're going to be setting the speed of the motor or the velocity. We're going to basically say, are you going that way or are you going that way and how fast. We're going to do that using a repeating waveform like a periodic waveform. We're going to have two types of waveforms, we're going to have a square wave or a pulse wave, which is going to essentially go from 1 down to -1, and it will essentially set the speed so the speed will start off at 1. It'll be maximum speed going that way and then at some point, it's going to switch maximum speed back to that way. That's the pulse wave, and then we're going to have a sine wave, which is similar, but it eases in and outso it doesn't suddenly switch to the other, it's going to slow down. It's more of a natural movement that which maybe will give a different momentum to the creatures as they walk in so that's why we're having two different ones. Those are the two motor types we're going to have. The first thing we need to do is go into the creature code, which I have over here and we need to write a motor type enum. First of all, we import the enum class from the enum module. I'm going to define a class called motor type because this is going to make my code more readable later, this is why I'm using it, and it's extending from enum. It's going to have just two fields, one is going to be called pulse and that's going to be 1, and the other one is going to be sine and that's going to be equal to 2. That was easy. The next step is to write the constructor for the motor class. I'm going to define my motor class no wand it is going to have a constructor, so def_init. The constructor is going to take their self argumentso that it has access to its own state and then it is going to have three more arguments, which are going to be the control_waveform, control_amplitude, and control_frequency. Let's just remind ourselves what we've got in the genome spec. The genome spec defines all the different properties that we have in the genome, and there it is, and you can see that at the bottom there, we've stashed away three parameters for the motor. We're going to be using those and we're going to come back to those. Luckily, when I built the URDF class that also, there's the URDF being created there, URDF link, it actually receives these values as well. I've already got these values that I need, this data that I need to create my motors, I've already got that stashed away on the link, so that's good. We'll come back to that, though. We're going to have those three values from the links. Great. What are we going to do?We're basically going to make a decision about what type of motor we want to be. If the control waveform is smaller than or equal to 0. 5then self. motor_type is going to be Motor Type. Pulse. Else, it's going to be sine. Okay, great so that's good. Then the next thing is that we need to just stash away the control amp and the control frequency. We just do self. amp equals control amp, self. frequency equals control_frequency. Not much point in doing any tests for this yet because it's pretty straightforward. Now I can construct a motor. I could write a quick test just to make sure that it's not going to fall over at this point. Let's just write a very quick test motor, so def test Motor, and it's just going to say m equals creature. Motor, and it's going to pass it 0. 1, 0. 5, 0. 5 self. assert(m). I'm just going to run the code to see if I've made any silly mistakes. Good, that's working. Great. Now back to the creature script. Next step is to generate the output from the motorso all it has to do is generate a periodic waveform. I'm probably going to use a library to do this, but so easy, there's no reason so def get_output, so get_output is going to be called by the simulation eventually to set up the speed of the motors over time and update over time. Actually, it needs to remember a phase. I'm just going to do self. phase equals 0 as wellin the constructor, so it has a phase. Essentially, I now need to do another test depending which waveform it is. If self. motor_type is Motor Type. Pulse, then I do one thing. Let's just put the stuff in for now. Otherwise, if it's a sine, I'm going to do something else. If it's a pulse, what I want to do is if the self. phase is less than np. pi, then output equals 1, else, output equals -1. Basically, the phase has to be updated so that it changes over time. Let's do that self. phase = self. phase + self. frequency, that is a cheap way to use a frequency basically. The higher the frequency, the more quickly the phase gets updated. That's okay and if the phase goes over, basically, pi, then that's going to generate a -1, otherwise, it's going to be a 1. Now, I'm going to modulate this actually on 2 pi to make sure that it wraps around on 2 pi. I'll do Modulo np. pi * 2, that means the phase is going to go up when it gets to 2 pi, it'll wrap back around to 0 againso that makes sure that my pulse wave is going to be able to generate both types. Then if it's a sine wave, then I just do output equals np. sine and the self. phase, easy. That's it. Then I just return the output. Now I can write a quick test for that one as well. Let's go over here. I know I'm breaking the rules a bit here. I'm breaking the rules of unit testing, don't tell anyone. I wrote the code first and then the test. Then def testMotorVal. Let's just do this again, self. assert Equal, that should be a pulse 1 because it's got a 0. 1 coming inso motor get_output should be equal to 1. Let's run that. Great. I passed my test. Perfect. I think I only got three. Yes, that's right. We've got a motor that works. Let's do one more test just to check the sine wave version. Since we've written all these tests before, no point giving up now. Oops. I'll just do a one that generates a guaranteed to be sinewaveso if it's 0. 6 it should be a sine wave. I'm going to call get_output a couple of times. Okay. Oops. Sorrym. get_output. I did, the typing is all gone to pot. Let's just keep going now. I'm calling get_output a couple of times just to set the sine wave away from 0 and then hopefully, I can do self. assert the output is greater than 0. Let's try that. Good. That worked as well. It's got the same name. Let's call it motor value 2. Next, we run it. Great. That's passing as well. I've set it as a sine wave, and I've verified that it's generating a value greater than 0. I now have my motor class written. My next step is to get that integrated onto the creatureso that it can generate the motors for me. Let's very quickly go over to the creature and tell it to generate the motors. Where's the creature? There it is. It's going to have a new function called def_get motors. The thing with the motors is you need one less than you have expanded links if you like. Remember, the link graph, the first link, the root one doesn't have a motor because it's not connected to anything but all the other links do. You need one less motor than you have links. That's what we're going to do. We're going to say, for i in-- Let's start simple here. Motors equals that. For i in range, length of self. exp_links-1. That's going to go from 1 up to there, -1. Oh no, I'm sorry, not -1. We go from 1 up to the length of links, if that makes sense. I just need to do motors. append motor-- Let's do it properly, actually, so m equals motor, and I need the link as well. L equals self. exp_link(i) and the motor is going to be Motor (l. )and I need to pull properties off of the link. What are the properties called?Let's go over to the genome and check what the properties are called. They're called control_waveform, control_amp, control freq. That's sensible. Good. So (l. control_waveform, l. control_amp, l. control_frequency. That's my motor. Then I just need to append the motor to the motors. I think that is it. I've now got all my motors, and I can just do self. motors equals motors. I can even return self. motors. Now, what I might do at this point is put that assertion in. Remember, I had an assertion up here. Just to check that I've called expand links first, and then just say call before get_motors. I've done that. Also, if self. motors is none, only do this if self. motors is none because no point in regenerating them every time. Also, I'm going to be repeatedly coding thisevery time I want to update the motor, so I don't want to have to do it like that. Then I just need to fix that. I need to do self. motors. I've already done that. Great. My creature is ready to go. Now I can call a test on it. Let's go test creature. I'm going to do testCMotor. This is basically testing if I create a whole creature and get the motors off that so let's just grab some code from up here. That is easy, so C equals creature. Creature, gene_count equals 4 and ms equals c. get_motors. I need to do c. get_expanded_links. L's equals get expanded links, and I can just verify that the number of motors is one less the number of expanded links. Self. assert equal length of ls-1, length of ms. That works?Yes, so it's basically checking that I have one less motor than I have expanded links because we've got the root link we don't want, so let's run the code and check if it crashes. It looks like it got a little problem there. Attribute error numpy, has no attribute l motors. L dot, okay, because I put a dot here by mistake. Back to creature. I was going too fast. Where is it? Get motors. There it is. Go back to the test. The test is good and that it runs the code and sees. That's passing that test. That's great. I've got some motors. They seem to work and generate a sensible output. I could do more tests, but that's good enough for me. My next step is to end this video because I'm happy with where I've got to. In the next video, I'm just going to write a very quick test script, where I basically generate a random creature, and then chuck it onto the screen, then move its motors. That'll be our sort of whole genome development process complete and tested, and will be ready to then implement the actual algorithm which evolves these things. Let's do that. I'm going to jump off the screen no wand just summarize the end there. So just in this video, I've just been building out the motor implementation. In the next video, I'm going to finally just builda test script to test the motor sand see if we can get them to move a creature around on the screen."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Lesson 3 summary,https://learn.london.ac.uk/mod/page/view.php?id=96305&forceview=1,"[music]Well done, you made it through the end of all of those videos, which is one of the more intense weeks in this course, although next week's even more intense. Headline shocker there. [chuckles] Anyway. What we've been doing is we've been learning all about how to express solutions to a problem in terms of genetic encoding schemes. We've actually built a genetic encoding scheme with which we can encode the characteristics of a robot with a three-dimensional graph structure, very similar to that that was seen in the Carl Sims project. We also implemented revolvable motor controls as well. Finally, when we come to loo kat more state-of-the-art systems later in the course, you're now going to have a good insight into how those genetic encoding scheme scan work and you will be able to analyze those and compare and contrast them more effectively. Well done. I'm reaching the end of Week 3 in the creatures case study where we've been building the genetic encoding scheme. In the next week, we're going to be actually implementing the simulation for real and actually completing the implementation of the genetic algorithm. at the end of next week. We will see our creatures evolving"
DSM100-2022-OCT,Topics 5&6: Creatures,Lesson 4 Introduction,https://learn.london.ac.uk/mod/page/view.php?id=96307&forceview=1,"[music]Welcome to Week 4. That's great. You made it through Week 3 and you came back for more. We're going to be doing some more intensive coding this week because this is actually the last week we're going to be spending doing a lot of development work. At the end of this week, we're going to have a completely functioning genetic algorithm which allows us to evolve creatures to this problem of moving around in the world. The learning objectives are listed here. We are going to be able to explain how fitness functions and population models are used in genetic algorithms to optimize a set of solutions. That means we're going to be learning about what the purpose of a fitness function is, what the purpose of a population model is. Secondly, we're going to be implementing a fitness function and a population model, then we're going to be using it to evaluate the creature sand to maintain a population of creatures that evolve over time. Finally, we're going to be using genetic manipulation techniques to create variation and cross-breeding within a population. That's how we get access to the special sauce of a genetic algorithm which allows it to do this multi planar sampling or searching where it's sampling and searching in different place sin the solution space as it were. Welcome to the beginning of the final intensive week of programming for the creatures case study, which is Week 4. There's Week 5 as well where are we going to be looking at some papers, but for this week, it's more programming. Good luck. Remember that if you find the videos are a bit too long and intense, that's okay. I'll give you further instructions in the lab worksheet sand also give you the codeso that you can fully recreate what I'm doing in the videos by just working through a worksheet if you prefer to work that way."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Evaluating creatures: the plan,https://learn.london.ac.uk/mod/page/view.php?id=96311&forceview=1,"[music]-In this video, I'm just going to give you a quick overview of the plan for implementing the next part of the genetic algorithm. Let's just jump over to my summary here. First of all, we're going to have a quick look at the genetic algorithm, just to remind ourselves what the whole process looks like and where we're at in that process. Then we're going to be looking at how we can generate a random population which is going to be pretty straightforward actually. Then we're going to be thinking about how we're going to run all the individuals in a population in simulation and then how we're going to evaluate them by implementing a fitness function to measure how far they've actually moved. First of all, here's a reminder of the genetic algorithm. We start off on the left there with a varied population with heredity. That's the random population. What we don't see in the GA-- [chuckles]what we don't see here is all that work we've done to make this really complicated genetic encoding scheme, but that's fine. That's not a generic part of the GA. That's a specialist part that we spend a lot of effort on for a specific problem, which is the creature's problem. The second phase is the selection phase and there we need to do some sort of test. Essentially, we're going to need a random population of individuals, then we're going to need some sort of tests, and then finally, we got this phase behind me here which is where we basically take the things that get the highest scores in the test and we make variations of those. We're going to be doing that in a later video. What about generating a random population?Well, for now, we'll just going to do it in a very straightforward way. We're just going to essentially create multiple random genome sand wrap those up in a creature class, and then that will be our random population. It's as simple as that. Then the next step will be running in simulation. We're going to create a new class again. We had a population class for the random population. We're going to have a simulation class for the simulation. This is going to do slightly more complicated stuff. It's actually going to be able to create a pybullet environment which is going to run in direct mode which is a different mode. Normally so far we've been running pybullet in GUI mode where it pops up the user interface and you can see the objects that we create moving around in real time in 3D. Direct mode is different in that it runs offline, it doesn't try and render everything out in real time as it's happening. The idea is it's a lot faster. Because we're on the run lots of these individuals in simulation and quickly evaluate themin order for the GA to complete in a reasonable time, then we need to be able to run it fast. That's where the simulation direct mode comes in. What we're going to do, we'll create our direct mode simulation. We need to instantiate an individual inside the simulation which is similar to what we've been doing already in a previous video we saw just generating a random individual and running it in real time. It's just very similar, but we'll just be in direct mode. Then we're going to have to step through time, so run the simulation as quickly as we canand update the motors as we goso that the creature is correctly moving how it should. At the end we have to end the simulation and see what happened basically. That's the simulation. Also within the simulation I guess is we're going to have to figure out what the individual actually did in the simulation, because if we just create in pybullet and then shut the simulation down, how do we know what actually happened?We going to have to add some variables to our creature class which allow us to essentially monitor what it's been doing. Simply put, we're going to look at the start position and the end position and we're going to measure how far it moved. That will be our fitness function which allows us to evaluate the individual. The farther it moves, the more fit it is. That's just a brief overview of what we're going to be doing in the next few videos. We had a quick look at the GA, and then we're thinking about random populations where we evaluate all the individuals by running them in an offline simulation. At the end, we're just talking about the fitness function and how we're going to essentially measure how far these creatures move in the simulation. In this video, we just had a quick overview of what we're going to be doing to evaluate a population of individuals."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Generating a random population,https://learn.london.ac.uk/mod/page/view.php?id=96313&forceview=1,"In this video, I'm going to be implementing the population class. In summary, what I'm going to do is create a new module, then I'm going to use the creature class inside that module to create a list of creatures and then we're going to be done. Okay, let's jump straight over to the code editor so we can see what we're going to do. The first thing we're going to do is create a test population file, because we're going to do test driven development as usual. I'm going to create a new file. I'm going to save it as test_population. py, and I'm just going to quickly write the boiler plate code for doing some basic unit tests so I do import unit test and I'm going to create a class called Test Pop, which is going to extend or inherit from unit test. Test Case and then I'm going to write my first test function, which is going to call testPopExists. It's going to say, well, let's import the population module, which we haven't written yet and then we're going to assume that we can do population. Population to instantiate a new population and it's going to receive two arguments, which are going to be the population size and the gene count, which in this case are going to be, 10 and 4. Okay. Then I do some assertions of IsNotNone pop. Then finally, I just need to write the code to actually execute the unit test so I do unit test. main. Let's run that and see what happens. Okay. It fails. We knew it was going to fail. It's failing on the fact that there is no module named population yet because I haven't created it. So let's create it. Another new file. This one's going to be called population. py. I'm going to write the minimum code to pass the test, which is to implement a class called population, and population is going to have a constructor, which is going to take the instance of the object as its first argument, of course, and then it's going to have a pop size and a gene count parameter. It's for now, just going to do pass because that's all we need to pass the test and then I go over here and I run the test again. Now you can see it's running one test and passing it, so that's good. I've got my basic population class mapped out. The next step is to maybe verify that it's actually got some individuals in it, so we can do def testPopHasIndis self, and I'm going to do the same line there, but this time I'm going to assert that the number of individual sin the population is 10, which is what I've asked for, so I'm going to do self assert equal the length of pop. creatures call it actually creatures and 10. Right? See there. Basically, I'm just going to check that the length of the population creatures list, which is going to be inside the population, is 10. Remember, the concept of test driven development is we are writing the specification, we're writing active specification, which we can actually use to evaluate our code, which is what makes it really powerful. I'm saying I need this function to have updated the creature object, sorry, the population object so it has some creatures in it. Of course, that's going to fail because it doesn't yet. It just does pass. Population has no attribute creatures. We know that because we haven't added it, so let's go back over to our population. Now we're going to do something like this, so we're going to do import creature. What I want to do is say self--well, now we're going to create a list of random creatures. Self. creatures equals, we can do it like this. We can do the list comprehension so we can do creature. creature. Let's just check the parameters on creature there. Okay. The base glass, gene count, it just takes a gene count. Gene count equals, and we just do gene count like that, so that for i in range, pop size. Okay. I've just done a list comprehension. Let me just zoom out a bit so you can see that and get out of the way. That's my list comprehension there. It's a bit of a trick. With the list comprehension, you assume that the first thing in the list comprehensionis going to be the thing that you're going to be-- that's going to come from the iteration, but mine doesn't. I'm just actually just calling the creature constructor here. Look, I'm just calling this creature constructor as the first thing in my list comprehension, therefore I get a list of creatures basically, which is pretty neat. Then the number of times that the iteration runs is i in pop size, so it's up to 10 and that means I get a list of 10 creatures, which is pretty nifty. I'm storing it onto selft. creatures. I can now remove that because I'm doing something and that hopefully will pass the test. Ran two tests. Okay, great. I think that's more or less all I need from my population at the moment. Just there's my tests and that's it for the code. I'm going to stop there. Let's not get too wordy and continue to the summary. Yes, we've just implemented a very simple population class, which allows us to generate a random population of creatures and to access those through the creatures variable and so in the next video, we're going to continue with the simulation. In this video, we've just been working on the population class."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Running creatures in simulation,https://learn.london.ac.uk/mod/page/view.php?id=96315&forceview=1,"[music]-In this video, we're going to do the hard work of building the core code for the simulation. In summary, we're going to start by creatinga new module with a simulation class in it. Then we're going to create a run_creature function, which allows us to run a creature in simulation. Then we're going to look at the update_motors function or create the update_motors function, which moves the creature while it's in simulation. Finally, we're going to look at a way of seeing where this creature is at the beginning of the simulation and at the end, which will lead into the fitness function that we'll implement later. First off, building the simulation class. Let's go to the code editor, what I want to do is, first of all, create a file, which is called Test Simulation. Now, I'll just say that I'm not going to write really detailed unit tests for every single thing that I'm doing because I just want to get through it in a reasonable time. I'm going to use the unit tests as a way of running tests against the code in a more basic way. We'll see what that means as we go on. Let's import the unit test, and let's create a class called Test Sim. What we'll do is we'll start by defininga test basic, does it exist, test. We'll do sim = simulation. simulation, and self. assertIsNotNone(sim)and I need to import simulation as well. Finally, I need to do unit test. main. Okay, so, let's run that. Import simulation is throwing an error because I don't have a module called simulation, which makes sense. Let's go back and create that. I go, Create and then Save As simulation. py. In this, what I wanted to do is basically write a simple simulation class. I'll do class simulation, and I'm going to define a simple constructor. Just pass for now. That should be enough to pass my simulation test. Okay, ran the test, passed it. Okay, great. What's next?What we want to do next is start thinking about setting up a pybullet environment. When we instantiate a simulation object, we want it to go off and create a pybullet environment, ready to run the physics in. How do we do that?Just before we get into it, every function in pybullet, more or less, has an additional parameter, which we've not seen before. I'll just show you an example of that. If we go into, say, reset Simulation, which is one of the functions we're going to be using, so reset Simulation has this physicsClientId parameter. That means that when you call these functions, you're telling pybullet of the several physics engines that are available, which one you're going to be calling that function on. That means you can have multiple physics engines running inside the pybullet environment. That's what we want to do because we want tobe able to make this multi-threaded eventually. How do we do that?What I want to do is write a test to say, do we have that value?TestSimId, and so let's create a sim. We're going to do the same thing again, but we're going to check that it has a physicsClientId. self. assertIsNotNone(sim. physicsClientId) so that's what we need to do. That test is failing, you can see because it doesn't have a physicsClientIdyet because I haven't created a physics engine. Let's go back to simulation and do that. Import pybullet as p. What I'm going to do instead is I'm going to do self. = p. connect(p. DIRECT). Now, you'll notice that previously, what we put there is p. GUI. p. GUI creates a physics engine with a graphic user interface that we can then run in real-time. We can see things moving around, but we don't need that. What we need is a physics simulation that's going to run as fast as possible offline, and potentially run it in the multiprocessor setupso that we can run several of them at the same time. That's what the direct mode allows us to do. It allows us to have an offline physics engine. That's it. That will create a fresh physics engine for this simulation and remember its ID so that we can use it later. No module names pybullet. I just need to make sure that I'm in the correct Python virtual environment. Sometimes when you start your code editor, you might not be in the right virtual environment. I'm going to make sure I am by telling it where Python is. Python is there, so what I can do is I can just whack that in, and so that's the path for Python. Let's try again. Okay, great. That test is now passing because I told it to runin the correct virtual environment with pybullet. The next thing I need to do is, I've got my unique physicsClientId, I need to get on and write the run_creature function. The run_creature function is the guts of it really. This is the thing that actually takes a creature and runs it in simulation. Let's go ahead and start coding that up. I'm going to, again, write a test for it. I'll do def test Run. It's going to say, I'm going to import creature into this because I'm going to have to create a creature to run in a simulation. We can do Sim, and Sim is there, and cr = creature. Creature and I need to do (gene_count = 3). I've created a creature, and then I can do self. assertIsNotNone(sim. run_creature). Actually, I don't need the creature yet, so I can just come in the [?] now. Okay, now let's run that. It's going to tell me, ""Okay, you failed a test because simulation has no attribute called run_creature. ""That's the problem, so, I need to go and implement that. Here we go, so, we define a new function called run_creature. It's going to take as its arguments, the self because it's stateful, it needs access to the simulation. There, we need self, and then we have a creature, which we're going to call cr. We're also going to say how many iterations we want to run it for, but I'll default back to 2, 400. The reason for that is that, in the pybullet environment, what it does, when you run it in real-time, is it actually updates the physics engine 240 times a second. It might not update the display at that speed, but it iterates on the physics engine 240 times a second. By choosing 2, 400 there, that gives me 10 seconds of simulation if I run it for that many iterations. Run_creature now exists, and I'm just going to pass, so that function should run now. I'm running all the tests, I'm passing. Great. That's good. The next step is to continue working on my run_creature function. What's it going to do?It's going to reset the simulation, it's going to set gravity, it's going to write a creature file to XML. Let's get to that point because that's all fairly straightforward. I'm going to go over here, and what I want to do is say, p. reset Simulation. The simulation I want to reset is going to be the one with this Client Id. I'm going to pass it that. I'm going to have to do this for pretty much every time I call one of the pybullet functions, I'm going to have to pass it the ID of the physics engine which I want to use. Reset it, and then do set Gravity. I'm going to set Gravity to-- In fact, what I'm going to do, just because it's a little bit verbose, having this self. physicsClientId all the time, I'm going to do pid equals that, and then I can just drop pid in here. Just because that's a bit neater. Okay, now, set Gravity, and I'll set Gravity to 0, 0, -10, and then again I need the physicsClientId argument = pid. That'll reset the gravity and the simulation. The next step is to figure out how to get the robot or the creature loaded into the simulation. In order to do that, I need to render it out to XML. Now, at this point, we can get into some engineering discussion about which bit of the program should be responsible for writing the XML out to disk. Should it be the creature or what?I'm going to decide that it's going to be the creature that generates the XML, the string, but the thing that's using the creature is going to be responsible for what you do with that string. In this case, it's the simulation that's responsible. What we do, I'll just need to adapt the creature. It's got a couple of slightly annoying features, which is that you have to call certain functions before it will work properly. What I'm going to do is I'm just going to call self. get_flat_links. I'm going to do self. get_expanded_links. The creature, that means the creature is going to be ready to use before I run it. Now, next thing is I'm going to go into my simulation and-- Actually, no. Before I do that, I'm going to go into my test, and I'm going to write a test, which is, after run Simulation has been called, I want to make sure that the creature has actually been written to disk. Let's just get this whole test here and testRunXML. Then we're going to create a creature, and we're going to verify. After we run what we're going to call sim. run_creature, passing it the creature. Then I want to verify that a file has been written to disk. Now I'm going to decide that the filename is going to be tempt. urdf. I'm going to do self assert. Self. asserTrue(OS. path. exists. It's going to be temp. urdf. That will verify that, by running creature, a temp. urdf file was generated, and it will say I need to import OS. That will verify if that file has been written to disk. If I press play again, you see that's now failing that test because false is not true because the file doesn't exist, so I need to go back to simulation, and I'm now going to write the file to disk. I'm going to do XML file='tempt. urdf'. That's the file name. xml_sr=cr. to_xml. Fine, and then I'm going to do withopen(xml_file, -f. write(xml). Sorry, that should be str. F. write(xml_str). Okay, so that should be enough to write the XML file to disk. If we go back over here and run the test again, hopefully now, it is passing. Yes, it's passing that test now, so, the XML file has been written to disk. I can even go and have a quick look at it to verify that it is what I think it is. In theory, because it's a random creature, if I run the test again, it should change. I don't know if it's changed or not, but it probably has. We could even write a test that checks that it changes. What's next?We've written the file to disk, so the next step is to read it back in again. I now need to do cid =, and I'm going to read it in, so, I doloadURDF(xmle_file). Now I need to specify the physics engine ID because I'm loading this XML file into this physics engine. Then that should be good to go, so, I'm happy with that. Then next step is called step Simulation to step the simulation through. Before we do that, let's just run our tests again just to verify that that load is not failing. Yes, that looks good. The load is working so that's good. What I need to do now is step Simulation. Remember we were saying earlier, that the step Simulation, you would typically want to run 240 steps per second. That's how quickly it updates, so, let's just do that. We've got these iterations up here, which is going to give us 10 seconds. I can simply do for i in range, or, let's give it a more meaningful name, for step in range (iterations). I just p. step Simulation, and I pass it the physics engine ID, and that's it. That was easy. Let's just run the test again, see if it's fallen over. I don't know if you saw that, but it thought about it a bit more. Let's run that again, just to see that again. Possibly thought about it a tiny bit more because it was now running a simulation. If I was to pass it a much longer number of iterations, let's say we do 10. 000 iterations, let's see if it slows it down. Not much. It's pretty fast, which is pretty cool. Anyway, so that's good, so, let's get rid of that. Just run the creature. Right, so, the next step is the update_motors function. I'm going to jump over to my slide here. Every one-tenth of a second, we're going to update the motors. Why every one-tenth of a second, in the example script that we saw earlier, which loaded in the random creatures and random, I was basically running the thing in real-time, but then sleeping for 0. 1 seconds. It meant that basically, every tenth of a second, it was updating the motors. Now, remember when we update the motors, we're not moving them to a specific position, we're updating their velocity. They might be moving, but with our update, which is one-tenth of a second, we're saying, ""Move a different way. ""We're changing the direction or the speed of the movement. It's okay that we don't update them all the time because the simulation will update them because they're moving anyway. I'm going to cheat a little bit here because this code is a bit gnarly, but I will explain exactly what it's doing. This is my update_motors function. I go back into the simulation, and I'm going to say def update_motors. Now, what does it need?It needs self, it needs the ID of the creature, and it needs the creature itself. What it's going to do is, and I'm going to just put a comment in here for the first time, which is that cid is the ID in the physics engine and cr is a creature object. Okay, now we can whack this block of code in, and then fix it up. This first line here, let's go through it, is iterating over all the joints in the creature. It's worth noting that--Okay, let's just lay it out a bit neater. It's asking the physics engine how many joints this particular object has. Creature ID is actually cid. Then it's telling it what the physics ID is. It's, in this physics engine, this creature, how many joints does it have?Then, I get the motor from the creature. Remember the creature has this list of motor objects inside it, which we defined earlier. Then I'm going to update the speed of the motor according to the state of the motor basically, so, control motor, velocity control. That means it's changing the speed of the motor, not the actual position of the joint. Then the velocity is going to be the output of the motor, and finally, we set the physics engine ID to that. The question is how do I make it run every 24 steps basically, which is one-tenth of a second?I can do if step Module O 24 == 0, then self. update_motors, and I pass it the creature and the creature ID. Oh no, the other way around. It's creature ID and the creature. I can even make that really clear by just doing it like this. Okay, that should be more or less it. Let's just, again, run the simulation, which is going to test whether it's actually working or not. Hit Play. Okay, that all worked, nothing crashed, so, that's good. The final step is over here. The final step is updating the position of the creature. The way I'm going to do this is I'm going tohave an update_position function on creature, which, when you call it for the first time, it will set its initial position. Then, on subsequent calls, it will update the final position. You'll see how that works when I get into it. Back into the creature code here. Over to creature, and I need a function called update_position. It's going to receive self and a position. What it's going to do is I need to put some--When I create the creature back up to the construction of the creature over there, I'm going to do self. start position = None, self. last_position, let's call it, last_position = None. Then what I'm going to do is the first time these get called, so, if self. start position == None, self. start position = position, else: self. last position = position. That bit of logic there is basically, first time update_position gets called, it stores it as its initial position. Then subsequent calls, it just updates the other one, which is the last position. That'll allow me to have two positions so I can compute how far they moved. I just need to call update_position on the creature now. How do I know what the position is?Luckily, there's a nice function in here calledgetBasePositionAndOrientationin the pybullet API. I'm going to do that every step. I can say cr. update_position but I need to get the position. Whoops. The position and the orientation =p. getBasePositionAndOrientation. I need to say which thing, which object, so, its creature ID, and also which physics engine of course. I want to say, ""Give me the position of this creature in this physics engine. ""Then I just need to pass that position to the creature. How am I going to test this?I'm going to verify that the creature's position has changed during the simulation. I just do exactly this, and I'm going to verify that this creature's position has changed because the chances are, you saw how it instantiates, it pops, flies up into the air, then lands. There's a good chance that in 10 seconds that creature is going to change its position. I'll just write a test that does that. I'm going to do test Pos, and then my assertion isself. assertNotEqual(cr. start_position, cr. last_ position). There's my assertion, and I'll save all that. Make sure I've saved that one and that one. Let's run it and see what he does. Okay, that passed. Just out of interest, let's print out the position of the creature to see what it is. Here, we can do print(cr. start_position, cr. last_ position)just to see what they are out of interest. You can see there's the start position, it's -2. 55, -5, -0. It's there. That's actually slightly below ground, interestingly. Then you can see it ended up going in that direction, and it's really ended up miles below the ground, which is weird. Unless minus means up, I don't know. You can see it's clearly moved, and that's the important thing, it is the creature has changed its position. That means we've got a pretty good sense that something has happened in the physics engine where the creature has moved. I'm happy with that, and I'm going to end it there. What we've been working on is this new module, which allows us to do simulation. We wrote a run_creature function, which initializes the simulation and puts a creature into it, and then moves it around and updates its motors with the update_motors function. Then at the end, we implemented some code to start the creature at a specific position, and then work out where it moved to by the end. In this video, we've started out implementing the main code for our simulation."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Implement the fitness function,https://learn.london.ac.uk/mod/page/view.php?id=96317&forceview=1,"[music]In this video, we're going to implement the fitness function. In summary, we're going to start out by thinking about what the fitness function is going to be, in other words, what we're going to measure. What we're going to do is actually measure the distance traveled by the creature. Then we're going to look at how we can evaluate a whole population, and we're going to encounter some other consideration sand problems along the way. First of all, why measuring the distance traveled?In the original Sims' paper, that's exactly what he does. In later papers, he does measure how much time the creature shave been able to hold possession of a particular object, and they also following lights, and things like that. I'll certainly leave that for you for future work. Here, we're just going to do the classic thing of measuring how far the creature moves in the simulation time. We can compute the distance moved using this bit of numpy code. I'm going to add that into the creature class right now. I'm inside the creature file here, creature. py. I'm going to add a new function called ""get distance traveled"". Because it's going to use the state of the creature, it receives self. What I want to do is, first of all, I need to convert the two vectors which represent the start position and the end position of the creature, or the last measured position of the creature, I need to convert those into numpy arrays. I'm going to just say position one equalsnp. array of self. start_position, and position two equals np. array of self. last_position. Those are converted. Now, I just need to do distance equals np. linear_algebra. That stands for, lin alg. norm. I'll just do p1 - p2. Then I can return the distance. That's it. That should allow me to ask the creature how far it's traveled. Now, there's one other thing with the creature, which is if we call this function before it's had its position set, then we're going to have a problem. What we need to do is just we can say if self. start_position is none, or self. last_position is none, actually is not equals, return zero. We just say distance traveled is zero if you're not ready to go yet. That's great. That's done. I can just go back over to my slides now. The next step is to use this in our test script. Remember we had this test motor script which essentially generatesa random creature, sticks it on the screen, and moves its motors. Let's just see that script again, and then we can add this code to that. This is the script, which basically does it. Then if we run it, we should see a random creature springing into the air, and then attempting to move along by moving its motors. What we're going to do is we want to add some functionality to this scriptso that it brings out the distance that the creature has traveled as we go. What I want to do, I need to initialize the creature's position. I'm just going to say initialize it to the zero position. Should be creature. update_position. I'm just going to send the origin. I'll assume that it starts at the origin, and measure the distance it's traveled from there. What I can then do is I can do c. update_position. Now I'm going to update the position according to where it is. In order to get the position of the creature at the moment, I can do pos, orientation equalsp. getBasePositionAndOrientationfor this creature. That's an API function from the PyBullet library, getBasePositionAndOrientation, which will return a pos and an orientation. The orientation's what's the angle of the creature, and then the positionis where is it in xyz space. I can then update the creature's position using the position part that return like this. Finally, I can actually now print out the distance traveled. I can do print(c. get_distance_traveled). As the simulation's running, it should be printing out the distance it's traveled, or at least not necessarily the distance traveled, but the distance from the origin, from the original point at the current time. Let's run that. I'll pop that over there. Hopefully we'll see the numbers scrolling up over there. You can see I've got this slightly interesting-looking wormy type creature, and it's not really getting anywhere. It's sticking at around 1. 9, whatever the measurement is there, units. That's cool. That seems to be working. It's got a bit farther now. We can run it one more time just to see if we get something that moves a bit more dynamically. Notice that as it's flying into the air, because remember it has to be rejected from the plane that it sits on, and then it falls down, so you saw the distance went up, and then it went back down again, and then it started moving. That seems to be working. The next step is to actually do it in the simulation. That's the test script, how do we add it to the simulation?You may have spotted on the slide there that it says whoops, no floor. Let's see what that does for us. We'll encounter that problem as we go. What I want to do is I'm going to go into the simulation code here. The part of the simulation where I'm actually just moving the creature and moving the motors, I've actually got the update positions being called already for me. What I'm going to do just to test it, I can actually go and do this properly and write a script. I can say let's go test, let's do a quick test for test distance moved. I'll call it testDist(self). I'm going to basically copy this. I'm going to just run a random creature in simulation like this. At the end, I'm just going to measure how far it's moved. I'm going to do self. assert, sorry, going to get my auto-completion, assert Greater. I want the distance moved to be greater than zero, basically. I'm going to do cr. get_distance_traveled. The test is going to assert that the creature has moved, distance traveled is greater than zero. Let's run that one. That all looks good. It looks like it passed that test, good, confirming that the creatureis indeed traveling in a simulation. I want to dig a bit deeper and see what's going on here. If I just do a printout here to saycr. get_distance_traveled, we can see what's going on. Let's run that test script again. That's just in the simulation run creature function. I'm just going to print out how far it's traveled to see what we get. You saw there, see how far it's traveled. It's way higher than those simple creatures we saw in the other script. That makes me suspicious. When I see that 136 like, ""How did it get to travel 136?""Either it's an amazing creature, by chance, I've ended up with this really amazing creature that can just run niftylyacross the screen, or something going wrong. Chances are something is going wrong, because that's what normally is the case when you're writing code. Let's see if we can evaluate that a bit. What I can do is I can say print(pos[2]), so print(pos[2]). Pos[2] is the y position of the creature, because my suspicionis that something interesting is going on. You can see my y position, which is the first value, coming out there, seems to be negative. What do you think it means to have a negative y position?Yes, ""oops no floor"". [chuckles] That's what it said on the slide, remember. It means the creature is basically being dropped, and then it just falls through the floor because there is no floor, and it just keeps going down. That's why we're getting these fantastic distances. The solution is to put a floor in. Let's check our code and see what's going on. The relevant bit of the code is up here. At the top of the run_creature function, we are configuring it al land everything else, but we're not adding a floor, because when we do reset simulation, that will remove everything from the simulation, and there's no floor. We never added a floor anyway. We can go back over to our motor test script and steal the code from there, which creates the floor, which is these two lines. We'll look at those in a sec, what they're doing. Essentially, it creates a, what's that, collision shape. It creates a shape. Then it uses that shape to create something in the world using that as the collision shape, and as the visual shape as well. Yes, we just need to do that. The only thing we need to add is the physics client ID, because, remember, we're creating the floor in our own physics engine. If we don't put an ID in, it'll default to zero. I'll only ever create it in the first physics engine that we create. I've created a floor. If we now run the test again, it should hit the floor, and then we should get a much more reasonable distance coming out there. That's looking good. You can see that the-- I'm not sure which one's which, but I think the y one's probably the higher one, which is going down. It's gradually-- I guess it's going to fall down eventually. Have you gone down? Yes, it looks sensible now. It looks more reasonable. I'm happy with that, that it's now got a floor. If I just remove that printout, because that's a little bit verbose, I can run it again and just verify I'm getting a sensible distance there. Good. That's all working, brilliant. Now, the next step is, now I'll fix that, to see if we can go about evaluating a whole population. Let's see what we can do there. This is pretty easy actually, all we need to do is create a population, and just run each of the creatures in turn through the simulation. I can do that right here in my test simulation script. I'm going to test pop. I'm going to create a population. Pop equals population. -- I haven't imported population yet, let's do that. Import population. Let's try again. Pop. population. Pop size, I'm going to have 10 of them. Then gene count, say three. I'm going to have 10 individuals each with three genes. That's that. Then I need to just create a simulation like this. Then next, what I'm going to do is iterate over the creature sin the population, and evaluate each of them in turn. I can do for cr in pop. creatures. I can just run them in simulation, like that. That's good. I'm iterating-- Oh, sorry, that shouldn't be tapped in. I'm iterating over them. At the end, what I can do is I can just collate all of the distances traveled and print them out, how about that?We can say dists equal cr. get_distance_traveledfor cr in pop. creatures. It's going to iterate over all the creatures using a list comprehension, and ask each one how far it's traveled, and store that into dists. I can just do self. assertIsNotNone on the dists. For now that would be fine. It allows me to run the code without having to do too much evaluation there. Let's run that one and hopefully-- That worked. Sometimes we hit this error. I discovered this when I was developing the code. The error is-- I should get out of the way so you can see it. The error is that something's going wrong when I'm accessing the motor son the creature, because the list index out of bounds. Luckily you don't have to spend the time I spend debugging this, because I'm going to tell you what's wrong. The problem is there's one other little linein the test motor script which we haven't go tin our simulation initialization, and it's this line here. Remember the file caching that I talked about earlier. If you've got the same file name and you try and load it twice into the same simulation, it will cache that file, it won't update it. What we're doing, remember, is we are rendering out 10 files to disk, one in turn, all with the same file name, and then attempted to load them in turn. The problem is it's just caching them. I just need to add this line to our simulation code as well to make sure it's being configured correctly. We'll do that here after resetting. Remember, I need to pass in the physics client ID as well. Then that should be hunky-dory, as they say. Let's run the test. Another interesting thing is it only appeared the second time I ran it, because it's random, it's not going to happen every time. Let's run it. That's cool, but I want to just make sure it's not going to happen, so I'm going to just make a big old 100 population that I'm bound to see the error in a 100 of them. It's evaluating 100 creatures now for 10 seconds each. It's definitely thinking about that one a bit more. It's not crashing, and that's the important thing. I think I'm more or less there. Great, nothing went wrong. I could even print out the dists if I trim that back down to, say, five. I can just print out the dists just to see what kind of spread of distances I've got. Let's just do that. There we go. These are the distances traveled. You can see that one. I'm not sure which one it is because there's two printouts. Probably this one with a variety of different distances there, and so you can see. Yes, it's definitely that one. It's got the two, three, four. We've got a nice variety of different distances already, which is what we want for our evolution process. That's great. That's pretty much it. That is our fitness function. Whoops file caching, yes. There's one other thing I wanted to deal with here, right at the end, which is the flying into the air problem. If I run that test motor script again, you can see--Let's run it from here actually, I'll get out the way. If you run this again, one thing I noticed, you see how it flew up in the air?Yes, that's a bit of a problem, isn't it? Why is it a problem?Because depending on the shape of the creature, when it flies up in the air, depending on the shape of the creature, it'll fly up more or less, it'll get further or closer to the origin. If we're measuring the distance from the starting point, then it's a bit of a problem because it might gain an advantage by having a weird shape, and getting some big boo stat the beginning when it's actual walking might be rubbish, but it ends up really far away because of its shape. We don't want it to be allowed to do that. What we're going to do instead is we're going to just drop it. Instead of letting it fly into the air, we're going to drop it just above the surface, and it'll fall downand then start walking. How are we going to do that?What we do is in the simulation-- Let's try it in the test script first. Up here we can just doto p. resetBasePositionAndOrientation. That allows me to specify a hard coder position and an orientation for the robot. The orientation's going to be just that, which is straight up, basically simple. Then the position is going to be zero. X and Z are the first two, then Y is the second one. Let's put it up at minus five to see where that puts it. If we run that one, you'll see that it crashes, whoops, because I forgot to pass it the idea of the creature. Let's try that again. There we go. That didn't quite work. Let's try that again. Reset base position and orientation. Zero, zero-- Oh, because I've put it minus five. Whoops, I put it below the floor, and then it got there. That's bad. Let's put it above the floor, let's try that. Yes, there we go. Now it's sinking down nicely and landing on the floor. Then it can start moving from there. That's quite a good one, isn't it?We just need to call this line in the simulation. Maybe five is a bit too high, let's put it at 2. 5, one last test. There we go. That's good. I'm going to just put it at 2. 5. I'm going to put that into my simulation. I would do it at the beginning. That's when I've created the creature in the simulation. I just do this p. step-- Then I just need to tell it which physics engine it is. That's it. Then hopefully, if I just run the tests again, it won't crash because that's all good. Fine. I'm happy now. Other things that we might consider. I'm not going to go too much into this, but you could punish the creature for being too big. There's all kinds of other things we could put into the fitness function. We'll think more about those later. For now we're just going to say, it doesn't matter how big it is, how many motors it has, we're just going to let it try and get as far as it possibly can. That's fine for now. In summary, we've just been building the fitness function. We've been thinking about what we're going to measure, which is the distance that the creatures traveled. Then we've worked through various stages to get that to work, and found various issues, and eventually implemented that so that we can actually now calculate the fitnesses or the distance traveled by a whole population of creatures, which is setting us up ready to do the evolution. We also dealt with that flying into the air problem. We now drop the creature onto the ground, and it goes from there. In this video, we've just been implementing the fitness function."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Multi-process evaluation,https://learn.london.ac.uk/mod/page/view.php?id=96319&forceview=1,"[music]In this video, we're going to make our simulation run much faster. In summary, we're going to start out by just saying what the motivationis for making it run faster in case that's not obvious, and then we're going to go through the steps of preparing the simulation. Now, I'll just say in this video that I am not handwriting every line of code because it's not strictly on topic in a sense. It is quite complicated, and I don't want to spend too long writing every single line of code, but I'm going to explain every line of code. How about that?First of all, the motivation is that it's too slow. What does it mean to say it's too slow?When you're running a GA, you're going to have to evaluate thousands, and thousands, and thousands of individuals. Let me just illustrate what kind of problem we have in here by running an evaluation. Here's my test code. Let's put the population size up to 100, right. Oops. This test code which we saw before is going to basically evaluate100 individuals, so I'm going to do that now. You can see that is chugging away. This is all running on the CPU, so it's not GPU-accelerated like a neural network, unfortunately, although there is talk of PyBullet being able to use CUDAat some point but not yet, I don't think. Anyway, so that's how long it took to evaluate 100 individuals in series. What it's doing there, just to illustrate, is you've got a population of individuals. These are all the creatures in the population, so we give those a C for creature. Then we've got a single simulation like this, and what we're doing is we're basically, in turn, going in that one, putting that in the simulation, and that one, that one. Okay. You get the picture, right. We do each one individually. What we're going to try and do in this video is we're going to basically say, ""Actually, I'm going to have multiple simulations and they're going to run in parallel because my machine has multiple cores, so it can run computations in parallel. ""The way it's going to do that is we're basically going to say, ""Right. Okay. Put that one in there, that one in there, that one in there, "" then run it. Then we're going to load them up again. We're going to say, ""Right. Put that one in there, put that one in there, ""run it, and we go through it. Imagine, if you've got 100 of them, you're just going to do like four at a time. You going to have four, four, or however many cores you want to use. I think I might have six core son this machine, so yes, that means I can do that. How is it going to work?First of all, we've got to prepare the simulation class. The first problem, if we're going to be running the simulations in parallel is that at the moment, each simulation has the same file name. They all basically write to this file called temp. urdf, don't they?Imagine if they each write to temp. urdf in parallel, so they're going to overwrite each other's files. That's basically a disaster because they'll all have different--It's going to break the firewall possibly. It's just a disaster. How are we going to solve that?What we're going to do is we're going to go back to our simulation class, which is over here, and we're just going to set it up so it requires a unique ID. We'll call it sim_id, and we'll set it to zero initially. That means that all the tests that don't pass it, that number will still pass. The way I'm going to use that number is I need to store it, so I go self. sim_id = sim_id. It's stored. Then what I can do is in run_creature, this line here where I specify the file name, I'm just going to use the sim_id as part of that file name. I'll just do + str[sim_id] + . urdf. That should do it. Oh, so it's self. sim_id, isn't it?Self. sim_id. That should mean that it now creates a unique file name for its ID each time, so it can stand. Let's just check that. I'm going to reduce the number of things just down to here. This one should write all the files to something called temp. You can see there, that's the temp. It's now writing temp0. urdf instead of temp. That's good. Each sim has its own unique file name, which is great, so it means I can run themin parallel without them clashing with each other's files. Great. They've done that. The next step is to create a Threaded Sim class, so let's just do that. It's basically going to havea constructor which creates multiple sims, and it's then going to havea static function for running creature in simulation. I'll explain the reason we need a static function, not a stateful function for running the creatures in simulation in the pool mode when we're doing it with multiprocessor stuffis that that's how the pool object that we're going to use works, basically, so we have to do it like that. How are we going to do this?Let's just go in and createa Threaded Sim class in the simulation code. We're going to create something called class ThreadedSim[]:and it's going to have a constructor which takes self and the pool_size, which is going to be the number of things in the pool. That's how many CPUs it's going to use. You could call that CPU count or something if you want, but whatever. Anyway, so that's that. What are we going to do?We can do self. sims = [Simulation(i) for i in range(pool_size)]. That's it. That just essentially fills upa list with a set of simulation objects each of which has a unique ID up to the number of pool_size. The next thing is I need to writea static function which allows me to evaluate a creature, so I'm going to do that here. Remember, the static functions are functions which we don't need to create an instance of the object in order to run them because they have no state, they don't rely on the state of the object. They don't access self basically. This one's going to be called run_creature--or let's call it static_run_creature, static, so it's really clear what it is. Oh, run_creature. We need to pass in everything that it needs to run a creature in simulation. What does it need?It needs a simulation, it needs a creature, and it needs to know how many iterations to run for, right. That's it. All it needs to do is say sim. run_creature, and it's going to pass it the creature, and it's going to say iterations like that. Actually, what it has to do as well is it has to return the creature because in threaded mode, in multiprocessor mode, it's going to create a copy of the creature so that it passesby value rather than by reference. We need to make sure we return that so we can get it back. It doesn't seem to like that for some reason. Oh, because I've got a full stop there. There we go. Right. Okay. Good. That's that. That's now ready. It's got a static function. This is where it gets gnarly. What we're going to do next is we've got to figure out a way of creating the arguments that you pass into a pool. When we create a pool of processes, it needs all the data that's required to execute multiple threads. In this case, let's say we had four simulations, what we're going to need to do is pass it those four simulations, and then the four creatures that it's going to runin those simulations, and also the number of iterations. What each thread or each process is going to do is it's going to run static_run_creature, so it's going to need a simulation, a creature, and an iterations. What we'll do is somehow take that, which is the input to all of this and which is basically a population of creatures and a set of simulations, we've got to reorganize it into-- We've got iterations as well. Let's just pop that in as well, iterations. We're going to have to organize it like this. We're going to have to have a simulation, and a creature, and a number of iterations. For each process, say we had four of them, we'd need four of these, so we'd have to do SCI, SCI, SCI. That's what I need to prepare. That's my pool arguments. I have to prepare that as a list of lists. It's worse than that. Actually, you can tell I've got six. I haven't done all of the creatures there, have I?I've still got two hanging over. I'm going to have to actually create two pools, and I'm going to run those in series. The first pool will run on four CPUs and do everything, and the next one will do the other two. Imagine if I had 100 creatures I needed to analyze, to simulate, then I'll have to do it in sets of four or six or whatever. This data here that we can see, that gives me one pool's worth, but I need that for every pool, right. In this case, I would arrange it so I don't need an extra two. That would be a big list like that. A list of lists. Then, I'd need to create another one, which is-- got S, C, and I; S, C, and I for the final two, in the case where I've got a population of six, right. That's what I need to create that data structure. It's a little bit prone to error writing this kind of code. I don't want to go through a full unit testing process, so I'm going to cheat and use the classic British TV show, which is called Blue Peter, where they used to say, ""Here's one I prepared earlier, ""when they were making something on the show, and then they'll pop out this really complicated thing. That's exactly what I'm going to do. I'm going to start by saying I'm going to havea function called eval_population, which takes as an argument, a population, and a number of iterations. Then, the first step in thatis to create this rather complicated data structure. That's what that code there does. Let's just chuck that in, and just verify that it's working okay. Oops. Put that back there. It needs to be stateful. It needs access to the set of simulations we created. Right. Just to talk you through it, this is the big mega array that's going to store all of the sets of arguments, sets of sets. Then, this allows me to do the double iteration. The inner loop is creating basically each of these, in turn. That's your inner loop. Then, the outer loop is creating the whole lot. The outer loop is creating the whole thing. That's all that block of code is doing, it's just basically creating allof those objects in a nice list of lists of lists. At the end of it, pool_args contains all of that stuff. Great. Now, I've got my arguments ready. The next phase is to actually generate these process pools and run them. I'm going to go to that. We iterate over the sets of pool_args and create one pool for each set. Each of these sets. Remember, one set that I'm talking about here is like that. I'm going to create a pool that's going to run all of those creatures, and then another pool that's going to run all of those creatures. Again, I'm just going to grab the code here because it would take too long to write itand test it properly, and I've already done all that. Here we go. It's not that bad. We can grab that. Now, let's go through it. First of all, what we do is we create an empty list, which we're going to store the results in because remember, I was saying that it will basically create a copy of the creatures and we need to get that copy back and store it. First of all, I need to get this pool imported. That is from here, multiprocessing import Pool. Okay. Right. Back down again. Yes, where were we?Sorry, something's gone wrong there. Let me just get that back. [silence] Okay, that's it. We create an empty list of the creatures called new creatures. Then, we're going to iterate over the sets of args. For each pool's argument set, we create a new pool. A pool is this thing from the Python API that allows us to easily run a set of processes in parallel, and the arguments would dictate how many processes there are. We call this special function on the pool that we just created called star map, which sounds cool, right. It goes over the arguments and calls this function on each set of arguments, so static_run_creatureis run on each set of arguments. Now, we know that static_run_creatureis our function that we just wrote, which is up here, and it takes a sim, a creature, and a number of iterations. That's exactly what we're going to need to pass it in here. That's why we built that data structure. We don't need to print that. Then, it just extends the list of creatures with the return from that. It'll return the number that you pass it. The first time, it'll return four in this example where you've got six and four. It'll return four the first time, and two the second time. All of those creatures that come back--It's going to return creatures, and we're sticking all of those back onto the new creature's array. At the end, actually, just for interest, we're printing out the distance that they traveled. Finally, we assign the resulting creatures onto the population. We replace its previous set of creatures with the evaluated set of creatures. Final step is with that code to now writea quick test code, and we can then compare the speed to see if it was all worth it, so def test Proc. I'm just going to do all the same stuff here. I do like this. I don't need a sim. I'm going to dot sim = simulation. threaded sim(pool_size = 4). Then, all I do is I do tsim. eval_population(pop, 2, 400). That should do it. You can see. I can then do the same stuff I did here and the same test. Let's compare it. Let's put 20 in there and 20 in there. We'll run the first one, see how that one goes. This is the non-threaded one. Let's just see that again. That was the non-threaded one. Then, now let's run the threaded oneon 20 to see what the difference is. [silence]Hopefully, you can see that that was significantly faster. If I increase the number of threads to the pool_size to say, eight, I think, I'm not sure how many CPUs I've got, you see it's that much faster again. That's pretty quick. I've got another machine that we'll use later, which definitely has eight cores on it, and it's got a more powerful CPU, you can see how fast it runs them. You can see, suddenly, it becomes viable to run thousands of these overnight. You can run it for a long time and see what comes out. You'll get a really good search of the space when you have a big population. That suddenly becomes realistic because it's six times or eight times as fast. That's probably enough of that. Let's summarize. What we've been doing is just quickly making the simulation multi-processor capable so that we can evaluate multiple creature sin parallel and have a much faster genetic algorithm basically. The steps we went through, we prepared our simulation class, so it would be able to have a unique file name. That was easy. Then, we create this Threaded Sim class, which had the appropriate stuff it needed like a static evaluation function, and then all that fancy code to set up the arguments to pass into the pool using the built-in processing features, multi-process features of Python. In this video, we've been accelerating our genetic algorithm with a multi-fold increase in speed."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Planning selection and breeding,https://learn.london.ac.uk/mod/page/view.php?id=96324&forceview=1,"In this video, I'm going to give you a quick overview of the steps we're going to be taking in the next few videos to complete our genetic algorithm. In summary, we're just going to revisit the GA and see where we're at on that diagram. Then we're going to talk about how we're going to select the parents, how we're going to apply crossover to create different children from the parents, and then how we're going to mutate and finally, iterate on those to actually implement full evolution. First off, overview of the GA. Actually, we're in this bit now where, I'm going to disappear so you can see it. We've done the selection in a way, we've almost done the selection. We've basically done the test and we've calculated the fitness of each individual by seeing how far they move. We've done all of that, we got the simulation, we even made it multi process capable, so it's efficient. Finally, what we're going to do nowis we're going to select those parents, then generate the next generation from those, from the parents that we select. We're going to talk a bit about how that process works in this video really. The first part of selectionis we're going to use the roulette wheel style of selection. There are other types of selection which you could implement if you want this other further work, but we are going to do roulette wheels. Just to remind you how that works, I'm going to quickly draw a diagram. What we do is we essentially create a wheel, if you like, which has segments in it. This example, let's say, we had a population of four, that would be creature one, creature two, creature three, and creature four. Then the angle, this angle here, is decided based on the fitness of the individual. The fitter they are, the more space they take up on the wheel. You can see that three is not very fit and one is very fit. The idea in order to select two parent sis we basically take this and we spin it around. Spin it around, we basically pick a random point on there. We pick a random point on the circle and then that's the parent one and then we do it again, we pick another random point on the circle, and that's parent two. In this case, we've selected two and three to be the parents. That's basic roulette wheel selection. Obviously, the fitter they are, the more likely they are to be selected. We do this multiple times. If we say we're on a population size of 10, we'd run the wheel 20 times because we need two parents for each new one in the next generation. That's roulette wheel selection very quickly. We'll go into more detail later. Then we're going to do crossover between the parents. Once we've selected our two parents, we basically get the two genomes. Let's say that's genome one, so that's parent one, and parent two. I've talked about this in a previous video so we [?] very quickly. The idea of crossover is we say, ""Well, we're going to take that bit of that genome, we're going to take that bit of that genome, and that's the crossover point there. ""The child will basically have that part from parent one and then that part from parent two, so will end up with two chunks of a genome. It has good chance of recombining the different bits of the two parents to come up with a new variation. That's crossover. We can do that. That example there is single point crossover, but we could do multi point crossover, but we're just going to do some basic stuff like a single point crossover. That's that, and then the next one is the mutation. There's various types of mutation we could do. First thing is, say we got this child genome like this. We can then jump in there, and remember the genome is made out of lots of floating point value. We can just jump in, pick a point on there go, ""Okay, I'm going to mutate that one, that one and that one. ""We could do something like we're going to mutate, say 10% of them. Then what we can do is we can add something to that value. Add some random number to that value, which could be negative or positive. That means we're going to change that value and then we can add more onto that and add more onto that. That's the idea is that we mutatea certain number of points on the genomeso that they're slightly different and that gives us more variation. Then, the other thing we can do is the growth and shrink mutations, which I think are quite successful and we can see how they work. The idea is that we know that a genome is made out of genes where each gene encodes a body parton the robot basically or the creature, and what we can dois with a shrink mutation we can say, ""Well, we going to knock out that geneso we just end up with two genes in this example. ""We can delete one entirely or we can do other things. We can add a random one, so we could put it up to four. We could add a random gene or we could make a duplicateso we could actually say, ""Well, I'm going to add that and stick it over there. ""That would give us repeated version of that part of the body. That's an interesting type of mutation as well. There's various things we can do with growth and shrink and point mutation. The final step is to do the whole thing, to basically get this population of individuals, all the different creatures, do the selection, select to give us the parents for the next generation, select parents, and then we do the crossover and we do the mutationon all of them and then we get another set of individuals, and then that's our next generation and we keep going. We basically then wrap it around and we go back up to the top again. That process, that repeated, iterated process and that's it. In summary, we've just been very quickly running through the things that we are going to do in the next few videos just to plant them in your brain again. We've seen most of these ideas before, but just so that we are ready when we meet them in the video. We're going to code them up and complete the genetic algorithm. In this video, we just had a quick overview of what we're going to be doing in the next set of videos."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Roulette wheel selection,https://learn.london.ac.uk/mod/page/view.php?id=96326&forceview=1,"-In this video, we're going to start out implementing the selection process by building the roulette wheel. Basically, we're going to do two things. We're going to generate a fitness map from a set of evaluated individuals, and then we're going to use that fitness map to select parents. Okay, how does the fitness map work?Imagine we have three creatures. You can see on my slide here I've got creatures 1, 2, and 3, and each of them has a fitness. 2. 5, 1. 2, 3. 4. That's how far they moved in the simulation. Then my fitness map that I want from this looks like that list of numbers at the bottom. What I do is I essentially iterate through the creatures' fitness and I create a summed value as I go through. You can see 2. 5. I add 1. 2, that gives me 3. 7. Now I add 3. 4, that gives me 7. 1. That's what I want. If my input is the list of creatures and their fitnesses, the output is that which is what I'm calling my fitness map, which I'm going to use for my roulette wheel selection and I'll tell you how it works when we get to the next slide. Let's start out by implementing it. Def test, I'm in my test population function here, Fitmap. What we're going to do is we're going to basically set up--assume that the fitnesses are the ones that I have in that slide. 2. 5, 1. 2, and 3. 4. Those are the fitnesses. I want to create a function, which takes the fitnessesas an input and returns the right fitness map. Fitmap, and what I want, okay, so I write I want this, I want 3. 7 and I want 7. 1. What I can then do is create the fitness map by calling a function on population and check if it's correct, right?It's population. Population. get_fitness_mapfrom these fits. It's going to be a static function, which means that it's stateless so it's easier to test generally. That's it. Let's go over to population and let's run it just to prove it doesn't work. Okay, it doesn't work. Population has no attribute, doesn't have that attribute get_fitness, because I haven't written it yet. Fine. Okay, population. Let's jump over there. I've got my population class up here and I'm going to close up my terminal. I'm going to have to define a static method called get_fitness_map, and it receives a set of fits. Fitmap equals something and then I'm going to say--I'm going to do it like this, total equals zero, and I'm going to do for f in fits, total equals total plus f. Fitmap. append total. I'm just going to return fitmap. Really, I just iterate over accumulating basically. You can see I'm just adding it to the total and appending it onto this fitmap array which eventually I return. Let's run that test again and I'll get out the way. You can see the test is passing now. Let's give it something maybe a little bit--We could do more testing, we could write a bunch more tests here. Actually, I'm not even asserting anything. I'm going to do assert Equal that fitmapis equal to what I wanted. It's actually a test now. Okay, good it's actually passing the test now. That's good. I'm getting back what I wanted and I could obviously give it more tests, but I'm not going to do that. Once I've got my fitness map, the question is, how do I then select a parent from that?Here's where I'm going to draw you something. Imagine I have my fitness map, it's going to look like this. I'm going to have one, two, three. I'm going to say that's 1. 2. Actually, look, let's not have these. Let's just get rid of the circles because I don't think they're very helpful. Oops. I've got my fitnesses there, I've got 1. 2, sorry, my fitness map rather. I've got 2. 5, 3. 7, and 7. 1. 2. 5, 3. 7, 7. 1. What I'm going to do is I'm going to use NumPy to generate a random number, which say is 1. 5 or in the range zero to 7. 1. Then what I do is I say, well, I look at each of these things in turn. That's index zero, that's index one, and that's index two there. What I'm going to do is I say, I get this value, 1. 6, and I say, ""Is it less than or equal to that?""If it is, then return the index. In this case with 1. 6, I'd get an index of zero. If I generated instead the value, say, I don't know, 6. 7. Obviously, that is going to skip that one and is going to skip that one, it's going to skip that one, it's going to get to this one and say, ""Is it less than or equal to that?""It is. That means that it's going to return two. Let's implement that. I go back to the code, and it's going to be called select parents. I do def testSelPar. Again, I can do this, copy all of this. I don't need one. I've got my fit map and now I want to say P1 idor pid equals population. Population. select_parentfrom fitmap. Then what I can assert is that maybe it's less, the parent should be 0, 1 or 2. I can just do self. assert that parent ID is less than 3and that will be a good test, or a basic test. Again, I'm failing that test because population has no attribute select parent. Let's go ahead and implement that in the population class. It can be a static function, static method. I do def select_parent, and it receives a fitmap. Here's how we go. We're going to say for--First of all, I have to generate a random number. I'm going to do import numpy as np. Okay, I've got my NumPy library no wand then I'm going to generate a random number. I say r=np. random. randand that'll be in the range 0 to 1, and I do r equals r times fitmap, last item in fitmap scale. That's it. It's basically scaling it by the last item. In the case of this one, it would be a number in the range 0 to 7. 1because 7. 1 is the last number and we assume it's a sorted list. Okay. Next step is I need to iterate over the fitness map and select my index. For i in range length of fitmap. I say if fitmap, no, actually, if r is less than or equal to fit map i, then return i. Okay, and that's it. Let's do a test. All right. Sorry I'll just get out of the wayso you can see that one. I create a random number, I scale it into the right range, then I iterate over the elements in the fitness map and I check if the random number is lower than that element, and I return the index of that element. That's exactly what I showed you previously on the thing. Let's run that test. You can see it is now passing that test. I might deliberately set up some slightly trickier tests just to make sure it's doing what I expect. Imagine I had say, a fitness of 0, a fitness of 1000, a fitness of 0. 1. It should almost guarantee that the value is one that comes back, because it should always be between 0 and 1000. It's very unlikely to be in that range. It should always give me that one. Let's just run that and we call that Par2. Let's just run another test. One is not less than one, oops, I've got the wrong assertion, should be assert equal. Okay. If I could spell assert, that'd be okay. There we go. That's done. That's passing that test now. I can run it a few times. If I ran it 10. 000 times, probably, I'd find that it was going wrong, because at some point it's going to get a value in that range, which is very unlikely. That's a good test. Okay, that seems to be working. That is more or less it. What we've been doing is just been quickly implementing the core functionality that's going to allow us to select parents using a roulette wheel technique. You see it's a pretty simple algorithm really. We just accumulate the fitnesses into a list, and then we generate a random number and choose one of the indexes in that list. That tells us who the parent is going to beand I just demonstrated at the end there that if you've got a very fit parent, who's got 1000, then you can see that it's selecting that one every time, whereas in the other one it's more random. In this video, we have just been implementing the basic roulette wheel."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Crossover and mutation,https://learn.london.ac.uk/mod/page/view.php?id=96328&forceview=1,"In this video, we're going to be implementing the various genome operations which allow us to generate variation in the population. We're going to be doing crossover, point mutation, and grow and shrink mutation. Here's crossover. The idea is that basically we take a bit from each parent and we use that and then combine that together to get the child. We've seen that before, so let's just see if we can simplify that a little bit. What I'm actually going to do here is I'm going to take the two parents and I'm only going to crossover where the gene boundaries are. Remember that we break the genomes into genes where each gene defines a link so the idea with that is I'm going to take say, the first link from the one parent and then the other two links from the second parent so it combines the morphology a bit more directly rather than crossing over in the middle of the genes. We'll do that just to simplify things a little bit. Let me just slide my pen away. What we're going to do is write a test, of course, so we'll go into our test genome code and we're going to do testXO. Basically, what I'm going to do is I'm going to do it with raw numbers because rather than creating random genomes and all that, it's a little bit easier to test what's going on if I just hard code what's in the arrays. I'm going to, first, create two NumPy arrays like that and one's going to be 1, 2, 3, and that one's going to be 4, 5, 6. Actually, maybe let's put three genes in there just to make it really clear, 7, 8, 9 and then we'll do 10, 11, 12, and 13, 14, 15 in that one. Then 10, 11, 12, 13, 14, 15, 16, 17, 18. There we go. We just got two NumPy arrays each of which has three genes which you can see there and I'm going to get those and then see if I can crossover. G3 is going to be genome. genome. crossover g1 with g2. That's the idea in a single point crossover so we're just going to go ahead and do that. It's going to be a static method because it's going to be easier to test that way and it's basically stateless anyway so we don't need it to have any state. We're going to pass g1 and g2 in. The first thing we need to do is choose a crossover point. Crossover is going to np. random. random integer which is up to the length of g1. We're going to, first of all, pick a random point across the genes in the genome 1. Then I'm going to say g3 equals np concatenate and I need to pass that a tuple. I'm going to pass it g1 and g2. Normally, that would just stick g1 and g2 together and make something out of the two but what I want to do is take only a certain bit of g1. I'm going to take from zero up to crossover and then on g1, and I'm going to take from crossover to the end in g2. That's simply it and I can just return g3. Now, the only thing is that it might be the case that g1 is longer than g2 so I might have ended up with an index which is beyond the bounds of g2. What I can do is I can just double-check that. I can say if XO is greater than length of g2, then XO can just be set to length of (g2)-1 like that so I get a bit of g2 and that's it. Then what I do next here, yes, just do the concatenation. I return it, so let's run the test, and maybe I can do an assertion to say, self. assert equals-- and I might check that the length of maybe g3 is equal to the length of g1 just to see if it's working properly. It's a very simple test. We could, as usual, write more tests if that's what you wanted to do. Let's run it. That test is passing, that's great. I'm happy with that. I could even just print out the results just to see what's happened. Print g1, g2, g3. Let's just print them all out and see what we got. So what have we got?There's g1 so that's the first three, and then the second three in there. You can see I've then ended up with some of g1, which is the 1, 2, 3, 4, 5, 6, and then the end of g2, so that worked nicely. Let's just run that a few more times so we get another variation. There we go. That one I've ended up with the whole of g2 so obviously crossed over at the end of g1. That's interesting, isn't it?You can see that's working pretty nicely. If I wanted to be really sure, I can always just do it 100 times and check if it's going to crash which is something I would do just to stress-test the code to see if I've done anything stupid, basically. There we go. Ran it 100 times nothing went wrong. Great, and it's fast. That's crossover done. Next up, we have point mutation. Simply put, point mutation is when we add or subtract a value from a locus. What we're going to do here is go back to the code again and write a test. I can do def test_point and I'll call it then self, and again, I can do g1. I can do g2 equals genome point_mutate. The great thing about unit test so you can say this is the function I really wish I had and then you have to write it, you've set yourself the task. So we're going to point_mutate g1 with a rate of 0. 5 and an amount of 0. 25. I'm sending it two extra parameters there so the rate is per gene, how likely it is to mutate that gene, and then the amount is, in that gene, how much it will get mutated by. You could put the rate to 100 and it would mutate at least one bit of each gene, for example, and that's how I'm going to do it. Let's implement it. Static method and def point_mutate and it takes g1 and a rate and an amount. What we do is we are going to build a new genome. What we'll do is we'll do g-- Actually, let's mutate it in place for i in range length of g1. Actually, it's easier to go for gene in g1. How about that?For gene in g1, let's call it something better, genes. There we go. I prefer that. Yes, for gene in genes so if np. random. rand is smaller than rate, so if the rate is high, then it's more likely that it will happen. If it's smaller than the rate, we're going to mutate this gene. How do we mutate a gene?We're going to do locus or index is going to be np. random. random integer in the range length of gene, so it's going to mutate one of the numbers in this gene. One of the parameters in this gene is going to get mutated. The mutation value is going to be np. random. rand. Our gene values in the real thing are between in the range zero to one so I'm going to mutate it by that. First of all, I minus 0. 5 off of that so that will give me a value, a random number in the range minus 0. 5 up to 0. 5, and then I'm going to scale it by the amount. I just do gene index equals gene index plus R and that's it. Now, notice that that is actually mutating in place so that's fine. We might deal with that a little bit more later when we get into the full algorithm but for now, that's fine and then I can just print. Print g1 and then let's do print g1 at the end because let's just see if it's changed. It doesn't look like it's changed which is a little bit annoying and I'll tell you why because these are not floating points so I'm adding a floating-point value to integers so it's just rounding it down. Let's try that again. I'll just make these floats. There we go and run it again. There we go. You can see that the original one is there so that's it one, two, three-- oh, the other test is making it a bit confusing. Let me just get rid of the other tests because we don't need those anymore. Let's run it and then, I'll get out of the way, we can see it. Yes, there we go. So there's our original version with all the numbers in and then there's our mutated version. You can see one of them got mutated so each time I run it you should see that one. In that case, two of the genes were mutated. In that case, again, one gene. It's kind of random but the higher the values are, if we set the rate to one, for example, it means every gene will get mutated. Since every gene is mutated now every time. That's just how that works. That's good. That's the point mutation. The final operation I'm going to do is the grow mutation where essentially we generate a random gene and stick it on the end of the genome. This is going to use just some basic NumPy stuff to do that. So test, grow, let's go in here, test, grow. I'm just going to copy this code here. I'm going to call grow_mutate. Now this time, I'm going to say, yes, assert. The length of g2 should be greater than length of g1 because it's going to grow. Basically, I'm going to grow_mutate. There I've got my g1, I'm going to grow_mutate it with a rate of 1, so it's guaranteed to grow. I'll print it out. Then I'll just assert the length of g2. The new grown genome is greater than length of g1. Let's go ahead and implement this code now. I'm going to do def-- No, this is static again. Static method and def, grow_mutate, and I take my genes and a rate. How's it going to work?Basically, if np. random. rand is smaller than rate, then I'm going to do my mutation. I'm going to say new gene equals genome, get random gene, and the length of the gene is going to be the length of gene is naught so that it's the same in case as we change the number of parameters later, it just reads whatever the incoming genes is. That's my random new gene. Now I just need to append it on to the end of the genes. Now, this is a bit tricksy because in NumPy, it's a bit weird. Let's say I've got a genome like this, I've got an array like that, or let's call it g1. If I've got g1 like that, and I do np. append, this is what we might think we could do onto g1, I know I add, I don't know, a load of naughts. Let's say I wanted to do that, which you think if you would just stick it on the end, but what it does, unfortunately, is it flattens it, so you don't want to do that. What you got to do is make into the same shapeas the original array. You put some extra brackets around it, and you put the axis to zero, and then it does what you want. You can see that's doing what I'd expect over there. It's actually dumping it on the end. That's what we need to do. Basically, over here, we're going to do np. append genes, and I'm going to go put gene in an array just like I di din the example there and I set the axis to zero. Then that should do the trick. Finally, I just need to assign that to genes and return genes. Okay, there we go. That's the grow mutation. If you hit play, hopefully, we'll see-- yes, there we go. You can see there's my original genome. Then below that, you can see the original genome plus a random genome has been generated. That should run every time. There we go. There we go. That is the grow mutation. That's the end. In this video, we've been implementing all of the various mutation types that we want to apply and different operations on the genome. We've done crossover, point mutation, and grow and shrink mutation. In this video, we've just completed all of the genomic operations that we need to basically implement the variation in the population."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Evolution!,https://learn.london.ac.uk/mod/page/view.php?id=96330&forceview=1,"[music]In this video, we're going to be pulling all of the code together that we've been working on in the last many videos, which constitutes the genetic algorithm. In summary, we're going to implement the genetic algorithm, and then we're going to run it and we're going to make some tweaks as we go. Here is the genetic algorithm in pseudo-code form. We're going to generate a random population, we're going to evaluate it, we're going to get the fitness and the fitness maps, we're going to make a new generation by iterating over each creature that we want in the new generation and selecting parents, making new DNA through crossover, mutating the DNA, and then making a new creature with that mutated crossed over DNA. Then we go back, we iterate back to the evaluate step. We're going to do it all as a simple test. Let's do it here. I could write this in a nice genetic algorithm class or something like that, but for now, I just want to see the raw code sitting there and quick to run so we can work through it and test it as we go. Let's save this new file as Test GA. The first thing I'm going to do is I want to import unit test, and then I'm going to write my class Test GA, which is going to extend on unit test. Test Case. Then I'm just going to define a test GA function and I'm going to write the unit test. main. What's first?First off generate a random population. That's easy to do because you've got our population class. Let's import that as poplib. I'm importing it and calling it poplib. Pop = poplib. Population. It takes two arguments. It takes the population size and the gene count. The population size is 10 and the gene count could be anything, but I'm going to do 10 for now. Gene count is three. That's my random population. Next up I need to be able to evaluate the population. To evaluate a population, I can use my simulation class. Let's pull that in. Import simulation as sim lib. Now sim lib, so let's create a simulation = sim lib. -- and I'm going to create a threaded simulation. Threaded simulation takes one argument which is the pool size, which is the number of threads. Pool size, and let's set that to eight so that we can use as many of the threads as we can on this machine. Great. What's next? How do I evaluate population?I do sim. evaluate and I pass it. The population and the number of iterations. Now, the number of iterations maybe is a little bit of an obscure. What does that mean? Iterations, so let's go in and just put a comment in to remind ourselves what that is. Pop is a population object and iterations is frames in pybullet to run for at 240 frames per second. That's just a comment. You'll see now if I mouse over it, you can see my comment appears instead of that generic stuff. That's clearer. Let's run it for 10 seconds. Good. What's next?Get the fitness and the fitness map data structures. Fitness = and now evaluated the population. I can just do, let's just import the creature first because we might need that in a minute creature as crlib, cr. get_distance_traveled for cr in pop. creatures. With that code there, I'm iterating over all the creatures in the population asking each of them how far it's traveled, and then I'm storing the results back into an array with a list comprehension great. The fitness map is, I can get that by doing poplib. Population. get_fitness_map and I pass it the fitnesses. Just to verify. That takes an argument fitness. I've got my fitness map. What's next?I need to make a new generation or make a new generation. New_gen is going to be an empty list to start off with and I've got to fill it up with creatures. For creature index in range, and the range is going to be length of pop. creatures because I want to have one new creature for every oneso the population mains the same size as we iterate. I don't want it to grow and become a bigger population. What do I need to do to create a new creature?I, first of all, select the parents. Parent 1_index = poplib. Population. select_parent, and I pass it the fit map data. Okay, fine. Then parent 2 index is also pulled from that, so that pulls out two random parents. Well, not random. Weighted random from a weighted distribution, which is your roulette wheel selection. Then the next step is, make new DNA via crossover being careful, it says careful in brackets up there. How am I going to be careful?Let's just see how we go. I'm going to do DNA = pop-- Ooh, how do I get the cloud to do crossover?It's genome lib so I haven't got that yet, have I?I need to import genome as gen lib. That's the genome that gives me access to crossover and mutation functionality, gen lib. Genome. crossover. I'm going to cross over on the-- What does crossover take?It takes G1 and G2, whatever they are, let's make that clearer. G1 and G2 are raw DNA data lists of lists of floats. There you go. That's made that clearer. If I mouse over it now it should tell me that comment lists and lists of floats. How do I get the raw DNA data?That would be pop. creatures_parent1 index. dna. That's the first one. The second one is going to be P2 index. Let's put that down there, tab it in. Okay, P2 index DNA. I'm passing the raw DNA of the two parents I've chosen into this function so that I can now generate the new DNA. At this point, I'm going to go and be careful as it instructed me to, and I'm going to check out this crossover function. I didn't do many tests on this because I just wanted to write it quickly. It's possible that we can end up with some bad values for crossover here and the numpies catenate function might complain. What we're going to do is if XO comes out as zero, just return the whole of G2 actually, because we are crossing over at 0. 0. That means we're having none of G1, we have all of G2. If XO is equivalent to length of G1 - 1, then that would mean we want all of G1. Return in theory, G1. That's just making that a little bit it more robust so carry on. Then I can hit play at this point just to verify I haven't done anything silly yet, so let's just run it. That worked and it didn't crash. I've got some output coming out here that I'm going to get rid of because I don't want to see all that. I'm going to go into-- I think that's coming out of the simulation somewhere. Let's go into the simulation and find the print. Yes, there we go. I just did a search for print and there it was. Let's just come that one out, print that, and then I think there's another print somewhere else. Yes, there we go. Comment that out as well. That's got rid of the prints. Let's run it again and just check that it's nice and clean now. Nothing coming out there so that's good. Next step, where am I?I'm on the mutate step. That one. It says mutate checking for crashes on mutate functions. Let's mutate the DNA now. We've crossed it over now. Let's mutate it. DNA = gen lib. Genome. point_mutate. Let's do a point mutate first and point mutate-- Shouldn't be some square brackets there. Point mutate takes as its arguments the genes, the rate, and the amount. The genes are the DNA. The rate is the chance of a point mutation per number on the genes and then we pass the amount, which is how much it's going to get mutated by the maximum amount that it can be mutated by. There's another problem with point mutate, which is that it's inconsistent with the other mutate functions and the crossover function. Crossover, you can see actually returns the result of the crossover, which is great. Point mutate doesn't return anything, whereas the other mutation functions do they return the mutated gene. Let's just make it consistent, make it return as well. It's going to return the genes. That's now consistent. Let's go ahead and do the other mutations. We got the shrink and the grow mutate. Let's grow it first and then shrink it. It could be that we end up deleting the one that we just added, but that's okay. This one only takes one parameter, whereas it takes the DNA of course, but it also takes the rate, which is basically the chance of it deleting one. In that case, that's the chance of it growing, so a 25% chance of growing and then 25% of shrinking. That's good. What we want to probably do is check, shrink is robust as well because shrink is very basic at the moment doesn't do any checking. What I'm going to do is if length of genes is one just return genes, because I can't shrink-- If it's only got one gene, I can't shrink it because it will have no genes at all, it will be useless. Great. Where are we up to?We've mutated, we've checked for crashes on mutate functions and put the fixes in there and then we've got to make a new creature with this DNA. It gets tricky again here. The problem is that, as far as I know, you can't overload a constructor in Python. You can do some nasty stuff, like put some flags in there, which make it behave in different ways, but that's what we call module control coupling, which is generally considered not a good technique. What I'm going to do instead is basically construct a basic creature, and then I'm going to have a function to replace its DNA. What I'm going to do is say creature = crlib. Creature which I imported earlier and I'm just going to give it one gene, so it's just going to be emptyish. Then I'm going to do CR-- Now I need to add this set DNA function on the creature, which will cause it to update its DNA. I'll put it here. So, def set_dna(self, dna). What's it going to do, it's basically going to say self. dna = dna, and then it's going to do all this stuff. You notice the constructor, instead of receiving some DNA, it generates some random DNA. but then it does a bunch of stuff to set itself up, so it's ready to generate the XML, doesn't it?I need to make sure it does that here as well, because once I've updated the DNA, it needs to regenerate all the links and joints and stuff and motors. That's what it's doing there and resets its position and everything else. Great. That should be good to go. Now I have programmed the set DNA's function. I'm going to call it on the creature and I'm going to pass the mutant crossed over DNA, and then I need to just add the new creature to this new gen list. I just do that. That is more or less it for my one iteration GA. I can run that now and see if it's going to cause any problems. That's running well. Next, I need to actually iterate on it right. Of course, we've just evaluated one generation there, so we need to iterate on it now. What we're going to do is, so those are the bits where I create stuff, that all the rest of it is going to tap in and be put inside for loop. I'm going to say, for generation in range 10, let's just do 10 generations for now. It's just going to run that code again and again. At the end of this bit here, I need to then say pop. creatures = new_gen. Essentially, I'm just replacing all of the creatures from the previous generation with the new ones, and you could do something more complicated, like storing them or something, but I'm just doing the basics here. That's iterating. I just want to print out some stats just to show what's going on. I'm going to import numpy as np, and after I've evaluated the population, I want to print out some stats, I'll print the generation and I'll do the highest one, and I'll do the mean. That's just showing what the best one that we've achieved is and what the mean is. We're ready for a little drum roll now because we're going to run the iterated genetic algorithm for the first time and here we go. Let's press play. There it goes. It's kind of jumping around a bit and it's not necessarily increasing the fitness very well, but then we haven't really completed the implementation totally yet. Let's just do another run just out of interest, one more run. [silence]That's more or less it, that's what we're going to do. We've now got the genetic algorithm implemented, and it's running and we are generating different general population each time. The fitness is jumping around, it's not necessarily increasing, but we're going to work on that in the next video when we do a few tweaks. In this video, we've been pulling all the bits together of the genetic algorithm and kind of putting it into an iterated proper algorithm which is able to evolve a population of creatures."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Tweak the GA,https://learn.london.ac.uk/mod/page/view.php?id=96331&forceview=1,"[music]In this video, we're going to makea couple of quick tweaks to the GA to make it perform better. First of all, we're going to implement elitism, then we're going to improve a better distant moved metric In other words, a better fitness function. Elitism means keeping the fittest or fittests. That means that basically, we take one or more of the fittest individuals from each generation. and we store them unchanged into the next generation. How are we going to do that?Let's go into the code and do it. At this point, this is the algorithm. We've done our evaluation here. At this point, we know which one is going to be the fittest. What we're going to do is we're going to store that one for later. We'll have a variable called elite and we're going to store the fittest into the one called elite. We know what the fittest is. fmax = mp. max(fits)That tells us the fitness of the fittest one. We just have to locate it. for cr in pop. creatures if tr. get_ distance_ traveled() == fmax, then elite = cr, and then we can break because we found it. That just stores the fittest one for later. Then what we can do at the end down here is simply say--well, before we add it into the creatures, we can just delete one. We get a new_gen(0). Bad luck number zero. You're being replaced with the elite one. That's elitism. Let's just run that and see what it does Obviously, it's not necessarily comparable because it's a different ran, but you can see the numbers may be more favorable than the previous run. Like I say, each ran's different, because you're starting outwith a different ran and population. I would say definitely, it looks better because it's keeping that highly fit one for the next generation instead of making a weird mangled version of it. That's elitism and that's standard thing in GAs. The other tweak I wanted to make is a better distance. We could do this, right?One thing, if we notice the creatures, let me just run up my motor test script here. Oops, sorry. If we go into here, and go python motor_test. Let's have a look. You can see that some of these creatures, they move around, but they go around in circles. They've got some good movement going, but they don't really goin a straight line. What I'm going to do is make a fitness function, which means that we just measure how far they move in total. Rather than saying how far did they move maybe from the origin?We're going to say, during the whole simulation, how far were they able to get?How much distance did they cover?We can just make a little tweak on the creature to do this, and we'll see maybe that's going to be a friendlier fitness functionso they don't have to travel in a straight line to get the maximum fitness, if you like. It's all going to be in here. What I'm going to do is, start out with self. dist = 0. I'm going to do that here as well, because when we reset it, it should be reset as well, just in case. Reset the distance. The distance is going to be zero. What I'm going to do is then every time we update the distance, I'm going to say, here. Basically, once we've moved a little bit. I need to work out the difference between where we were beforeand where we are now. If self. last_ position != None, that means we do have a last position so we can work out our distance. We can just pinch this thing here, and say, basically-- Oops. Position one is my last_position. Actually, position two is going to be the new position, so the new incoming position. I'm just going to say, that's the distance that you can leave distance between the two. I do self. dist = self. dist + the new distance. It's really just saying how far have I moved since the last time my position was updated, and I just add that to this total distance. I'm going to calculate my total distance over time. Then what I can do here, instead of doing all this stuff, is I can just say return self. dist because it already knows how far it's traveled. It doesn't need to compute it at that point. Let's run that and see what difference it makes. Hopefully, it won't make the difference that it crashes. You can see, I've got greater distances. Oh, it did crash. Whoops. I think that's a different bug, but which I will solve for you later. Anyway, you can see I'm getting much higher distances because I'm now calculating the whole distance. It moves around in a circle really fast. It gets a nice long distance. You can see I'm getting much longer distances. It seems like it's going to be getting a bit more capability to improve its fitness over time. I prefer that fitness function. Like I say, you can spend the rest of your life tweaking these things, but I think that isa better fitness function to look for interesting types of movement in these creatures. That's enough. Back over to the summary. In this video, we've done elitism We've implemented elitism, which allows us to store the fittest individual every generation, and we've improved the distance metric. In this video, we've just be makinga couple of tweaks to the genetic algorithm"
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Saving and viewing fittest,https://learn.london.ac.uk/mod/page/view.php?id=96333&forceview=1,"In this video, we're going to complete the GA by adding some export capabilities so that we can actually see what the creatures areas they're evolving. We want to be able to export the genome to CSV. We could probably look at the XML files because we've already got that implemented, but the problem with the XML file is, it doesn't contain information about how the motors run over time, but we can get that information from the genome, so instead, we're going to write the genome to disk because that's the ultimate description of the creature. We're going to export to CSV. We're going to import it back in from CSVand just see how we can generate the DNA from a CSV file and then we're going to add some code to the genetic algorithm so that it saves the elite creature's DNA out to disk every iteration, so we get a copy of the best creature each time, which we can then review later using the final step, which is to actually be able to run something that we've loaded in from CSV in a real time simulation. First of all, exporting to CSV. What we're going to do, we're going to basically iterate over the genes and write the values of each gene out as one linein the CSV file. One line will contain all the values for a single gene, so we'll have multiple lines. How do we that?Well, I've got my test genome script open here, and I'm just going to write a quick test to start this off. Test tocsv. What I'm going to do is essentially just write out to seeif it can actually generate a file. I'll say g1 equals and I'm going to put a known genome in there. I'm going to say genome. Genome. to_csvand I need to tell it I'm sending the g1and the need to pass it, the file name, so test. csv. I'm just going toassertTrue. Having a little trouble writing assert True there. It's os. path. exist sand test. csv. It's basically just testing if it's written a file or not. I think I need to import os, if I haven't already. I've got os imported up there, so I've got access to that path. exists function. Let's run this test and you can see that it's failing because genome does not have a to_csv function yet. Let's go to our genome script and write that. I'll get rid of the console. Over to the genome. I'm going to put it at the bottom of those mutation functions that we were working on before. Let's just do it here, static function. It's called to_csv and it takes two arguments, which is the DNA and the CSV file. All I need to do is I'm going to create a CSV string, and then I'm going to do it with open CSV file, in write mode as f, and I do f. write. I'm just going to write the CSV string to disk, which is currently empty, but that is the minimum code to pass the test. Let's run that code again. Let me pop that up. You can see it's now passing that test. I've commented out all the other tests, by the way. That's why it's only running one. It's running one test. I'm going to write another more challenging test, which is to check what's actually in the CSV file. This time I'm going to do this stuff, but I'm also then going to say, I expect to see in my-- I expect to read back from the CSV file, 1, 2, 3, then comma, an extra comma at the end and then an end of line character. That's what I'm hoping to see from the CSV file there. Then I can then read from this what I actually got, which is CSV string equals, with open test. csv as f. CSV string equals f. read and then I can just say self. assert Equal CSV string and expect. Basically I'm saying, verify that the string that I read infrom disk is the same as the string that I've expected to read inand of course this is going to fail as well because it's just writinga blank string, so you can see it saying assertion failed. Blank string is not equal to 1, 2, 3. I need to do some work now and actually implement this properly. What am I going to do?I'm going to say, for gene in DNA, CSV string and then for value in gene, I'm going to do CSV string equals CSV string plus string version of the value plus a comma. Then once I've done one gene, I need to add a line, so I do CSV string equals CSV string plus I'll add the end of line character. That basically will print all the values out, convert it to strings, and then add an end of line character at the end. Let's run that one. Now that test is passing as well, so that's good. I've realized I've got two with the same name. Let's just do that and check that it's actually passing. It's passing the two tests. Okay, great. That is writing out to CSV. I could write one more test to try out a two liner, just to make sure I'm really happy it's working okay. If I do a thing like this where it's got two lines, 4, 5, 6, and in that case, I would expect to say 4, 5, 6, then an end line. That's just a two gene genome and I'm checking that I get two line son two separate lines there. Oops. I've got some indentation problem. Oh yes, run again. That test is passing as well. Even with a two line genome or two gene genome, that's working too. The next one that we're going to implement is the import from CSV. This is a little bit more complicated because we've got to--instead of just pumping this stuff out to disk, we've got to parse the strings and split them up and stuff like that. This is the rough process we're going to follow. We're going to read a file into a string, so I'll read the whole file in, first of all, then were going to split it on the end of lines to get the separate lines in the file and then we're going to basically split each line in the file on the commas, and then convert each of the values that we get into floating point values. Then we can compare arrays that we get from this with np. array_equal, if we want to. Over to this, and this time I'm going to do def test_from_csv. Let's write something out to disk. We're going to write a CSV file out to disk first of all. Then we're going to read it back in. We're going to say g2 equals genome. Genome. from_csvand I just write the name of the file in this instance. Then I do self assert True, and I'm going to do np. array_equaland I pass it the two arrays. It's going to be g1, g2. That's the way I can do it. Let's run that test. You can see that test is failing, we don't have an attribute from CSV. That means we need to write that function. Let's go over to this and do that. Again, this is going to be static. Def from_csv file name. Let's go through that algorithm that we just saw on the slide. Read to a string and split it on the thing. First of all, I start off with an empty string, actually CSV string equals empty, with open, file name as f. We don't need the write mode because we're just reading in default read mode now. I can do CSV string equals f. read. That'll read into the string and then I do lines equals csv string split, which is a built-in function on the string there. Split on the end of line character for line in line sand then I do vals equals line. split on the comma character. Then I can use a list comprehension to squash that set of values down to a nice array of floats. How am I going to do that?Well, let's start off with a DNA array here. I can say gene equals float version of v for v in vals. if v is not equal to an empty string, because we might end up with empty strings in there, it might be an empty line, we can do it like that. Let's just check if we got something. If length of gene is greater than naught, then dna. append gene. Finally, return dna at the end. That's the basic algorithm. Let's just go through that. We create an empty string, we open the file and we read the whole file into the string. We create an empty array, ready to store our genes as we read them in. We split the contents of the file on the end of line character to give us all the lines. Then we iterate over the lines and split into an array of values by splitting on the comma in each line. Then we convert the list of values, which we got from splitting on the comma, into a list of floats and check only if the values are not empty string sand we verify that we got some values and append it to the DNA if so. Let's try it out. Self. assert equals list object has no attribute g2. There should be a comma there. Let's try again. That test passed and just to sanity check it, I can actually maybe print out g1 and g2, just to have a look at them. Print g1 and g2 just to have a look at them. G1 is that, and you can see one difference it's made is that it's converted it into floats, but luckily the np. array equal is happy to accept that 1. 0 is equal to 1, so that's cool. We could do one more test the same way we did before with two gene genome. Let's do that and go 4, 5, 6, like that. That's a two-line genome just to put a 2 there and run that. That also works with a two-line genome, so that's good. That's all working. I'm done on my importing from CSV. The next thing that I need to do is to save the elite creature out to CSVas the evolution is running. I'm going to go back into my genetic algorithm code and see if I can do that. Here we go. Where is it? Test_ga. I think I actually wrote the whole ga in test_ga in the end, didn't I? There we have it. Where's the bit where I have the elite? I've got it. That's where I'm storing the elite genome onto the replacing the first new one with the elite one. At this point, I've got access to elite, so I could actually say, if I've got access to gene. I could saygenlib. Genome. to_csv. I could say elite, which I think is a creature, so I do elite. dna. Now the question is, what's the file name going to be?Well, I could call it something like-- I could basically generate the file name from the iteration. Then I would have a series of CSV files, one for each iteration, and that would be quite neat and I've got a value here, which is generation, which I could use to build that file name. Let's do that. We can just say csv_filename equals string version of generation plus . csv. How about that? Generation something elite, and then I can just put that there, csv_filename. That's pretty much it. I should just be able to run this and I should see that it generates these file names as I go. Let me run that here and see how we go. We should be able to see files appearing with this. There we go. There's each iteration, you can see a file was created with the genome of the elite individual, which seems to be the same each generation, which is interesting. Oh, no, slight variations, you can see that value there got mutated a little bitin that iteration from 7 down to 6. It's obviously changing. That looks sensible. That looks like that's working. I'm going to go to my next slide. The final step is to actually be able to run one of these things in simulation. This is the big moment really, because what we're going to do hereis actually load one of these saved genomes into a simulation, and then run it to see how it looks after the evolution. The idea is that we can then run the evolution many iteration sand see what really fit individuals actually look like walking around in the world. How are we going to do this? Well, it should be fairly straightforward. What I'm going to do is I'm going to take my test motor script. I've got all this stuff in here, but that's all right. I'll go to my motor_test, it's called, isn't it?Yes, there we go. I'm going to save this one as a new file and I'm going to call this onerun_genome, basically. Oops, run_genome. I'll call this one run_genome and what this is going to dois it's going to load instead of generating a random creature, which is what I'm doing up there, I'm going to loadthe creature off from disk so I can do it like that. DNA equals genome. Have I imported genome?Yes, let's import genomeas gen lib. We call it gen lib, then equalsgenlib. Genome. from_csv, and I'm going to maybe pass it3_elite. csv, and then I can do c. set_dna dna. That should be it [chuckles] with a bit of luck. If I run this one, I should be able to see what that 3_elite actually looks like. I've got as far as 3. Yes, let's check. Let's run this one. There it is. This is the one that was able to move the most. You can see it's clearly moving a bit there. It's got some sort of shoving motion going on, which is pretty good. The obvious question is what happens if I run it for a few more iterations or do a much bigger initial population and things like that?We're going to do that in the next video and we're going to see what some really fit individuals actually look like. Let's summarize. In this video, we've just been implementing some code that allows us to store and reload later the fit individuals as we're running the evolution, which allows us to actually loo kat these things and run them in a real-time simulation. We've now got our fully functioning genetic algorithm when we're ready to actually run some evolution runs and see what kind of creatures we are going to be able to evolve."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Demo some fit individuals,https://learn.london.ac.uk/mod/page/view.php?id=96335&forceview=1,"[music]In this video, we're going to look at some example creatures, which I've been able to evolve using the genetic algorithm. In summary, first of all, we're going to look at a kind of walker-type creature. Well, you can make your own verb up for what it's actually doing, but we'll look at that in a minute and that's a slow-moving one, but it moves in a slow and steady way. Then we're going to talk a little bit about the camera and how we can use the camera better to follow the creatures around. Then we're going to look at runner and we're going to think about whether we think that is a kind of cheating or not. First of all, the walker. Let me just play this one for you. What I've been doing is I've done a few runs and I've stored all the CSV files for the runs. This is my walker, which I've called it. If I just play that one, we can look at how effective it isand you can see it's definitely moving in a straight lineand it walks off the screen like that. That's exactly the kind of thing we want to be seeing when we're evolving these things. It's going to look similar to the Karl Sims ones if you dug them off of YouTube and that's pretty cool. Let me just show you this code for the camera now. The problem is as it walks off the screen, he can't see what it's doing anymore, and without having that much control over the camera, it's difficult to zoom inand see exactly how it's achieved its walking behavior. These lines here allow me to fix that. The first line is if we've seen before, it just gets the current position of the creature. The second line below is essentially changing where the camera is, first of all, and where the camera is pointing. Where the camera is essentially it's set at a distance from the creature. Let me just take you through the arguments actually. I'll put those lines back in. First of all, the first argument, this number five here, that's how far away the camera is from the creature. Imagine I've got the creature, the creature's there and I want the camera to be a certain distance away. That five is how far the camera is away from the creature. That's the first thing. Then the next two numbers are to do with the angle of the camera. Is it pointing up and looking down at the creature or is it at an angle and things like that?The first one, 0, which means it's flat on that axis and the second one is 200, which means it's at an angle point looking at the creature like that. I just wasn't quite sure exactly how they measured it, so I just experimented to get a good number there. The final argument is the target for the camera. If you think about the camera, so we've fixed that angle and we've fixed that angle and we've fixed the distance, but then where is it looking?The idea is that it's now going to look at this position of the creature, so I'm basically pointing the camera at the current position of the creature. Let's run it again and see what happens now. Now, you can see the creature is right in frame, and the camera is moving along as the creature moves along so we get a much better view. The only problem is now we've lost the ability to manually move the camera because, of course, that line is running every frame and it's updating the camera every frame. That's the walker and that's the camera fixed. The next thing I wanted to show you is what I call a runner and we'll think about whether we think this is cheating or not and talk about cheating in general. This is the runner. Let's have a look at this one. You can see this one is moving really rapidly. Now, if I just run that again, but with the camera movement disabled, let's see what this one does. I don't know if you saw that. One more time. Hopefully, the frame rate's high enough that you saw that it just whizzed off the screen. If I put enable-the-camera-following again, we can watch it zipping around. You can see it's leaping around and it's obviously figured outsome way of-- It's created a fancy ball, but it's got multiple joints in there. It's like a weird flexing joint ball. It's invented that, which is pretty cool. Maybe we don't really want creatures that can cheat like this. Often I found when I was developing this that I had cheaters that were exploiting possibly flaws in the physics engine or limitations of the physics engine. I was actually chatting to a friend and colleague who I worked wit hover in Australia, John McCormack, who actually knew Karl Sims. He said that Karl Sims said these kinds of cheats and things were a really good way of testing, making sure your physics engineis correct, because the more correct it is, the more difficult it is to cheat. I've given you a link to this surprising creativity paper later, which actually shows a bunch of examples of this kind of thing. Now, let me just show you one more if I just play this file. This one is a really interesting one, which if we look at this, you can see that it's spinning around and it builds up this momentum and spins itself around. Then finally, once it builds up enough momentum, it whizzes off into the air, which is pretty fantastic. I'm not sure if it's cheating, maybe it's cheating because I thin kit might have been exploiting some feature of the way the joints work. Some springiness of the joints to make this energy from nowhere so it could fly. When I saw that one, I thought, yes, that's amazing, but I'm pretty sure it's cheating. This is the question, is it cheating and what can we do about it?What you'll find when you start running this a few timesis sometimes the fitness will suddenly leap up from around a distance of 10, and then it'll suddenly leap upinto 200 or 100. That typically means you've got something a bit like my runner, you've got something that's figured out a way of getting a big burst of energy to whizz off. What you can do is you can actually improve your fitness function. What it means is the genetic algorithm is working. It's successfully finding out the best way to exploit the situation such that it can move as fast as possible because that's what you've asked it to do. This is a really important thing that you'll learnas you get more into machine learning and AI. This is what happens. It just tries to find the simplest solution or the quickest solution best solution that it can. That's what the algorithm does, so it is up to you to limit itand stop it from doing the wrong thing. For example, if you don't want things to move at excessive speed, what you can do is you can monitor their speed as they're being evaluated. If the speed looks like it's too high, then you can just set that creature with a fitness of zero and get it out of the population. That's the way to do it, or you can just replace it with a new random creature because it's obviously a failed line that's learned how to cheat. There's all kinds of things. It's all about improving your fitness function and probably then maybe looking a bit at the physics engine and seeing if you can tweak it a bit to make that less easy to do. Things like the inertia properties on the joints and things you can adjust those to fix some of those problems. It's a fascinating world where we're looking at here where the algorithm is exploiting the environment that we're placing it in and finding the best solution to the problem we've given it, which is fast movement, basically. I'm just going to wrap it up there. In the discussion threads and things, I'm encouraging you to go offand create and find interesting cheaters and interesting, good solutions as well that aren't cheating like my walker. Post those. If you can get videos on there and links to videos, just share it with other people because it's so interesting seeing all of these different things that grow. It's like a biology experiment. That's it. What we've been doing we've just been looking at a few examples of evolved creatures and I've been pontificating a bit about why that's interesting and different factors that are going on there. We looked at a slow-moving walker and then we learned about how to get the camera to follow the creature in case it's really fast. Then we looked at a runner and finally at the end there we were just sort of discussing how you can deal with things that look like they're cheating. In this video, we've just been discussing the outcomes of some of these evolutionary runs and looking at some interesting examples."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Lesson 4 summary,https://learn.london.ac.uk/mod/page/view.php?id=96339&forceview=1,"Brilliant, you made it to the end of Week 4, so you should now have a fully functioning genetic algorithm, which allows you to evolve creatures just like Karl Sims did, well, more or less, except for the neural network part. Just like Karl Sims did back in 1994 on those amazing thinking machine systems. That's really a great achievement, so well done for getting this far. Let's just check those learning objectives to see what we've specifically learned this week. We should now be able to explain what fitness functions are and what the population model is and how they're used in genetic algorithms to optimize a set of solutions, especially with regard to this idea of multi planar sampling where you're sampling in different place sin the search base at the same time by having this population of different solutions. Secondly, we've actually gone ahead and implemented a fitness function, as well as being able to explain what it is, we've also built one from scratch. We should have a really strong implementation knowledge of how to go about building a fitness function. Actually, the fitness function was quite simple, wasn't it?It was really just measuring how further [?] moved in different way sand using that. There are much more elaborate fitness functions and I've encouraged youto investigate those and see what other things you might be able to use to control the evolution of your creatures. Number three is that we've been using genetic manipulation techniques, so that's the grow mutations, the shrink mutations, the crossover, and point mutation to change and interbreed and crossbreed these different solutions to the problem, which gives us that multi planar sampling capability, which is one of the key features of a genetic algorithm. In summary, well done for reaching the end of Week 4. You've now got a fully functioning genetic algorithm. We're not going to do a whole lot more programming on this particular case study now. Next week we're going to be looking at where this stuff went next. What has happened since 1994?Surely people have invented some cool new stuff since then and they absolutely have. Come back next week to find out more about that."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Lesson 5 Introduction,https://learn.london.ac.uk/mod/page/view.php?id=96341&forceview=1,"[music]Okay. You came back for Week 5. Well done. If you really want to know about the state of the art, you're in the right place. We're going to be covering a few different papers this week where we find out about what researchers have done, if you like, since 1994, and certainly, the last few years to add to this ability of genetic algorithms to involve different types of morphologies. What are we going to learn about?Well, first of all, at the end of this week, we're going to be able to select and explain examples of the state of the art in evolutionary computing. We're not just looking at evolving weird and wonderful creatures using genetic algorithms, but you're also going to see some other papers where people are evolving things like neural networks to solve image classification problems and more traditional types of AI, machine learning type problems like that. Then we're going to have a brief discussion of the challenges in the field of evolutionary computing. What are people trying to do in the most recent papers?What are the problems they're hitting against?We'll learn that from looking at some of the papers and having a good read. You're going to have to do some reading of papers. I'm going to guide you through a few of the papers, but I'm also going to give you links, so you can go and read the details. Like I said before, now we've actually built this thing ourselves from scratch. We've got a really strong insight into what to expect from these papers. Hopefully, having built that genetic algorithm, when you look at a genetic algorithm paper, you're now looking for those things, you're saying, ""What's the encoding scheme?What's the population model? What are you using?Tournament or roulette wheel, how does it work?"" Because you know what those things are now. Well done for reaching Week 5. We're going to have some interesting looks at some of the state of the art work this week."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Going beyond Sims,https://learn.london.ac.uk/mod/page/view.php?id=96344&forceview=1,"-In this video, I'm just going to give a brief overview of some papers which I've dug up, which carry on in the vein of Sims of evolving interesting morphologies for moving creatures. These are the things we're going to be looking at some of the themes that have developed since Sims. First of all, we're going to look at a paper where they evolved hard body robots in simulation similar to the things that we're doing, and they then were able to actually 3D print those out into real robots, which could move around. Then we're going to be looking at some soft robot studies which is where the things went next after the hard body stuff. These are deformable robots that can bend and squashin different ways. There's various papers we're going to briefly look at which evolved soft body robots, and even a couple of them where they actually were able to implement these in real robots. Finally, at the end, I'm going to show you a paper which I really urge you to read which just gives lots of examples of evolving morphologies, and interesting stuff happening with artificial evolution, and soft creatures, and so on. The first paper is Lipson and Pollack 2000 paper, which was published as a letter in Nature magazine, because, I think, at the time, this was pretty hot stuff. It's quite exciting because people knew about Karl Sims. They knew it evolved these creatures in simulation. What was different about this paper is they evolved them in simulation, but then we're able to manufacture them using what was probably then quite an early version of 3D printing. Let me just show you roughly what it looks like. I can't show you the pictures, unfortunately, due to copyright reasons. This is roughly what they were doing. They had these sticks, if you like, and they had joints, and then they had motors. You could essentially build a robot out of a bunch of joined together sticksin different forms. Then you could print the sticks out, and the stick swill have different connecting angles here. You can 3D print these sticks, then you'd have to stick some little motors in, and then off they go. The motors would be controlled by some sort of micro controller, presumably, which would basically give it the action. Having looked at the Sims thing in such detail in the last many videos, you've now got real insight into how you might go about doing that. Also having looked at all this code in great detail, and knowing what it's all about, you're probably really good at thinking, what are the questions to ask when you're going into a paper, because when I'm reading research papers, often you can get lost and bogged down in the detail. Now we've looked at this area in detail, we've got good questions. Now the questions I'm going to ask you this paper are, what's the encoding scheme?How did the fitness function work?Those are the things we're going to be asking. I've looked it up in the paper. The population size was 200, so that gives us some insight there. They were evolving for several 100 generations, you can see there. The robots are represented by a string of integers and floating point numbers that describe bars, neurons, and their connectivity. It did actually have a neural network controller, which was controlling the motors, very similar to the thing that we implemented a rhythmic motor control system. The fitness function was just the distance moved in simulation, you can see there. As I said, they printed them with 3D printing with a micro controller's controller. That's the first paper. Obviously, I'm going to give you a link to this. You can go and read it. The second paper is Hiller and Lipson's 2012 paper where they reported on a system which allowed them to evolve soft robots in simulation. As I mentioned earlier, soft robots are deformable robots, maybe made out of foam with little motors embedded in and stuff like that, where you can actually bend and deform the robot. They're not like hard bodied robots that can't deform. They were able to evolve these things in simulation. Actually, they were able to do some rudimentary manufacture these robots as well, but it's not totally clear how advanced that part of the work was. You can read the paper and find out more. In 2014, Cheney, Mac Curdy, Clune, and Lipson, so Lipson again, worked on this new version of the system where they got bit more into the evolution of soft body. They had a more advanced version of the soft body robot thing. It's worth knowing that I think Jeff Clune is now at Uber. I think it was released at the time of filming. I think he's the head of research at Uber now. In 2014, Jeff Clune was messing about building these amazing evolving soft robots. That just shows you the kind of pathways that people take. Now, going back to my thing about--Okay, you know what kind of questions to ask now. The genetic encoding scheme was they made the robots out of deformable voxels. The thing that was being evolved was the neural network, which generated the creature, which is quite interesting. Let me just draw a diagram to maybe explain that a bit more. The idea is that you've got this cube like this, or a cuboid in this case. You've got a cube. The objective is to build a robot inside the cube. You have this neural network, which is the subject to the evolution. The neural network generates signals in different ways. It's more of a signal generating network, maybe than a neural network. Those signals dictate which of three materials you're going to choose. You've got three different materials you can spray in there, and also where they're going to go. What it does essentially is it goes--I'm going to put a bunch of that stuff over in the corner there. These are all deformable cubes really. Then they might switch to another type of stuff and put that over there. There may be a switch down here. It's the network that's controlling the spray gun, if you like. This is not how they describe it in the paper. This is how I understand it. It's my simplified description of it, this idea of a neural network controlled spray gun that spray voxels of three different materials into the space. What happens is then, I think they choose the biggest lump, and then that will be the creature basically. Anything that's not connected to it, they throw it away. You end up with this creature made out of lumps. There's some great videos on YouTube, which unfortunately, I can't show you here because of copyright. They're all there. I'll give you links. You can go and look at them. This is the idea. Eventually, the soft body robot have these three material types. One would be like a muscular type thing that could screech and move. The other ones might just be a spongy thing, or a hard thing. Any other robot then could have different patterns of these materials in there. They have all some really interesting gate sand ways of moving in this work. Moving forward, actually I'll leap into 2021, just to show you people are still doing this stuff. In 2021, there was a research paper published in Nature, machine intelligence, where they were looking at actually evolving morph able robots. The subject to the evolution was not so much the whole form of the robot, but the way in which it can morph from one shape to another. Again, this one had a hardware component. They're actually implementing these in hardware. For example, you might have a strip of deformable material. It could form into a wheel and roll along, and then it might change shape into an inchworm and do that movement, depending on the terrain. They were looking at changeable robots that could adapt appropriate control and shape for different contexts. The final paper that I'm going to mention is this one here. You'll notice that these are some of these familiar names here. Clune was on this one. Also, Peter Bentley worked a lot in this field as well. This paper just presents a whole load of examples of different systems, which were able to create really interesting form sand behave in unexpectedly creative ways. It's a nice, interesting paper that summarizes--It's from 2020. I'm looking back on lots of the older work, and also some of the more recent work and trying to show you some great examples. I recommend reading this which is quite long, but it's a really interesting article. In this video, we've just been looking at some state-of-the-art papers, and also looking at what happened after Sims, what people did building on these hard bodies, robots, and working towards actually implementing themin the real world by using 3D printing, and also looking at soft body robots which could deform and have different types of materials in them, and really just going to the next level from where Sims was at with these creatures work. We also finally looked at this set of really interesting anecdotes from key researchers in the field, who think this stuff's really interesting, and gave you examples of when it's very creative and comes up with unexpected solutions to problems and that kind of thing. In this video, we've just been reviewing some papers that happened after Sims that were evolving robot morphologies in different ways."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: State of the art for GAs,https://learn.london.ac.uk/mod/page/view.php?id=96347&forceview=1,"[music]In this video, I'm going to look at a couple of state-of-the-art trend sand applications of genetic algorithm sin other areas of morphology than basic robot design. What am I talking about?First of all, I'm going to talk about evolving neural networks. Instead of evolving robot designs, we're going to be evolving neural networks. Then we'll talk very briefly about evolving program sand some of the trends and benchmarks that are happening. This is a research paper where they report on the being able to evolve neural networks which are competitive with human design deep networks. I'll give you a link to this paperso you don't need to take it down now, but you'll have it in the reading after the video. Now, here's the motivation for doing this stuff. Why evolve a neural network when we've got really good ones at work already?The fact is that we are reaching the limits of human design of networks. It's very difficult to design and optimize the design of a deep neural network. It is a highly complicated structure, which you don't know how well it's going to perform until you train it. Then you have to figure out if it doesn't work, why it doesn't work?There's this whole hyperparameterelement to it where you're trying out different settings and everything else. There's a lot of what I would call black magic involve din neural network design because you're basically trying lots of things out. That's why you need labs full of people doing it to get it to work. Now, I think that one other really interesting thingis about neural network evolutionis that it can potentially not just come up with designs which are as good as the human ones, but they can be more efficient because you can optimize them to get rid of the stuff it doesn't need, which will be very difficult for a human to do tosay, ""Oh, we're not really using that layer much, we can get rid of that. ""Potentially, it could be much more computationally efficient and that's really important. Deep neural networks use a lot of electricity. If you can do the same job but with a smaller network, that's great. On the opposite side of the coin to that, they're very computationally expensive. In order to evolve a neural network what you have to dois generate lots of different neural network sand then train each one with the same dataset, presumably, and then evaluate how well it performs, and potentially even try it with different parameters and so on. You can imagine that rather than just designing one network and training that, it can be very expensive to have to generate thousands of network sand evaluate those as we have done withthe Karl Sims' creature. It could be very computationally expensive to get the thing in the first place. Once you have it, once you've got your evolved neural network, it can potentially be much more efficient. It's the evolved neural network that's going to be running all the time, doing all of the inference. In a way, it's better to have that being really efficient but having spent a bit of electricity at the beginning but to be able to then havean artifact which is more efficient. How does it work?There's two schemes which I'm not going to go into great detail, but let me just summarize. You've got NEAT and Deep Neat. NEAT is essentially similar to the system we've been looking at. You have a chromosome if you like. You've got what they call chromosomes, but they really mean genes. They separate into chromosomal unit, so that's your genome. It's got a bunch of chromosomes as they call it, but I would call them genes with my biological sciences background because actually, a chromosome is a collection of genes, so it doesn't quite work as a word. Anyway, they call it a chromosome, so let's go for that. Each chromosome encodes for a node in the network, and how it connects to other nodes presumably. Then depending on what the chromosomes are and what their characteristics are, you get different types of connections. You can have a huge variety of different networks, very similar to what we were doing with the creatures. That's NEAT. Now, DeepNEAT, and it's got certain features of the genetic algorithm which make it interesting as well relating to how it manages the population into species and things like that. Some interesting state-of-the-art GA stuff going on as well. Now, DeepNEAT is different in that instead of evolving these smaller networks, it's actually looking to evolvea great big network made out of lots of layers which is what we do with deep networks. We have lots of layers with lots of nodes in them. Then they then get wired upin different ways and in different patterns. As you might be guessing, what I'm going to do here is that instead of encoding individual neurons, DeepNEAT actually ""has a chromosone""for each layer. You'd have a chromosoneencoding each layer and its properties. It operates at a higher level of organization than NEAT does. That's NEAT and Deep Neat. That's how they do it. No huge surprises there. Now, the question is, are they competitive with human designs, state-of-the-art networks?According to one of the authors, in 2019, the results show that the approach discovers designs that are comparable to the state of the art and does it automatically without much development effort. It makes very good design sand it's not that hard to use it because it's designing them for you. That's pretty impressive. That's in 2019 as well, so this is in the midst of deep learning conquering all genetic algorithms, still a very valid toolin the arsenal of the artificial intelligence scientist. Here's another one that I actually wanted to briefly mention. There's a real history of people trying to evolve software using evolutionary techniques, and here is a 2018 paper showing that people are still doing it. In this case, this is a big survey that will show you loads of examples of it. If you're interested in the idea of automatically designing software, which again is a topic that's coming up in deep learning now as well with the recent release of various tools from Microsoft based on the OpenAI networks like GPT-3. So yes, Autocode completion, that kind of thing. People have been doing Autocode stuff for a long time with genetic algorithms, and here is a survey of that. The final thing I wanted to mention is that another trendin genetic algorithms and evolutionary computing or bio inspired computingis the idea of benchmarks. As any computer science field, especially one that's related to optimization and efficiently solving problems, that kind of thing. As it evolves and matures, often it'll end up having a benchmarkso that when people come up with new algorithms, they're running them against the same dataset, or there's a fixed challenge which people attack, and then that gives thema valid way of comparing algorithms. That's absolutely happening in bio inspired computing as well. Here's a bit of a review of some of the stuff covered in this article. In this video, I've just been giving youa bit of a run-through of some of the other areas that evolutionary bio inspired computingis being applied to. We looked in little bit of detail how we can evolve neural networks using genetic algorithm techniques. Also, I mentioned briefly that people are also using these techniques to evolve software programs. Also, I mentioned some of the trends such as benchmarking that are going on in the field. In this video, we just had a brief review of some other application areas that are current and trending in the area of evolutionary or bio inspired computing."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Lesson 5 summary,https://learn.london.ac.uk/mod/page/view.php?id=96352&forceview=1,"[music]Here we are at the end of Week 5. This is more or less the end of the Creatures Case Study. Just to say congratulations on working your way through this. I hope you've enjoyed this deep dive into genetic algorithms and evolving morphologies. Really, for me, this has been such a great journey. I've really enjoyed developing and explaining to you the art of genetic algorithms, which is something I havea personal interest in and having studied it in depth in my PhD. It being one of the things that really got me into programming and computer science in a really intense way back when I did my master's course where I worked on artificial life systems and genetic algorithms. Let's look at the learning objectives. Now, having finished Week 5 in this particular part of the course, we should be able to select and explain examples of state-of-the-artin evolutionary computing. We should now have that having built one of these things in depth from scratch. We now have a real insight into how the guts of them are, how do they really work?You should be able to discuss the challenges of the field of evolutionary and bio inspired computing. What are the things that people are really trying to do at the moment?Looking at those evolving deep network sand those really heavy computational task sand trying to make those more efficient and see if they can compete with the behemoth of deep learning using these bio inspired techniques which, in a way, are much more nimble and can potentially really revolutionize the way that deep learning is done. Indeed, some of these systems are working towards that. Well done in reaching the end of the Creatures Case Study. As I said, I really hope you've enjoyed it. I've really enjoyed explaining all this stuff to you, and hopefully, my passion for the subject has come across and you found it engaging and there wasn't too much programming. If there was, that you were able to skip it and catch up in the lab worksheets. Well done. You've reached the end of the Creatures Case Study."
DSM100-2022-OCT,Topics 5&6: Creatures,Lecture: Summary of Case study 1 – Evolving artificial creatures,https://learn.london.ac.uk/mod/page/view.php?id=96353&forceview=1,"[music]-That's it. You've reached the end of the AI creatures case study. What I'm going to do in this video is I'm just going to review some of the things we've been working through and give a few reflection son what we were doing as we went through. What I'll say, at this point, is I really hope that you enjoyed watching me program and then following along in the lab worksheet sand programming your own version of this genetic algorithm. I really hope that you're inspired to go off and try and adapt it, improve the fitness function, work on the morphology and coding, and all the kinds of things because I think it's really inspiring area to investigate and work in. You could end up tweaking this thing for weeks and week sand really get some interesting results. I hope you're able to share some of your interesting creature sin the forums with other students so that people could see. If you think about it, what we created here, each of you evolving, doing your evolutionary run, was like a separate niche and ecosystem. Then, we're sharing the creatures over multiple populations across multiple machines. Really, what we've done is created a distributed genetic algorithm, but that only works if we're sharing CSV files with each other. Let's go back down to earth and see what we've been doing in the last few weeks. First of all, just to remind ourselves, we've been learning about genetic algorithms. Genetic algorithms are a branch of, what's generally known now, as bio-inspired computing. It used to be evolutionary computing. That is a branch of artificial intelligence. It's a very powerful set of algorithms you've seen that can really figure out how to solve problems really from scratch without needing to have a massive data set to begin with. That's what's interesting about this type of algorithms to me. We've been actually using genetic algorithms to solve the problem of how to make walking robots. We've been re implementing a famous research paper from the '90s, which, if you ever study bio-inspired computing, typically that video, you'll get shown that pretty early in the course. This is amazing. What's great about this course is we've just implemented the whole thing ourselves from scratch, which is probably not so common in those other courses. What did we do?Week 1. We were learning about bio-inspired computing, we had an evolution theory crash course, and we had an introduction to the original research paper and how it works. You might remember my slightly iffy pie chart showing how roulette wheel selection works. Week 2, we were looking at beginning our coding process. In particular, we were looking at the URDF file format. URDF is really an interesting file format to know about because, if you go and look at other robot simulation environments like MuJoCo and Gazebo and ROS, of course, you'll see that that file format is supported in many different environments. Once you know what it's about, it's really transferable and useful skill to know. It's been worthwhile learning in detail how that works. Of course, we've been abusing it terribly by automatically generating URDF files, but that's the way it is. You might remember this image of this poor robot stuck in the floor because we weren't running the simulation and in the middle of an interesting temple there. Week 3, we started getting more into the coding. It was a pretty intensive coding week there. We were working on the genetic encoding, which allows us to get a list of random number sand basically turn them into a moving, real robot or simulated robot in our 3D environment. You may remember we started seeing things a bit like this at the end of Week 3. Week 4, we continued with the intensive coding and worked on the fitness function and the population models. We actually completed the genetic algorithm. Hopefully, in Week 4 is when you were seeing you were able to do some evolutionary runs, and you were sharing with other people on the forum sand seeing weird and wonderful creatures coming out of the system and lots of cheaters, no doubt, as well. You remember I showed you this spinning flying robot. That was my favorite cheater, the kind of thing that invented being a helicopter, basically. You may or may not have seen something as complicated as this depending if your fitness function actually selected for huge complexity or not. Week 5 was when we'd completed the GA, but we looked at some of the State of the Art papers, just tracking what happened with this research thread going from when Sims wrote his original paper in '94 through to, more or less, present-day. Looking at some of the latest articles where people are actually evolving soft robots using soft-body physics, and also even creating these things in the real world. Actually, being able to, once you've evolved something, then create it, 3D print it or something like that, into the real world and see how it actually works. That was pretty interesting. We also looked at some of the neural networks, how people are evolving deep neural networks. Actually, some of them are competitive with human-designed deep neural networks. This is definitely a hot area of neural networks research at the moment, bringing evolutionary or bio-inspired computing back into the mainstream again. That is, more or less, it. I'm just going to wrap up here and say, thanks a lot for following along for the course to the end here. Well done. You've done a lot of thinking, a lot of learning, and I hope that it was interesting and worthwhile for you to do that. I've really enjoyed developing all of this content. I hope that enthusiasm comes across and that you've now got an interesting evolutionary computation system running on your machine. Great. Hope to see you again in some of our other content."
DSM100-2022-OCT,Topic 7: Robot Scientist,Introduction to Topic 7,https://learn.london.ac.uk/mod/page/view.php?id=96357&forceview=1,"[music]This is topic seven. We'll focus on a case study, a case study of the robot scientist. The robot scientist is an AI system that is capable of scientific discovery. In this topic, we will look at the history of this area of research, scientific discovery. We will talk about real system making real scientific discoveries. We will have a closer loo kat this particular project, The Robot Scientist project. We will talk about its history, the main concepts of the robots scientist, what is the architecture of the system, and what are the components. To better understand this extremely complex system, we will consider a toy robot scientistas a multi-agent system, and we will look inside several agents. We will mainly focus in this top icon knowledge representation. We will consider a knowledge agent, agent that has knowledge model of a particular domain. Our toy robot scientistis working on drug design. We will see how we can encode our knowledge about drug designin a machine-processable form, such systems as a robot scientist can work with it. Good luck with this topic seven."
DSM100-2022-OCT,Topic 7: Robot Scientist,Lecture 1: Discovery science and the Robot Scientist system,https://learn.london.ac.uk/mod/page/view.php?id=96361&forceview=1,"AI is increasingly affecting more and more areas of life. One such area is scientific research. The application of AI to scienceis called scientific discovery. As in other AI application areas, there are three technological trends driving progress. Computers are getting faster and faster by integrating more processors on chips. There is more and more data available for AI systems to work with. New and better AI softwareis being developed, especially in the area of machine learning. The history of AI is one of booms with overly optimistic expectations, and busts, so-called AI winters, overly pessimistic disillusionment. In the past, AI experts have made embarrassingly over-optimistic predictions about how fast things will develop. However, as someone who has worked in AI for over 30 years, I have recently been surprised about how fast things have been developing. One thing that is different nowis how central is to the most valuable tech companies in the world, Google, Facebook, Amazon, and their Chinese rivals. Vast fortunes are being made using AI, especially machine learning. AI systems have superhuman scientific reasoning powers. They can flawlessly remember vast numbers of facts, execute flawless logical reasoning, execute near-optical probabilistic reasoning, learn more rationally than humans, learn from vast amounts of data, extract information from millions of scientific papers. I argue that AI is a wonderful application area for science. Scientific problems are abstract, like games such as chess and goal, but science also involves the real world. Scientific problems are restricted in scope. By this I mean that to be goo dat science AI systems don't necessarily need to know about anything else but science. Another advantage is that nature is honest, not malicious. By this I mean that if a humanor a robot scientist does an experiment, we are confident that experimental results are not contrived by nature to mislead. This is not the case with many other applications of AI, such as in economics warfare, et cetera. Nature is also a worthy object of our study. In my view, it is a tragedy that given the many problems in the world which AI could potentially help, global warming, diseases, food shortages, et cetera, that so much AI talent is being used to develop more addictive advertising, to develop better weapons, et cetera. I also believe that the generation of scientific knowledge, as long as it is in the public domain, is a public good."
DSM100-2022-OCT,Topic 7: Robot Scientist,Lecture 2: The concept of Robot Scientist,https://learn.london.ac.uk/mod/page/view.php?id=96363&forceview=1,"[music]In this mini-lecture, we will introduce the concept of a robot scientist, an AI system for scientific discovery. We will discuss why such systems are required, what was the motivation for the development, the advantages, and disadvantages. We will consider the architecture and basic components of a robot scientist system. What is a robot scientist? The concept has been introduced in 2004 by Professor Ross King, currently working at Chalmers University of Technology in Sweden. He and his colleagues have demonstrated that it is possible to automate the scientific process. They reported on the successful development of a system capable of carrying out cycles of scientific experimentation. The system was named Adam, a discovery machine. It is when I joined the Robot Scientist Project and I continued to be involved in it ever since. Robot scientist is an AI system capable of originating its own experiments, physically executing them, interpreting the results, and then repeating the cycle. The system relies on a knowledge base that encodes knowledge and data about a particular domain. Robot scientists can infer from its knowledge base scientific hypothesis, something new, what is not already encoded in the knowledge base. These hypotheses are not necessarily true. There are possible explanations of the available data and phenomena to establish if the inferred explanations are correct. The system designs experiments to test those hypotheses. Robot scientist is a physically implemented system that includes robotic equipment for testing generated hypotheses. It can carry out many experiments in parallel and then analyze the results. The analysis component makes a decision if the hypotheses were confirmed or not. It then updates its knowledge base with new experimental data and conclusions. Then, the cycle can be repeated. New hypotheses formulated and updated knowledge basecan be used to generate new experiment sand execute them, results are analyzed, and so on, thousands of times if necessary. The concept of a robot scientist is a technological implementation of the hypothetical deductive method of scientific discovery. According to Encyclopedia Britannica, it is a procedure for the construction of a scientific theory that will account for results obtained through direct observation and experimentation, and that will, through inference, predict further effects that can then be verified or disapproved by empirical evidence derived from other experiments. Not all sciences include experimentation, but many do. Computer science involves computational experiments. Social sciences involve surveys and observational studies. Of course, not all experiments can be automated. The research field of scientific discovery is still in its infancy, but it has an immense potential for speeding up the technological progress, and ultimately, improving the quality of our lives. Why do we need robot scientists? Don't we have enough of scientists already? Firstly, it is important for our understanding of what science is. As Richard Feynman, a Nobel Prize winner famously wrote, ""What I cannot create, I do not understand. ""If we can create a machine that is doing science, only then we can truly understand what science is. Secondly, it is necessary for the technological advance. The production of data significantly outperforms the production of knowledge. We do not have enough resources to analyze all the data we are producing to decipher useful knowledge from the vast amount of data we're producing every minute. Robot scientists have the potential to increase the productivity of science. They can work cheaper, faster, more accurately, and longer than humans. One of the positive aspects of robot scientist sis that they can record and share all steps of scientific discovery. Humans are reluctant to do so, and it requires time and effort. Making science openis important from ethical and legal perspectives, and we do not have enough scientists to tackle the challenges of the modern world. It takes several years to educate a new scientist. Robot scientists can be easily multiplied. A robot scientist can be built in a couple of months. They're still expensive because they're not mass-produced. The estimate is that it should cost as an ordinary car. Human scientists no longer can read all the scientific articles, check all the results, and keep in mind all the information about complex systems. They're good in deep understanding of phenomena, making conclusions from small data, leaps of imagination, but they need help in processing big data and making unbiased, accurate, logical inferences. In this mini-lecture, we discussed the concept of a robot scientist, why we need them, and what they're capable of. Robot scientists offer a great technological advance but their application is limited to certain scientific areas amenable to automation."
DSM100-2022-OCT,Topic 7: Robot Scientist,Lecture 3: History of Robot Scientists and future trends,https://learn.london.ac.uk/mod/page/view.php?id=96367&forceview=1,"In this mini-lecture, we will talk about history of automated scientific discovery. Many believe that it's not possible to automate scientific discovery. However, the history proves them wrong. We will see examples of fascinating, autonomous discoveries. A common view in the philosophy of science is that only humans can make discoveries, that the discovery is not logical, and therefore, can't be formalized and automated. ""The initial stage, the act of conceiving or inventing a theory seems to me neither to call for logical analysis nor to be susceptible of it. My view may be expressed by saying that every discovery contains an irrational element or a creative intuition. ""Karl Popper, 1961. Yet, advances in AI proved it to be wrong. Dendral is a pioneering systemin the research area of automated scientific discovery. It was developed in the 1960s at Stanford University by Edward Feigenbaum, Bruce Buchanan, Joshua Lederberg, and Carl Djerassi. All are outstanding scientists. The primary aim was to study hypothesis formation and discovery in science. The chosen task was to help organic chemist sin identifying unknown organic molecules by analyzing their mass spectra and using knowledge of chemistry. The motivation for the project was the exploration by robots of new planets. Such robots will need to take samples of organic matter, analyze them without directions from humans. The Dendral Project consists of research on two main programs, Heuristic Dendral and Meta-Dendral. Heuristic Dendral is a performance system and Meta-Dendral is a learning system. Many systems were derived from Dendral including MYCIN, for identification of bacteria causing severe infections and for recommending suitable antibiotics. Another example is Bacon. It is an AI system that rediscovered numerical laws. It was developed by Langley, Simon, Bradshaw, and Zytkow in the '80s. Bacon carried out search through a problem space of algebraic terms to generate new terms from old ones. This led Bacon toward higher-level term sand a rediscovery of Kepler's third law of planetary motion. Bacon reconstructed a variety of numeric relations from the history of physics and chemistry. There are many interesting discovery stories in AI research. To name a few, following Pat Langley, Reaction Pathways in Catalytic Chemistry by Valdez-Perez, Qualitative Chemical Factors in Mutagenesis by King and Srinivasan, Quantitative Laws of Metallic Behavior by Mitchell et al, Qualitative Conjecture sin Number Theory by Colton, Bundy, and Walsh, Temporal Laws of Ecological Behavior by Todorovski, Džeroski, and Kompare, Models of Gene-influenced Metabolism in Yeast by Kim et al. I have been working in this area for 20 years and I'm privileged to personally know many of these brilliant scientists. Ashwin Snirivasan, Alan Bundy, Ljupco Todorovski, Sašo Džeroski. I've worked with Džeroski on the robot scientist project since 2004, and I would like to tell you more about the history of this fascinating project. The history of the robot scientist project dates more than two decades back. The first initial work started in 1999. The initial system had limited hardware and could only rediscover original functional genomics knowledge. It was an important proof of concept work. In the next stage of the project, a robot scientist Adam has been constructed. It was a sophisticated automated lab system designed to discover new knowledge. The application domain was yeast functional genomics. Yeast is a simple model organism that has being intensively studied by biologists because it shares many gene functions with humans, and it is easier to experiment with yeast than with humans. In yeast, many functions of genes remain unknown. The task was to determine gene functions by growth experiments. The background knowledge, yeast metabolic model was encode din logic, so Adam could process and reason over it. Adam also had access to bio informatics databases. Hypothesis generation was done with the use of abductive reasoning and bio informatics techniques. Experiments were designed to use active machine learning and we employed machine learning tools. Specifically Random Forest to analyze experimental result sand make conclusions if the generated hypothesis were confirmed or not. On this slide, you can see a diagram of Adam. It occupies the space of a small room. You can see Adam at work following the provided link as a YouTube video. Adam generated and confirmed the novel functional genomic hypotheses. Its conclusions have been also verified manually. Adam was the first machine to autonomously discover novel scientific knowledge, hypothesize, and experimentally confirm it. Times Magazine named Adamas the fourth most significant scientific advance of 2009. The next robot scientist was named Eve. Eve worked on drug design for tropical diseases. Millions of people die of these disease sand hundreds of millions of people suffer infection. It is really hard to cure these diseases, kill the parasites, yet they are neglected by the pharmaceutical industry. That is why robot scientists are of particular assistance in finding new drugs for tropical diseases. Background knowledge was encoded as graphs. Induction was used to generate novel hypothesis. Experiments were designed with the use of active learning and econometric model. One of the most interesting discoveries by Eve was repositioning of triclosan for malaria. At a position, it means that the chemical has been already approved as a drug against a certain disease. Then it was shown that it is also active against a completely different disease. Triclosan is used in toothpaste, so it is known to be safe. It is a simple compound and easy to produce. What is most remarkable is that it is resistant to several variants of malaria. The next stage of the robot scientist project was Ada Lab. It was a European project with several universities from France and Belgium. The application domain was systems biology. Even simple model biological systems like that of yeast are incredibly complicated. Thousands of genes, proteins, small molecules, are interacting together in complicated spatial-temporal ways. There are not enough specialists in the world to disentangle these system sand such models are beyond human intuitive understanding. It is where robot scientists can help. The main focus of the Ada Lab project was on closing the loop and repeating the cycles of scientific discovery on making the integration of machine learning tools and robotic equipment seamless. The project also was looking at how human and robot scientists can collaborate on scientific tasks. Ada Lab produced mainly machine learning tools and models. Please refer to our open access publications that lists and describes them. Ada Lab carried out three closed-loop cycles of experimentation. As a result, substantial amount of knowledge has been discovered and added in the form of an executable systems biology model. Ada Lab results are of importance for cancer and aging research. However, the development of high fidelity systems biology models require hundreds maybe thousands of closed-loop experimentations not just three. [?] of the next stage of the robot scientist projects, Genesis. Genesis has been designed and currently being assemble din the Chalmers University of Technology in Sweden. Genesis hardware will be able to execute 10. 000 closed-loop cycles of experimentation in parallel. This is equivalent to 1000 biologists working in one lab. Genesis AI will be able to automatically design, plan, and execute thousands of experiment sand improve a very complex systems biology model. There are other robot scientists in the world. For example, a mobile Robotic Chemist recently developed by Burger et al. in Liverpool is automating the research rather than the instruments. It can operate autonomously or for several days performing hundreds of experiments within a complex experimental space. You can watch it at work [?] provided link to the YouTube video. Another example is the Automatic Statistician. This project aims to automate data science, producing prediction sand human readability reports from all datasets. There are more and more automated discovery systems are appearing in different domains. I would like to conclude this mini-lecture by telling you about the Turing Nobel Prize challenge proposed by Professor Kitano from Japan. By 2050, develop AI systems that can make major scientific discoveries that worth Nobel Prize. Several of my collaborators and myself were at the meeting setting up the agenda for this challenge, and I hope to participate in the development of such an AI system. In this mini-lecture, we looked at the history of automated scientific discovery. The history dates back more than half a century with first systems developed in 1960s. Since then, a steady progress has been made. It is true that the discoveries are still on a rather modest scale, however, theory is flourishing. The target is to develop systems that are capable of significant scientific discoveries."
DSM100-2022-OCT,Topic 7: Robot Scientist,Lecture 4: Legal and ethical issues in robotics and lab automation,https://learn.london.ac.uk/mod/page/view.php?id=96371&forceview=1,"[music]In this mini lecture, we will consider important ethical and legal issues. We will not cover the whole area of AI, instead, we will be focusing on ethical and legal issues that are most relevant to the considered case study, the Robot Scientist, and a little bitwhy the laboratory automation. We developers of search system have to observe such issues. We have to remember about it and we are, ultimately, responsible for developing ethical AI systems, and we're also responsible for how such systems are used. Everyone working in this area have to be aware of all surrounding issue sand problems, and try to mitigate whenever possible. There is a whole area of research, now is called Roboethics. It covers ethical issues related to robot design, operation, and use. The focus on answering such interesting and important questions like, ""Can we design robots to act ethically?How people should treat robots and how should robots treat people?""""Should robots have rights?Who or what is responsible if a robot or other automated system causes harm?Are there any risk to create emotional bonds with robots?Which type of ethical codes is correct for robots and automated systems, and so on?""There are many questions and issues and we will not cover them. We will not answer all questions since that will we'll consider only few issues that are most relevant to the Robot Scientist Project, our case study. The Robot Scientist Project has been frequently criticized for replacing human scientists, and more generally, yes, it's a serious problems that more and more robots performing jobs that are typically performed by humans. The whole job market is distorted and the tendencies firmly set, so it's unlikely to change. On the slide, I provided several links to YouTube videos, where you can watch how robots are performing jobs that we typically associate with humans. Amazon robots move products for packaging and sending to customers. Robots are making drinks in a bar, and soon, you will be able to buy a robotas a personal chef for your kitchen. In laboratory automation specifically, an issue is that lab automated system are replacing lab technicians and threatening jobs of scientists. I'm working in this area many years, and I can confidently tell that in this particular area, it's not a big issue. I don't claim that it's not issue for the whole AI. I do believe that it has negative impact. It also has positive impact, new jobs are being created. Particularly laboratory automation, the true goalis to automate repetitive tasks or tasks that must be performed in environments that are dangerous to humans. I would like to give only one example. During the pandemic, many scientific labs were shut down because it was dangerous for humans to travel and to be there. In many labs, it's not possible to observe social distancing. The best solution is to build a remote autonomous laboratories that can be directed by humans remotely by phone or emails. Automated robotic parts of this lab can carry out experiments autonomously with minimum or maybe even no directions from humans. Moreover, it is important to understand limitations and advantages of both AI systems and also humans. AI systems are better in processing large volumes of data, reasoning, and in parallel tasking. AI system can walk nonstop, 24 hours, seven days a week. They don't need holidays, don't have headache. Nevertheless, humans are better in higher-level planing, making decisions in uncertain, nondeterministic, or partially observable environments. I firmly believe that the target should be human-robot collaboration and co-creation. It is known that the AI system can beat the best human chess player. Interestingly, a team of AI system and humans can play better chess than a team of AI systems or a team of humans. I believe that the same would apply to scientific research and many other areas. Another issue I would like to highlightis when AI systems exhibit some bias and also makes mistakes. It's important to remember that AI systems can be biased. They have less bias, but it can be inbuilt by design by humans. For example, it is well known that medical data sets are biased simply because not enough data collected from female patients or from some minority groups. As a result, AI systems that uses this biased data set scan make biased decisions based on these datasets. The best mitigation is, first, to be aware of that. Try to compensate for bias and data sets, for example, bias stratification, or at least, fairly accurately recording everything what is done. Decisions can be checked, what AI decided and how, and then decision can be made if to trust this decision or not. If it's biased decision or not. AI system can make mistakes. There are technical systems, it can go wrong. For example, the now project and the Robot Scientist Project, all experiments are repeated many times to ensure that there is consistency in the results because if one something goes wrong, then other experiments will show different results. Only when results are consistent, only then they have been used for further steps. Legal issues. Existing laws have not been developed with AI in mind. Again, I will give only one examples that highlights this problem. The Copyright Law didn't permit us to include Robot Scientists, Adam, as a cause to the paper reporting the discovery made by Adam. We published this in one of the most prestigious journal sin science and the editor explained that we cannot include Adam as a cause because Adam cannot sign the copyright agreement. It doesn't sound right, but this is how it is. A bigger question is, ""Should robots have rights?""There is no definite answer, not yet to these and to many other questions, but this area attracteda lot of attention. There is a lot of research in this area and, hopefully, substantial progress will be made fairly soon. I would like to talk about one more issue, it's issue of safety. AI systems must be designed in a way that they do not pose any threat to humans. Again, I will focus on our case study, the Robot Scientist Project. The Robot Scientist Adam was placed inside in enclosure. It was even not possible to approach it. The Robot Scientist Eve didn't have an enclosure, but it had sensors. If someone would approach, any dangerous piece of equipment, the system would automatically shut down. In our case, it's very unlikely that this system can cause real physical harm. Of course, there is a possibility that they may make predictions or like for application of cancers, they can make wrong recommendations for drug to use, but that is, again, accountability and checking how a decision was made. Otherwise, I believe that the area of lab automationis less proneto these ethical and legal problems. However, I do recognize that why that in AI research there are many ethical and legal issues that need our attention. There is a good steady progress is madein answering these important questions."
DSM100-2022-OCT,Topic 7: Robot Scientist,Lecture 5: Overview of the toy system: components and their interactions,https://learn.london.ac.uk/mod/page/view.php?id=96375&forceview=1,"[music]In this mini lecture, we will look at a toy robot scientist. The robot scientist, AI system, is highly complex, perhaps the most complex system in existence given its advanced AI components, integrated with machine learning tool sand also automated laboratory equipment. Our toy robot scientist will help you to better understand the inside workings of the main system. Firstly, I will remind you about the main components of the robot scientist. The concept of the robot scientist reflects the typical cycle of scientific discovery. To automate the discovery process, an artificial scientist needs background knowledge encoded in machine processable form, an inference engine to formulate new hypothesis, an inference engine to design experiments to test those hypothesis, a real physical system to execute those experiments, a data analytics component to decide if hypothesis were confirmed or not. New experimental data in conclusions about the hypothesis can be added to the background knowledge base, and the cycle can be repeated again and again till a target is achieved, or the system runs out of resources. Each of those components is a complex system, which may have sub components that are also far from being simple. Moreover, this components are not located in the same place. Machine learning software runs in France as the lab system is in Sweden. Each of these components and subcomponentscan be viewed as an agent, and the robot scientist system can be viewed as a multi-agent distributed system. Our toy robot scientist is designed to assist in drug discovery. It can recommend what chemical compounds are likely to be drugs. Our toy is a multi-agent system that can carry out the most essential parts of the discovery process. It has knowledge agent that can accessa data set and update its knowledge model. Hypothesis generation agent. Hypotheses are generated from the background knowledge about drug design and available chemical data. Hypotheses are only guesses and they need to be tested. A planning agent designs experiments to test the formulated hypothesis. It needs to know not only what to test, but also what material sand equipment are available. It passes an experimental protocol to the lab agent, a sequence of actions with instructions about devices, parameters, timings, and so on. The robotic lab agent follows the protocol, runs experiments, and records observations, experimental results. The robotic lab passes the experimental datato the results interpretation agent. This agent can decide if the hypothesis will conform or not. New data and new knowledge about confirmed or rejected hypothesis are added to the knowledge base and data sets. The new hypothesis can be formed, and so on. Our toy robot scientist has only a few agents to enable them the main discovery process. It can be enhanced by many other agents. For example, a meta-analysis agent, a collaboration agent, a monitoring agent, an external services agent, a model agent, and an explanation agent. A meta-analysis agent can collect information about all previous rounds, results, and compare performance of other agents. In what situations they work best?For example, does the planning agent design good enough experiments?Can the results interpretation agent consistently make decisions based on the produced observations?If not, what needs to be changed?This is a highly intelligent behavior. Sophisticated reasoning over an elaborate knowledge baseis required to implement such an agent. A collaboration agent can be added to support a team of robot scientists, or a team of human and robot scientists. It is like a next level of a multi-agent system, a team of multi-agent systems. This will require orchestration of the goals, knowledge basis, complex scheduling of experiments, sharing outputs, and of course, they can be communication problems. Having a monitoring agent is always a good idea. Such an agent can monitor and record everything what is happening. When hypothesis were passed to the planning agent, how long did it take to produce experimental protocols?Is everything all right in the lab?Are there enough consumables for the next bunch of experiments?This information is essential for learning from the whole processfor the meta-analysis. It is also pivotal for transparency and accountability of AI systems. For sharing of also data and knowledge. An external services agent can be an interface for external users. For example, they can be permitted to submit their hypothesis to testby the automated lab agent. External users will receive not only experimental data, but also conclusions made by the results interpretation agent. This is the vision of a lab automation cloud. It doesn't matter to external users where or on what equipment the hypothesis were tested. If they have accurate, well-annotated experimental result sand conclusions, then they will be happy to use the service. A model agent can substantially enhance the intelligence of the robot scientist system. It can work not only with data and rules, but also with executable computational models, run simulations, and enhance production of interesting hypothesis, and also interpretation of the results. For example, if there is a suitable computational model about mechanism of a particular disease, then this knowledge can be used to formulate hypothesis and more targeted selection of chemical compounds to test. An explanation agent can explain to users why and how these conclusions were made, and not answers. It can explain to external AI developers, how the system components work, and to biologists and chemists, what experiments are run and why. We can imagine many other agents that can enhance our toy. In the real world, some of such agents exist, and some are not. For example, there are no communication agents to support collaboration of a team of scientists. The work on producing explanations is still in its infancy. In this mini-lecture, we talked about the toy robot scientist, what agents it has and what agents might enhance it further. Our toy robot scientist work on drug design problems, and it requires a knowledge base about drug design, and drug design datasets as an input. It can formulate hypothesis about what chemical compounds may be the potential drugs, design experiments to test this hypothesis, run experiments and analyze the results."
DSM100-2022-OCT,Topic 7: Robot Scientist,Lecture 6: Modelling a toy domain,https://learn.london.ac.uk/mod/page/view.php?id=96379&forceview=1,"[music]In this mini-lecture, we will look at a toy knowledge base and a toy datasetfor our toy robot scientist. I will briefly introduce you to the domain of drug discovery. We will discuss the ways of representing domain knowledge and the advantages and disadvantages. Our toy robot scientist is designed to assist in drug discovery. The drug discovery is a long process. It usually takes on average 10 years to deliver a new drug to a market, and it can cost billions. It usually starts from a library screening. A library is a collection of chemical compounds. Pharmaceutical companies can have millions of chemical compound sin their storage rooms. Screening is running experiments or assays with chemical compounds in a lab. Chemicals are added to biological culture to see if there are any signs of biological activity. If [?] a chemical has a desirable effect. This is how drugs are working in our body. They are affecting our biological processes. The select compounds for screening and to prepare biological samples are challenging tasks. If experiments show that the compound is biologically active, then it is investigated further. Such compounds are called hits. The next step is a hit confirmation. It can be for example, [?] identified hits at different concentration levels. Confirmed hits are called leads, and they have a chance to be an approved drug. Of course, many more stages are required for that, including clinical trials. Our toy robot scientist is focusing on identifying leads. If the initial stages of drug design from a compound to a lead can be automated, then that would speed up the whole drug discovery process. It will save money, and ultimately, it will save lives by bringing drugs to the market earlier. How can the data knowledge about chemical compounds be represented?Each compound is a molecule. If you remember chemistry, then you know that the structure of a molecule is formed by atoms and chemical bonds between them. On this slide, you see an example of molecular structure. This chemical compound is Aceclidine. It is a drug that is used as eye drops to treat glaucoma. A molecular structure can be represented in many ways. For example, as an image, a molecular formula, In Chi string, In Chi key, Canonical SMILES, et cetera. I will not explain what In Chi and SMILES mean, but they're commonly used at representations. The most common representation is a table because it is amenable to machine learning, even if it is not the most native or elegant representation of a molecule structure. On this slide, you see a toy dataset in the form of a table. Compounds are represented. They are properties of molecules and molecular structure, including molecular weight. It is measured in daltons. Hydrophobicity is the physical property of a molecule related to water solubility. Hydrophobic compounds do not dissolve well in water, like oils or fats. Hydrogen donor is a compound that gives up or transfers a hydrogen atomto another substance. Hydrogen acceptor is a compound that accepts a hydrogen atom from another substance. Heavy atoms refer to any atoms that is not hydrogen. A compound has a ring if its atoms are in a ring or cyclic configuration. We want to go beyond the table representation to include the full power of logic, especially inductive reasoning. To enable that, we need to represent domain knowledge and data in logic. First, we will design a conceptual model of our domain. What we know is there are chemical compound sand they have several properties. This is represented in the diagram in blue. This is what we know from the table. However, we also know more about drug design. We know that there are already approved drug sand there are toxins, and toxins can't be drugs. Well, because they are toxic. We also know a little bit about our biology. For example, small molecules with a low molecular weigh tare more likely to have an effect when administered to our body. Large molecules are unlikely to go through our cell membranes. Such knowledge is not in the table. If there is enough suitable data, then machine learning can discover this knowledge, but we don't need machine learning for that. It is common domain knowledge that this would make sense. Anyway, common ground knowledge or results of machine learning, we want to add it to our knowledge model. This highlights one of the principal limitations of conventional machine learning. It may be tricky to add new information to improve the learning unless it is in the same form and as the remaining information. In our case, we can easily add more lines to the table. Adding information is trickier. We can add columns with the classification drug, toxin, unknown and small, medium, large molecule. Then it's still not immediately clear that the small molecule has a high chance to be a drug than a large molecule. Our toy knowledge domain is tiny, and perhaps we can express most of itin the form of a table or as a suitable for machine learning representation. Imagine a real complex domain with many hierarchical layers, complex connections, assumptions, and restrictions. Most of such knowledge will be missed by conventional machine learning approaches. We can represent our domain knowledge in several ways using the skillset of this module. Encode it in Python. This is tricky. Python is not designed for knowledge representation, but that is possible, and I will show you how to do that. Encode in RDF and export it to Python. This is better because RDF will dictate a good structure for our data and knowledge. Or we can use a dedicated tool, Protégé to encode it in RDF, and then export to Python. This is good because Protégé has inbuilt reasoners that can checkour representation for logical consistency. Moreover, a reasoned can infer missing facts. We also can encode it in First Order Logic in Prologand use Prolog Inference Engine. This is also good but it does require knowing Prolog. Prolog also can take an RDF representation as an input. In this mini-lecture, we talked about the toy domain knowledge for the toy robot scientist. Our toy domain is simple and can be represented as a table or in a more amenable for reason and form. More complex domain knowledge is harder to represent as a table or even as a database. Logic is more suitable for capturing our knowledge and reasoning [?]."
DSM100-2022-OCT,Topic 7: Robot Scientist,Lecture 7: Modelling a toy domain (demo),https://learn.london.ac.uk/mod/page/view.php?id=96380&forceview=1,"[music]In this mini lecture, we will look inside a Knowledge Agent. This agent contains knowledge about our domain of drug design. It's an important agent because other agents can communicate with this agent and consume this information. In this mini lecture, we will look at how to encode our conceptual knowledge models that we already developed in a machine process able format. We will use a tool named ""Protégé"" to do that. Protégé was developed in Stanford University and it's a popular tool. It is good because it enforce in you to follow clean, logical structure. It is easy to install and tutorial is available. People use the very simple operations and they're enough to encode the knowledge model. In our case, we will do it finally in RDF format, Resource Description Framework and serialization Turtle because it's more readable for humans. I will show you how to do it. This is an interface of Protégéand the main tabs are entities. You can press, you will see more types. What we will use is classes, individuals. These are nodes in our knowledge model. Classes it's more generic types, like chemical entity, individuals is concrete chemical, so instances of these classes. There are also links between nodes as they can be between classes, then it's object properties or between individuals instance sand classes, then its data properties. Okay, so you already see one class is defined. It thinks, it's anything. This is our root node, so the very top of our tree. We will grow tree like logical structure, is a hierarchy and that is what can be used for reasoning. You also see that there isan automatically generated UDI. We can change it. We can use different UDI. We can register, put it somewhere, but we'll keep it for now. Now, when I want to add classes, I just press this button and it will go to the next level. I want to add chemical entity. Okay. We have next level in our hierarchy and you can see that Protégé generates this Unique Resource Identifier for each class. It just add in label and this is how it ensures that everything, what is defined here, has unique resource identifier, and that is important. Okay, now we will grow our tree further. We want to tell that some chemicals are drugs, and some chemicals are toxins. They're on the same level, so we will use these buttons. You can see pictograms, quite informative. Toxin. Okay, and again, you see it generates this UDIs for toxin and drug. If we don't know what chemical compound is, we can keep it just like chemical entity, or if we know if it is drug, we can add instances as drug, or even now toxin, we can now add instances as toxin. Let's add one instance. We will add morphine, and morphine is a well known drug. Yes, so we go to individuals, the class drug is highlighted and we can add instance or we will add, the same logic, you just press the button is plus, and it adds your new entities, morphine. We have this instance. Now, if we go back to class view drug and we can see, at least, what instances it has. We can add, of course, much more. Now, what we will do, we will do agent links. We have six properties of chemical entities. I will show you how to add it. You can use subclass, of, and data restriction, create. You can use properties to link it, but first, we need to define this property. Let's go, data property. The same logic, you already have a top data property and you can add what you need. Let's add molecular weight. Okay, we have this property. Now, we can link. We can link chemical entity. We can say that every chemical entity has a molecular weight, subclass of data restriction. Now, we can say that we want this link and we also can say what is the data is, so we can do it later. Okay. Now, what is happening? What is interesting?This logic allow us to propagate these properties. We defined it for chemical entity and now we see that all other subclasses inherited this property. Drugs, so they all have this property molecular weight. Good. Now, let us say that morphine has this concrete weight. We can add this, it's also molecular weight and we can put those. The weight is 285 [?]. Okay, so we have molecular weight and, of course, we can add more instances. I will not do it right now. What I want to show is a set of nodes that a toxin cannot be drug, not just because it is toxic. We can say it explicitly and quite easily using these disjoint ways, so we can put, ""The toxin is disjoint with drug"". Okay, and if we go to drugs that it's automatically already and further it's also disjoint with toxins. Now, I will show you how to use Reasoner. Protégé has inbuilt Reasoners. In this case it's Fact++ and HermiT. We can use any of it. We can start Reasoner and it will check if our logical structure's clean and nice, if there are no errors. Of course, we have only three classes. One instance, it's easy for us to maintain logical consistency. If there are more and if you're aiding more restrictions or this properties, they start to build complexity, then it's extremely difficult to make sure that logical structure is clean. It is essential to use a Reasoner to check that everything is all right. If something is wrong, then you will see it's in red. In our case, it looks fine and this asserted, it is what we--Invert what Reasoner can invert and that is fine. Asserted, so everything is well. Just to show you how it looks when things are not right, I will put here that the chemicalis disjoint with toxins. We may assume that if it's toxin, we just don't want to consider any of these chemicals. Yes, let's put it like that. Let's run Reasoner now. It doesn't run. Start Reasoner. Toxin disjoint as chemical entity. Start Reasoner. Yes, so you see that something is wrong because something appears in red. This system is not happy about it, so it shows that something is not right. It's possible to see explanation, has generated explanation, what is inconsistent with what, but we will not go into it. Let's fix it, let's remove it. We don't want this being disjoint. Stop Reasoner and start again. Now, everything is fine. One more things that are here, I want to show you is a defined class. We want to capture knowledge that if molecule is small enough that it's more likely to be a drug. We can add here concept of a small molecule. What is a small molecule when molecular weightis below a certain weight?Let's add the class, small molecule. Let's make it defined class. Defined class, it means that we explicitly define conditions, so what is this class?We will use class expression editor. What we will want to say is that any chemical that hasa molecular weight below certain number can be considered as a small molecule. We need the chemical entity. While it is in red, so instruction is wrong, so we need to continue before the editor can understand it. Now, we want to put this restrictions, 'molecular weight'. If you remember when we defined the options, some, exactly, so I just mimic from that. If I remember correctly, it was rational. We just put below certain number. Still not happy. Data type, rational. Let's put 'integer'. I think it's not happy because it is how we defined the property. Let's try to fix it then later the property here. Let's go to individuals, drug, morphine. Yes, it is integer here, molecular but integer. Okay. Fine. Chemical entity. I just thought you could. Here, rational, so let's check. I could select it immediately, but I was too lazy. Now, I cannot spell it. Integer. Now, let's try again. We want to put this class expression editor. It still doesn't want. Chemical entity and molecular weights. Oh, it wanted the-- Okay, so, yes, you need to know how to do it, but I just will show you once and that's it, you don't need it. Any chemical entity with a weight less than 800. Okay, I put it in the wrong place. Small molecular. I added that restriction to small molecules. Equivalent to class expression editor. Now, we can put this class expression editor. Yes, and it comes in here. Now, you can see this symbol, equal, so it's 'small molecule'. What is happening now?If we're on a Reasoner, Stop Reasoner. Let's use this Reasoner. Start Reasoner. It inferred that Morphine is a small molecule because it weighed below 800. If we go to individuals, it shows. Morphine, it's a drug, but it's also a small molecule. This is how Protégé can check your logical structures, that its claims there are no contradictions, no unexpected, what is called Entitlement. Also, it can help with the inference of new facts. We didn't put explicitly that Morphine is a small molecule, but if a class small molecule is defined like that, deductive Reasoner can pick up and classify. This is classification. Our instances can be classified and you can imagine many rules, but we will not do it. I just wanted to show you this deductive reason and classification. What we will do now, we will save this file. 'File Save As'. We can select the input format we would like. We will use Total Syntax because it's easier for humans to read this. Just okay. It will give you an option where to save it and also a name, so we will put this. I already have a folder, total, but scientists but you can select 'Save'. I would like to show you how to capture such knowledge that some molecules are small enough to go through our membrane. They are called small molecules and they are more likely to be drugs. What we want to do, we want to add the class. We will make it a defined class. Class that has the precise condition when instances will be considered as such. In our case, we want to say that every chemical entities that has molecular weight below certain number can be considereda small molecule. Let's do that. We will add this class small molecule and we don't make it a different class. To do that, we use class Expression Editor and I already prepared it at home. What it says that any chemical entity, a just logical expression and molecular weight below certain number, then that is that class. You see the pictogram for the class changed. It's now defined class and what is happening now?If we start Reasoner, you can see it inferred that morphineis a small molecule because it has small enough molecular weight. This is called Classification. Reasoner can do deductive reasoning and classify instances into defined classes. Again, it's a very simple knowledge model, but you can emerge in many rules defined classes and then your supply instance, you just put it on this level or whatever your knowledge model is, and then the system can classify these instances following these logical conditions. Wonderful. Now, we want to, of course, we can add all other properties and all other instances, but I will not do it. It's just the demos with the same operations. I will show you how to save this file now in the format that we want. You just go File Save as usual. We want Save As. You can select many options. We will use Turtle because it's more readable. It gives you an option where to put it. Let's go with the best knowledge base and where we want to do. I have forwarded to a robot scientist, let's put it there. Now, if we go to the folder to robot scientists, yes, test, knowledge base. You see that extension is actually all still. I would recommend you to change it just for future uses because, yes, we want to use it. Because we will consume this knowledge model using Python, so other agents that will be encoded in python can work with this. Let's just look on site. Let's open it with text editor. This is what was generated. Just very briefly, we will look at it to all sort of prefixes, what stands for what, so it's all based on this UDIs. It starts from this UDI that was generated for ontology, so we just can put different here what we want, but we will keep it. It immediately says, this type so that this is an ontology, so effectively what we created. We created a knowledge model in the form of an ontology. Then we have data property. We put only one data property, so molecular with RDF type. It's a data property. Yes, this is what we wanted, classes. Then for each class, we have name, type that it's class, and it shows what subclasses it ha sand so on, drug. Here, our different class small molecule. Again, it's all here, all this maximum, minimum, so it can be far more complicated rules that we added. Now, individuals, there is only one individual to morphine, so this all generated. This is how it looks like. This is what the machine learning tools can consume, if they can work with this, the python also can consume it. In this mini-lecture, we looked at how to translate the conceptual knowledge model into machine processable format. Using Protégé, we generated an RTF file in total serialization, and we added key nodes and key relations between our nodes."
DSM100-2022-OCT,Topic 7: Robot Scientist,Lecture 8: Knowledge agent (demo),https://learn.london.ac.uk/mod/page/view.php?id=96381&forceview=1,"[music]In this demo video, I will show you how to walk witha toy robot scientist in a visual studio. First of all, we will look at the main file, main Bison, and then we will talk about knowledge mod eland then knowledge agent. Main Bison file is the main entry point to all other files. Here we have console menu, it is a library that allows youto have some simple user-friendly interface. It is available as an open-source and it's not a full-fledge do graphic user interface but it's still good to have and support, and you see when you run it, it generates you key menu options that you can navigate using these options. If you don't have it, you can install using. PIP and then we also input OSS library. Now, OSS library, it is for working with bars. It just makes easier navigation to al lour other files like experimental protocol, experimental results, or our data outputs. It is very helpful to have, and also you can install it. We have several files that are essentially functions like knowledge model, abduction reasoning plan, and so on. Their functions and they can be called, sources files will be called. They will be used for these menu options. We create menu first, so these are our parts, and here we create menu options. We have five options and six exit and we create it here. Then, we can add these items to the menu, and we can initialize it. This is how interface is organized. Now we can call menu show. Now, if we press one, we initialize the knowledge agent. Let's go to model agent and see how it is all organized here. The knowledge agent when we press one, it initiates this knowledge and there's a script this and this file knowledge model. We will use RDFLib library that is specifically designed to walk with RDF files. We created our knowledge model or it can be somewhere externally. You can take any file, RDF file. This library enables you to process it in Bison. This is how agents can communicate and pass information between each other, so knowledge agent works with RDF representation, but it's possible to represent it in Bison too. It's okay to file the book directly here. In this demo, I would like to show you how RDF files can be parsed and this RDF file can be locally on your computer or it can be externally anywhere, just specifies the parse to it. RDForm library is quite rich library. It has rich functionality, for example, it enables SPAR QL queries over RDF Triplets. A SPAR QL, it's like SQL queries for databases but it is for RDF knowledge models, but we will not touch on it. Here, we'll use graph. This is the main interface that enables us to parse RDF files, and we also just go to simple visualization. Here, we just make our life easier by using OSS as specifying all our parse to data to RDFso where is it and also where is image is. Now, we initialize graphs or knowledge graph, for that, we call this graph function. Then what we door what we do we just parse our file. This is parse to our file, again, it can be localor it can be external and you can play with it, just put any valid RDF file and valid link to that file. Here, also format, so RDF cause many variants, it can be XML, it can be total, in here, it is XML. All over this happening, it's just loops through all these triplet sand then prints them. We can see that it is being processed, but also count how many triplets there and some image. Let's see, in our output so yes. Let's press something to come back, and let's press one again, so that you see, so yes, we got this image, is not dynamic, it is what is inside RDF file and it is like conceptual model. We have chemical compounds and we have six of them. We have two toxins, we have two drugs, and we have this defined cause and small molecule. Here, you see these properties molecular weight, and also define some other properties, but we didn't use them in our example. This is the image, once again, if you modify knowledge model so you can modify, you can add triples, remove triples, but it will not be reflected here. It's just an image generated by WebVOWL. Let's go back. That also shows that it has 74 entries, so this gives us confidence that it can process everything correctly. In a [?] demo, we looked inside the main file. We talked about how menu for our toy robot scientist is organized. These key options that corresponds to key agents, and we looked inside one agent, knowledge agent. This agent can process knowledge modelso models that create it somewhere else. Or, alternatively, it can probably just haveas these triplets encoded in by symbols. That is not so much advisable, so it's better to use something what is designed to work this knowledge models and then input to Bison. Or strictly follow the logic set was checked in some special phonology presentation tools. I hope now you can work with RDFLib library and you'll also find useful information about other libraries, like console menu and [?]."
DSM100-2022-OCT,Topic 7: Robot Scientist,Lecture 9: Modelling a real domain (demo),https://learn.london.ac.uk/mod/page/view.php?id=96383&forceview=1,"[music]In this mini-lecture, we will look at how data and knowledgeis encoded for real AI systems. I will briefly introduce you to several external data and knowledge representations relevant to the idea of drug design. I will then explain how we use that external source of information for the robot scientist. We have developed a very simple toy domain model. Our toy data set has chemical entities, then chemicals with six properties. We also encoded some knowledge that there are drugs and toxins, and that small molecules are more likely to be biologically active. You may have guessed that real sources are far more complex than that. First, we will look at KB. On my screen you see ontology of chemical entities of biological interest. If you look at summary, you can see that it has well over 150. 000 classes and it's also has different formats, including RDF. It all suitable for processing by our agents. We now will jump to classes. If you go classes and you can add for example, we will check what information it has about one of chemical compounds that is in our toy data set. I will type here morphine and so you just can follow, so I will read it, repeat it. Morphine has this ID 17303, and you see name morphine. You immediately can see useful information. It gives 12 synonyms and difference balance. For text mine application, this is extremely important and for many, many applications. Because biochemical entities can have so many different names, these IDs, unique resource identifiers are critical to be sure that the different resources actually referring to the same entity. Then we have definition, what it is, it's more [?] alkaloids and so on. I'm not a chemist, it doesn't tell me much, but for chemists and also for systems that are reasoning over this knowledge, that is important. We also see all these properties, all these InChI, there is information molecular weights. All properties are here. Now we will look at database cross references. There are plenty of external resources, ontologies and databases. Here some of them are listed that also have these entry about morphine. We will go to drug bank. Here is ID. I already put it here. Let's go to set entity. Yes, it's here. What we see here, we see again brand names, generic name. We have more background information. Drug bank is not an ontology, it is database, but it has a lot of useful information. You can see names in different languages. Mechanism explained, or usage or how it works, mechanism of action. There is even volume of distribution. Adverse effects, so toxicity and what pathways. Then you can see information on drug-drug interactions, with what other drugs it may interfere. Here you see more than 1, 000 entries. There is also food interactions, like avoid alcohol. Plenty, plenty of useful information that can be used for various purpose sand cogent reasoning. Now, we will use another cross reference. This set is the most relevant to usto case study of drug design is Campbell. Also it has morphine and Campbellit is created database that some humans really sit in and read in a lot of papers from this areas and say, extract information and they then say put it into Campbell database. It also has data. If we look what is about the morphine, Campbell contains information about more than two millions chemical compounds. About Morphine, yes, it is approve dafter maximum force stage of clinical trials. Again, synonyms, trade names, molecular type, small type. Yes, again, InChI, SMILES, alternative also you can see chemical structure. What is interesting here, what I would like to show you, you have these molecular features and you have here rule of fiveor so called the Lipinski rules. One of these rules is that, if a moleculeis a small molecules then it's more likely to be drug, and other rules. Composition of these five rules, it makes a chemical compound likely to be a drug. It not necessarily makes it a drug and there are plenty of drugs that breaks this rules, but it's still something useful to know. This encapsulates our knowledge about drug design. Here, it's immediately classified as small molecule. Campbell provides extremely valuable data on experiments with chemical compounds. We can see all these references to clinical trial sand you can access and see data. Quite a lot of them. In our project then, the robot scientist, we extensively use such resources so the robot produce its own drug design data and plus it access the external data sources including, from Campbell. You can imagine that putting data and knowledge from these different resources, even if they provide this ID, so you can gather, that they are talking about the same entity, but still there is so much diverse information and some information just textual. Putting it together, it's a difficult task. It's knowledge and data integration. We will not talk about it in this lecture but it is an important step for any real AI system to gather this data and combine together, and altogether represent in machine-processable form. To show you how it may look like, I will show you our Ada Lab ontology which is this Ada Lab project that worked with system's biology. We mostly imported classes from other sources of information, mostly about systems' biology, about biological entities, but we put it all together, all entities that we need for the project, all information that we need for the project, what robot scientist needs to reason over, and it is in very strange and logical format, so 460 classes. If you just can have a look. What we have here, so we use this upper-level ontology, it's called BFO Basic Formal Ontology because most of biological bio ontologistsare using it so it ensures interoperability. We can just have a look on some classes. Most of them, as I said, is imported. Yes, it takes time. Let's go to material entity, biochemical entity and interactions, so we worked only with some specific interactions between biochemical entities and we all listed them here. Let's just see. You see that this is imported from INO ontology and because definition, so most of classes are like that. All classes that we need for reasoning about the output in unified structure. This is how real knowledge models look like. They are mostly externally available representations. Some have strict logic behind them, some are not. This is the challenge in task to put together all available information and data set in logically consistent form, encoded in a forms that the AI system can reason with it. We looked at the drug bank, we looked at KB, and we looked on Campbell, we also looked at our own ontology that the models entities for the Ada Lab project."
DSM100-2022-OCT,Topic 7: Robot Scientist,Topic 7 summary,https://learn.london.ac.uk/mod/page/view.php?id=96384&forceview=1,"[music]In this topic, we started considering a case study of the robot scientist, a system that can discover new knowledge. We discussed the whole history of this area of research, automated scientific discovery, and relevant legal and ethical issues. We learned how to develop a conceptual knowledge mod eland how then to encode itin machine-processable format in RDF, resource description framework. Then, we learned how such RDF model scan be parsed in person. In the next topic, we will continue considering this case study, and we will talk about other agents like hypothesis generation and planning agent."
DSM100-2022-OCT,Topic 8: Robot Scientist part 2,Introduction to Topic 8,https://learn.london.ac.uk/mod/page/view.php?id=96388&forceview=1,"[music]Welcome to topic 8. In this topic, we will continue considering the key study of the robot scientist AI system. We will inspect how the core components of the robot scientist were designed and how they operate. We will closely look at the hypothesis generator, how abductive and deductive reasoning can be used to formulate new hypothesis. This hypothesis is then passed to the planning agent. The task of the planning agent is to design and experiment to test those hypotheses, and to produce an experimental protocol for the automated laboratory. The experimental protocol has to be precise. Thousands of experiments are carried out by following it. The experimental results are passed to the data analysis and interpretation agent to decide if the hypothesis will confirm or not. These components comprise the very core of the robot scientist system. This system also can include many other components like a communication agent to facilitate collaboration between human and robot scientist, and a meta-analysis agent to detect what configurations work best and why. To help you better understand this complex system, we will consider two examples of several agents. Good luck with this topic."
DSM100-2022-OCT,Topic 8: Robot Scientist part 2,Lecture 1: Hypotheses generation: abduction,https://learn.london.ac.uk/mod/page/view.php?id=96392&forceview=1,"-In this mini lecture, we will talk about how abduction can be used to generate a hypothesis. Abduction is a mechanism that can take us outside what is already known. It requires further testing and verification. But that is a mechanism to advance our knowledge. We will consider how the robot scientist, Adam, used abduction to formulate new hypothesis. Firstly, I will remind you what deduction and abduction are. In deduction, we have a rule, if P, then Q, and an assertion P. Therefore, we can deduce Q, if our rule and assertion are correct, then the conclusion is guaranteed to be correct too. In abduction, we have the same rule, if P, then Q, and an observation Q. Therefore, we can abduce P. P is a possible explanation of the observation Q. On this slide you can see examples of deductive and abductive reasoning. Rule: All swans are white. Fact: Bruce is a swan. Deduction: Bruce is white. Rule: All swans are white. Fact: Daffy is white. Abduction: Daffy is a swan. Unfortunately, both inferences, deductive and abductive, are incorrect. Indeed, before the discovery of Australia, people thought that all swans are white, but then, they discovered that swans can be black. It is critical to remember that our knowledge may be incorrect. There is also a lot what we don't know. New knowledge can be discovered and confirmed through observations and experimentation, and then our knowledge, in form of rules and facts, can be updated. Using updated knowledge, we can make new deductive and abductive conclusions. The robot scientist, Adam, used abduction to generate hypotheses. The application area of the robot scientist on themis functional genomics. Background knowledge is a yeast metabolic pathways model. It is a graph representing chemical reactions. The metabolic graph was encoded in prolog, in First Order Logic. Yeast has about 6, 000 genes and their functions are similar to those in humans. It is still unknown what genes are responsible for some of the reactions. It is possible that several genes are responsible for one reaction and one gene may be responsible for several reactions. On this slide, you see a fragment of a yeast metabolic graph. Nodes are chemicals, their IDs are in red, links are chemical reactions. These chemical reactions are controlled by genes. Controlling a chemical reaction is a gene function. It is how genes controls the production of the necessary for the cell elements. On the figure, genes are depicted in blue, starting with the letter Y. You can see that some reactions have several genes associated with them, and some links have none. For example, that are no genes specified for link between chorismateand Prephenate in the middle of the graph. A metabolic graph represents functioning of a yeast cell. For a cell to survive, it needs to receive and also to output certain elements. It can receive the necessary elements from the growth media, or it can synthesize them. The input to these metabolic pathways model isglycerate 2-phosphate, see the depicted in green on the top of the slide. That is glucose, food for the yeast cell. The cell must output tyrosine, phenylalanine, and tryptophan, see in green on the bottom of the slide. If a cell does not output this element, or doesn't receive food, it will die. If a cell doesn't receive enough food, and cannot output all the elements, its growth can be affected. How robot scientist formulate hypothesis about gene functions. For link between chorismate and Prephenate it considered possible candidate genes from the list of 6, 000 gene sand also their combinations. He tries to find genes that can control production of Prephenateand ultimately, biosynthesis of phenylalanine, tyrosine and tryptophan. The search space consisting of 6, 000 gene sand their combinations is huge. Adam has access to bio informatics databases, and he uses them to reduce the options by finding:What enzyme catalyzes the considered reaction?What genes are responsible for that catalyzer in other organisms?And, homologous genes in yeast; homologous means, that they are evolutionary similar. After abducing possible options, it inserts candidates to its logical model and runs reasoner to check if Prephenate can be synthesized. In the considered example, Adam abduced the gene YPR060C might be responsible for this reaction. You can see on this slide, the question mark is replaced by the name of the gene. It is only plausible explanation, an abduction, and not necessarily correct. It makes sense logically and computationally, but it has to be experimentally validated before it can beadded to these metabolic pathways model. Our knowledge is limited, not always accurate, and this is how it is updated:Through informed guesses and experimentation to confirm or disapprove them. In this mini lecture, we highlighted the difference between deductive and abductive reasoning. We discussed that our knowledge is incomplete;it has gaps and inaccuracies. It requires updating and filling the gaps. We inspected how the robot scientist, Adam, abduced hypothesis about yeast gene functions. Background knowledge was represented in logic, to formulate new hypothesis. and robot was able to reason over it"
DSM100-2022-OCT,Topic 8: Robot Scientist part 2,Lecture 2: Hypotheses generation: induction,https://learn.london.ac.uk/mod/page/view.php?id=96394&forceview=1,"[music]-In this mini-lecture, we will talk about how induction can be employed for hypothesis generation. Induction is a mechanism that enables us to generalize facts to formulate new rules. Inductive inference requires further testing and verification. We will consider how robot scientists can use induction to formulate new hypothesis. In induction, we have several facts. For example, Daffy is a swan and white. Tweety is a swan and white. We can induce a general rule. All swans are white. The rule is not guaranteed to be correct. A verification step is required. Even after more observations and experimentations, we still can't be completely sure that the new rule is correct unless we can inspect all the instances. Nevertheless, it is how we form our understanding about the rules that are around us. We observe, experiment, and generalize, and we tend to overgeneralize. Remember, our knowledge is not perfect. There is a lot we still don't know. Even well-established series like Newtonian mechanic scan be overturned by the theory of relativity. Machine learning is essentially based on inductive reasoning. It takes us input many data points, identifies patterns in the data, and then generalizes them to unseen new data. For example, if many three-bedroom houses in Brighton were sold for 300K to 350K, then a prediction in the near future of the three-bedroom house swill be sold for the same money makes sense. Such predictions are not necessarily correct as with any inductive reasoning but often, they are, because the world we live in is stable enough. It is possible to estimate the accuracy of predictions, but it all works only if nothing extraordinary happen like a real estate bubble. Inductive reasoning is widely used in biomedical research. One of the reasons for its success is that biology is remarkably conservative. Many gene functions are preserved across many species including humans. Many genetic factors that are correct for budding yea stare also correct for humans. Genes in different organisms are not exactly the same, but they are similar enough for inductive inference. For example, Gene X has function Y in cerevisiae, it's yeast. Gene X has function Y in C. elegance. Some generalization of these facts can be, Gene X has function Y in all organisms, and therefore, Gene X has function Y in Homo sapiens. Of course, it's not guaranteed to be correct and further experimentation is required. I will give you an example of SIRT genes, sirtuins. They have been shown to regulate organism al lifespan. Abundance of sirtuins has been first reported to increase lifespan in budding yeast, and then in several other model organisms, in nematodes and fruit flies. The reasoning behind this research is induction. If something is true for yeast, it may be true for other organisms. For example, Kim et al ran experiments in zebra fish, and discovered that the absence of longevity gene SIRT1 in zebrafishleads to oxidative injury, chronic inflammation, and reduced lifespan. Not surprisingly, scientists are investigating how sirtuins affecting longevity in humans?Robot scientists also use induction to formulate hypothesis about gene functions and potential drugs. In this mini-lecture, we discuss the use of induction for hypothesis generation. Inductive reasoning is probably the most widely used type of reasoning in scientific research. For example, in biology, hat is true for one organism, it can be true for another organism, including humans."
DSM100-2022-OCT,Topic 8: Robot Scientist part 2,Lecture 3: Hypotheses generation agent,https://learn.london.ac.uk/mod/page/view.php?id=96398&forceview=1,"-In this demo video, we will look at the reasoning agent. The reasoning agent implements abductive reasoning to generate hypothesis. This agent requires knowledge model. Every [?] toy robot scientist knows about the drug design. What it knows it's in knowledge agent it is represented and passed over that knows there are 10 chemicals. Two of them are drugs, two of them are toxins as they have certain properties like molecular weight. If this molecular weight is small enough, more precisely below 500 daltons, it's considered as a small molecule and then if it is a small molecule, then it's more likely to be a drug. The abduction is like some assumptions based on our knowledge. In this case, encoded in our knowledge models that if a chemical isa small molecule, then it's likely to be drug and it's worth testing. It may be true. It may be not true. It's possible to generate the same hypothesis by other means, for example, filtering database. The disadvantage is then first, the logic will be lost. The logic is abductions that explicitly dictate that conclusions are not necessarily correct and it's very important that if you just filter database, you may not take due care about it. Mostly important is that for complicated situations, a robot is very complicated. Many links, many connections, many inference steps. It's very difficult to do it by filtering or just machine learning some patterns. It's much easier and it more corresponds to how humans are thinking. It's just put in form of rules. What we know. If the answer that it's likely to be this is, if this, then maybe that. It can be done with probabilistic reasoning and it's fine, or it can be done with abductive reasoning. This is what our toy reasoning agent does. It needs knowledge as input and it will output hypothesis. In my file, it's option two. It is how we can initiate our knowledge agent. Before we start, I just want to show you that all files are cleared. Before anything is done here--If something is left from previous runs or whatever, so it's all files hypothesis data parts. It's all cleared and also other files that I used at some point. Everything will be generated. If I press 2, then we will initiate our reasoning agent. First of all, you see, it generated potential drug sand that is successful rules this hypothesis to [?] file. Then this file can be picked up by other agents and they can do whatever they need to do with it. Let's look at how it's all working. It is our abduction reasoning. Let's have a look inside. This is already a familiar library [?] to work with [?] to make it easier. For abductive reasoning--It also can be used for deductive reasoning. We will use Cambrian. It is libraries that--Essentially, it imitates the working of Prolog. Let's see how it is all organized. First of all, it inputs facts and rules. It inputs from files. We didn't link it directly to knowledge model. It can be linked. It's just a matter of passing the script. We did it in an easier way. We have a list of facts where it's all listed what chemicals, what properties, and also a list of rules. We have only one rule. Is that small molecules are likely to be drugs. This [?] pass. First, we print all our facts and truths just to be sure that it can pick it up. Now it constructs the relation. What it does, it now needs to put it in a structure that Cambrian can work with and it needs triplets. It needs this relation. We defined relation is up, and it also constructs this fact. There are two lists. Fact 0 fact 1. Fact 0 is these chemicals and fact 1 it is its properties. It can be a small molecule, a large molecule, [?], it can be drug, it can be toxic. Whatever property it has. It's two lists and then it's constructed. Then it defines function likely drug and it imitates the logic of our rule that it's likely to be drug if chemical is small molecule. In this case, X will be replaced with this chemical, and Ywill be replaced with a small molecule. This is just the construction. It is not where reasoning is happening. Construction of facts is happening here. List of facts and then its list of lists. [?] is our relation and then take fact 1, take fact 0, fact 1. Chemical name and some chemical property. The reasoning happening here-- Which chemical is a drug?First, a variable is defined which chemical is a drug?Then is this [?] chemical property for the drug it has to be small molecule, and then this is how Cambrian works. This output will be these potential drugs. All chemicals that have properties, small molecule. It's inbuilt function run. Zero stands for output all possible options or it can be numbers, then number of outputs will be limited by that number. Then this variable and this what we defined previously likely drug, this variable and chemical property rule that is set to small molecule. This is how syntax of Cambrian implemented by the logic behind it is likely to be drug if chemical is a small molecule. Then it just prints it and [?] to hypothesis file, and this is what you see as an output. If you're interested in understanding better how Cambrian work sand essentially using unification the same as in prolog, I provided the link notes so you can check. Otherwise, so this demo example of how reasoning can be implemented in python to generate some guesses. Hypotheses are not necessarily true, but something would be interesting to test because of a chance of being true. We used only one rule. In real-world, it can be many rules. [?] It's probably the best approach to try to generate something new, something what was checked using these rules. The output of the knowledge reasoning agentis a set of hypotheses will be passed to as agents"
DSM100-2022-OCT,Topic 8: Robot Scientist part 2,Lecture 4: Planning and scheduling experiments by the Robot Scientists,https://learn.london.ac.uk/mod/page/view.php?id=96401&forceview=1,"[jingle]In this mini-lecture, we will talk about how experiments for the robot scientist can be designed and planned. A planning agent requires knowledge about how to plan experiments. It also needs a formal description of what is in the lab, what equipment is available, and what it can do. Also, what materials are available and what are the constraints for planning, for example, how long experiments can last. The hypothesis generation agent is used in abductiveand inductive reasoning to formulate hypothesis. This hypothesis may be correct or they may be not. They need to be experimentally tested. It is critical to understand that not all hypotheses can be tested. We can't check every guess we may have like why stop the phone call with me?I need to check that with Ann, and Ann might not wish to answer my question, and I will be left with unconfirmed guesses. The same is with experimentation, especially automated experimentation. Some hypothesis operationally impossible to testor maybe they can be tested in one laboratory but not in the other. That depends on the available equipment, materials, and other resources. Maybe a hypothesis can be tested, but then something related to that hypothesis can be tested. Then from the test results it can be inferred if the hypothesis is correct or not. This is how it is often done. For example, in functional genomics, it is impossible to experiment with genes directly. Instead, an experiment is designed with a proxy for a gene of interest, and then a gene function may be inferred from the experimental results. I will explain how planning of experiments can be do neon the example of Adam planning of functional genomics experiments. To recap, Adam has a yeast metabolic pathways model encoded in logic and it can abduce hypothesis about what genes are controlling what chemical reactions. For example, Adam can abduce that several genes might be responsible for biosynthesis of prephenate from chorismate. Moreover, it uses bio informatics databases to find out what enzyme catalyzes this reaction. It is a class of enzymes, EC. 2. 6. 1. 39. The hypothesis can be represented in Prolog S, ""Gene and the class of enzyme. ""It is also important to capture negative hypothesis not, gene enzyme class. A set of this hypothesis is an input to a planning agent. The planning agent needs to know what resources are available for experimentation, equipment and its functionality, materials, and it also needs a knowledge base with rules for planning of experiments. We developed LABORS, Laboratory Automation Ontology for Robot Scientists to capture knowledge about experiments run by the robot scientist sand to assist in experiment planning. It is available at the link provided on this slide. The planning agent will check the list of available material sand it will find the genes YER060c and YER152c are not in the list. What is available is yeast strains with those genes removed. Experiments can be designed with them as proxy and not with genes. Such rules should be in the experiment planning knowledge base. This is a common experimental approach used in functional genomics, since it is not possible to directly observeor experiment with genes functions, experiment under the cells or organisms having those genes removed. It is like trying to determine the function of a steering wheel in a carby removing it and then observing what would happen. It can be observed then, the engine still can work. Say for inference, starting an engine is not a steering wheel function, but is no longer possible to turn the wheels, therefore, it can be inferred that the function of the steering wheel is to control or turn the wheels. It is the same as functional genomics. The planning agent designs experiments with yeast strains that have those genes removed. To observe if its behavior has changed. If gene YER152c is indeed responsible for the synthesis of prephenate, then without the gene, prephenate will not be produced and yeast cell will die. This can be determined in a yeast growth experiment. Still, yeast can die for some other reason. Can a more conclusive experiment be designed?Yes, it is possible to add metabolites or small molecules to the growth medium to restore the growth of yeast, then the experimental results will be more conclusive about the gene function. Without the gene, yeast can't grow, but if a metabolite can stimulate production of prephenate, then it can. There are several metabolites that can compensate for the missing gene including lysine. This knowledge again comes from the knowledge base. Now, the blanket agent can replace the initial hypothesis, ""gene enzyme function, "" by a proxy, affect growth metabolite yeast strain. The predicate affects growth, corresponds to the representation of what can be observed in the lab. The lab has optical density readers that can scan yeast culture to determine if it is growing or producing more cells or not. Now we have a testable in-the-lab set of hypotheses. Metabolites and yeast strains are in the list of material sand the equipment has the functionality to observe if yeast growth has been affected or not. This slide shows the representation of hypotheses for the robot scientist, Adam. Experiment plan knowledge base should contain rules reflecting good experimental design practice. For example, experiments require inclusion of controls. The best control is a comparison of the growth of deletant yeast strains with the growth of so-called wild yeast with no genes removed. The recommended duration of yeast growth experiments is 48 hour sand the recommended temperature is 30 to 37 degrees Celsius and so on. Unfortunately, in the reality, there are still no such experiment, plan, and knowledge bases. The necessity for planning information usually is added by human scientist sand also many shortcuts are used. For example, hypotheses from the beginning must be about entities that are available in the lab. Planning agent has to produce an experimental protocol to test the input hypothesis. An experimental protocol is a list of experimental actions with specified parameters for the robotic lab to execute them. For example, at 50 milliliters of YPD medium to 500 milliliters conical flask, experimental protocols can be complicated, containing many steps, producing them requires elaborate planning. To support planning for the robot scientist, we develop an ontology EXACT experimental actions. It defines typical experimental actions like add, move, inoculate, shake, and set parameters like temperature, speed of shaking, and duration of shaking. EXACT cools experimental actions in RDF, Resource Description Framework, on this slide you see an example of how experimental action scan be represented in a textual form suitable for humans to understand. The experimental protocol was represented in RDF first, and then translated to a textual format. An experimental protocol in EXACT is a high-level protocol, the level at which human scientists plans their experiments. This protocol needs to be translated further to a low-level Machine process-able protocol. Different pieces of lab equipment may require different inputs in different formats. That depends on the supplier of the equipment. Integration of various tools in one lab is a challenging task, there is a lot of research into supporting interoperability of automated labs. For example, SILA standard enables communication of open systems, integration of instruments, and also lab systems to each other. In this mini-lecture, we inspected how robot scientist scan produce experimental protocols to test hypotheses formulated by the hypothesis generator. Experimental planning is a highly demanding task requiring sophisticated reasoning over knowledge about experiments. Robot scientists can carry out thousands of experiments in parallel, testing many hypotheses at the same time. Human scientists are not good in parallel tasking, but they're better than robot scientist sat the high-level planning of experiments. In reality, experiments are planned jointly by human and robot scientists."
DSM100-2022-OCT,Topic 8: Robot Scientist part 2,Lecture 5: The toy planning agent,https://learn.london.ac.uk/mod/page/view.php?id=96404&forceview=1,"-In this demo video, I will show you how our planning agent works. A plotting agent takes input a list of hypotheses, and it needs to output an experimental protocol to test them. To initiate the planning agent, you need to press three in the main file. Let's do that, three. First what it does it checks the hypothesis to test. We have a separate agent, reasoning agent, that can generate this hypothesis. If planning agent was called before that, maybe you press three first thing not one, not two then file this hypothesis would be empty and this wouldn't be hypotheses to test then it will inform that there are no hypotheses to test. Keeping hypotheses in a separate file it's good because they don't need to be generated may be some human scientists just don't have hypotheses to test and it can send these hypotheses to robot scientists through this agent to generate the protocols. Hypotheses can be read. It's important to have it so this is an input so there are hypotheses to test. Wonderful. Let's look inside. This is planning agent script the what we see insideso again important posts for working with files, and the nit imparts several custom modules. It needs not only hypotheses, this agent needs quite a lot of knowledge. It needs to know equipment, what equipment is available in any particular laboratory, and what this equipment can do. Different labs will have different equipment with different functionality. It's important to specify. Let's have a look what is in our list of equipment. We have here equipment and we have only a few piece sin this case, we have plate that has functions talk and then whatit can store some cell culture. This is just a template the form description of equipmentso what it is function and to what so with his workings object, then we have an incubator that can grow. And with it's working with some plate growth some cell cultureis on this plate, then we have a reed with functionality to measure potency. Potency is drug activity. Drug activity can be measured by you various measure sand potency indicates how activities and in our case, we will measure it by some real number from zero to one. The closer to one, the more active the more potent the chemicalis closer to zero less potent, and so it can measure potency then and it can take plates, books with the plate and also have trashed it can discharge this plates. This is the list of our equipment what it can do and with what. As then we also have materials, let's have a look, materials. Any lab has a description of what is available. So here is just the list of all chemicals. Whatever requires for experiments, again, depends on lab, depends on a day something can be available something might have been available. If it's not available, then it's possible to purchase or maybe it's not possible. Something is easy to buy something is not some chemicals are expensive. Difference in price can be 100 times between different chemicals. Any lab will have this accountancy what is available and what quantity, how much it cost. If something needs to be ordered or orders that are awaiting sets, what is shipped. It's quite complicated accountancy usually for each series labs. In our case, we have only chemicals, some cell culture not worried about costs. Let's go back to our planning agents. It takes a description of this equipment, materials, and also experimental protocols. Output will be experimental protocol but what it takes us input it's some generic protocol, some standard, each lab is designed around that usually several standard protocols. Protocols it then can be modified, substantially modified, but there are some basic steps. It's not done from scratch. I've never seen it. In theory, it's possible, but it's just not how it's done. We can have a look at the experimental protocol so yes. With this here, it's actually a script for generation of protocol. It is based on some standard protocol set has key steps. Steps are defined like name or step, and then both equipment and would heat. Let's have a look. First, they needs to know iterations. Usually, everything is repeated like we have a read, we have an incubator. So regions measurements usually are taken several times, we just specify how many times. You grow for 20 minutes and every 20 minutes you make measurements of what is happening and put it back to grow. You need to specify. Since you have these typical actions like add what to add to what so you can add chemical to play the move, you can play it move play to an incubator, then you grow and play it for 20 minutes. It's like recommended duration. It's in this standard protocol, but it can be more define dit can be more frequent or less frequent move to reader. This is like cycle every 20 minutes, grow in incubator move to reader, measure potency, and move back to incubator advisor and discharge. It's stanard protocols that reflects current knowledge about good experimental practices. What this particular lab can do. Also, in the beginning, there is add control drug. If you remember, our hypothesis generation agent formulated for hypothesis for chemicals were identified as potential drugs to test. This agent is about to design protocol. More specifically it will be instantiated this generic protocol to test this hypothesis but what it also does, it adds control. In this case, it adds morphine. We know that it is a drug. It is in our knowledge base that it is a drug. We added it's called positive control. Just to be sure that our measurements indeed shows that it is a drug then we will have more confidence than other measurements will show that another chemical is a drug but have more confidence that measurements are accurate that we can trust it. It's also good practice to add some negative controls that we know for sure is not good. Those are toxins, we know for sure it's not good but we are not doing it here. This is our standard protocol. Go back to planning. This is what planning agent needs to know. It needs hypothesis. It needs description of equipment. It needs description or materials available materials. It also needs some knowledge about planning of experiments like agent controls, or what are standard protocols?What are possible?A lot of kinds of knowledge. The reality is that this experimental practice, experimental knowledge is not well-documented. There are protocols, but also knowledge, it's usually in a few months, heads passed from professor to students or not passed the tools, many errors are repeated again and again, but it's a good when it is recorded. It's good when it is represented in knowledge-based, and then it can be used for generating better protocols. It input all this information, define some parts that we need, like where this hypothesis. Yes, it's first checks if there are hypotheses on some. If there are hypotheses, it start learning. First of all, it retrieves hypotheses and brings them. This is what we see, and then press something to go to the next step. Oh, what is happening on the next step?It translates, given hypothesis is to testable hypotheses. Let's press and see what happened. Our hypothesis is likely drug [?]Whether with [?] it is a chemical that is in Tumeric. This is what is used in Indian cooking that gives this bright yellow color. There are indications that it is active. We run experiments with that and there are indications that it can be used for cancer treatment. In this knowledge, one that we don't know it, so we would like to check it, but we don't have any equipment that can measure, ""It's likely drug or not?"" Without equipment it can measure potency. There should be a rule in a knowledge base that the this likely drug can be replaced to measure potency. If potency is high then it is equivalent to likely being a drug. This planning agent replaces hypothesis, generated by hypothesis generation agent, by testable hypothesis, but hypothesis is that it's possible to test in this particular lab. This is a difficult step, and in the reality, these hypotheses are usually given in this form, and it's also how humans operate. They may check something. They would like to test some hypothesis, but then they are bound by the reality. What is possible to test and not every hypothesis can be tested. Equipment is limited, so a lot of processes are used and then backtrack inferences done about what the initial question was. Shortcut will be to give hypothesis immediately in testable format. In these cases, planning agent just can't check functionality of the equipment. Also yes, measure potent so it can be replaced. The likely drug by measure potency chemical, and if this potency high, then yes, there should be [?]Yes, now it has this testable hypothesis, then it will attempt to test sti0ll. It's quite important to check. What it does, it checks that material is available. It checks if equipment that measure potency exists, so yes and it checks if there is everything in store. If all chemicals are available if they are in the list of material. It's easy checks but they are important. The next step it causes some standard protocol. This is how this protocol looks like, so move plate to incubate. Bottom line is missing. First add chemicals to plate, and so in the [?] standard, the protocol generations that we have four hypotheses, four chemicals and it's adding one extra chemical as positive control. It adds more so we have five chemicals here. By the work can show you example, how these chemical scan be distributed on a plate. It's just an example. Here very simple plate. It has four rows and six columns. In reality, plates can be 96 well plates, so each cell it's a well. Different chemicals, different gross media can be there. Each well can be like a different experiment. It can be 3, 8, 4, well plates or even more. Then ultrasound is used to transfer chemicals to that super small well, so it is all very advanced. In our case, it's a vey simple plate. This is chemical. It's quite important to distribute them on a plate quite evenly because even on the small plates, there can be side effects. It's in an incubator, it's growing. On age, it will be more exposed to air. It can grow differently from being inside. It's also important to have this control. We have a line where there is just nothing, and here nothing and take measurements there. This is negative control, and we have positive control having more films that you know for sure it's [?]Then each chemical is placed several times. It's placed three times at each measurement. It will be measured several times, but it's also three times on a plate, and distributed randomly, or some more regular design like Latin square design, but it's such small plate it's difficult to do. This is just how it looks like. Let's go. All right, so it adds chemicals to our plate, in the way how I've showed you. It follows this plate design, what to put where?Then move plate to incubator, to grow. Repeat how many times?Then, grow for 20 minutes. Move to reader. Measure potency, so take measurements, and go back. It loops several times, and then discharge the plate. It takes this protocol and so it's instance sheets. Instance sheets protocol it just adds these concrete chemicals, and non both loops. It's all quite easy, but it's easy only if enough knowledge is given. If there is this genetic protocol. If this hypothesis are specified, if hypotheses are testable, if equipment is described. Then as this is a toy example, but it's not very far from the reality. The reality, of course it's just more complex. There are many licenses that protocols many iterations, culture will be growing like for a day, or maybe several days. Technologically it's very similar. A standard protocol is taken and refined. Again, if something doesn't work, then some alteration to this protocol can be done. Maybe it will grow for longer. Maybe temperature will be higher, till good combinations are found, but this is how its pretty much is done in the reality. We looked at the toy planning agent, this agent can take input the hypothesis, it also needs to know about the lab, what the equipment is, what it can do, what materials are available, and also what experimental protocols. Standard protocols H-lab usually designed to run certain protocol sand some good experimental practices like agent positive-negative controls. It takes hypothesis mainly on outputs and experimental protocols. executed in real laboratory. Experimental protocol that then can be"
DSM100-2022-OCT,Topic 8: Robot Scientist part 2,Lecture 6: Analysing experimental results by the Robot Scientists,https://learn.london.ac.uk/mod/page/view.php?id=96407&forceview=1,"[music]-In this mini-lecture, we will talk about the analysis of experimental results by Robot scientists. Rigorous analysis is required to make correct conclusions, bounds are considered hypothesis. Human scientists tend to be biased, naturally, they want their hypothesis to be confirmed. They also tend to overgeneralize their conclusions. Robot scientists are more objective in their investigations. Also, because every step of the discovery process is recorded, it is easy for an independent expert to check how conclusions were made and if they're unbiased and generalized or not. Robot scientists can run thousands of experiments in parallel, non-stop. They produce lots of experimental data. They use machine learning to analyze experimental data. Interpretation of the result is important for understanding what the results mean and if the hypothesis were confirmed or not. It is not always to do so. This slide shows how the results of yeast growth experiments look like. Yeast culture is inoculated to plates with growth medium. The plates are scanned every 20 minutesto estimate how many cells are on the plate. The curves on the figures are yeast growth curves. The experiment is around for 48 hours, it is depicted on the horizontal axis and on the vertical axis are optical density measurements that corresponds to how many cells are on the plate. In blue, the growth rate of wild-type yeast unaltered with all genes. In red, yeast mutants strains with the gene removed. Interestingly, in some cases, a removal of a gene actually improves the growth rate of yeast. The growth scales have several critical parameters. For example, at doubling time, when the number of cells has doubled, maximum density and so on. These parameters along this information about yeast strain sand added metabolites are put into a table for processing by machine learning tools. Please note that on these figures the names of genes are represented by different IDs. For example, S and F cell is the same as YLR or 25W. Unfortunately, using different IDs for the same entity is typical for biomedical research. There is even a joke biologist would rather share a toothbrush than a gene name. That is why assigning all entities with globally unique identifiers like UT Isis critical for the interpretation of the result sand sharing information. Robot scientists then use decision trees to decide if the growth of yeast strains was affectedby the addition of specified metabolites or not. As you could see from the growth curves figures, the difference between the growth of a wild type and a mutant can be subtle. Since experiments were run by the automated system, it was easy to replicate them. The robot scientists replicated all experiments many times, producing many data points for the analysis. This enables the detection of subtle difference sin a statistically significant manner, which would not be possible to do otherwise. Experimental results are available by following the lin kat the bottom of the slide. It is important to remember that experimental results are only a proxy for the initial hypothesis. It is impossible to experiment with gene sand satisfy his strains with genes deleted or used. Experimental data are about those strains, not about genes. Therefore, an extra step of inference is required to make conclusions about the initial hypothesis. Since the hypothesis includes gene enzyme class was replaced by three proxy hypothesis about eating metabolites, first conclusions have to be made about proxy hypothesis. If they were confirmed or not, and only reason about gene functions deciding if a particular hypothesis has been confirmed or not may be difficult. For example, three different metabolites may restore mutant yeast growth. However, experiment shows that only two metabolites, indeed restored the growth, but one did not. Overall the experimental results were not fully conclusive. In consultation with the domain experts, hypothesis acceptance ratios were set up when it is scientifically viable to accept the hypothesis and when it is not. It is important to understand and remember that many scientific results are like that they are not fully conclusive, and they may be about some proxies, not the real entities of interest. The robot scientist Eve walked on identifying potential drugs, leads for malaria. Eve [?] lab experiments with purposedly engineered yeast strains, chimeras. Yeast has been outed to include proteins of interest from both humans and two major malaria variants. In this way, experimental results could indicate what chemical compounds are harmful to both malaria variants but not to humans. That is why what we want from a new drug to kill malaria, but not the human. Experimental results were analyzed with the use of Random Forest, support vector machines, and many other machine learning algorithms. Random Forest was one of the best performing algorithm son such data. You can read more about these experiments, how they were designed and analyzed, follow the link to our Open Access paper at the bottom of this slide. In this mini-lecture, we looked at how experimental results were analyzed by the robot scientist. Various machine learning algorithms were used to decide if initial hypothesis about gene functions were confirmed or not. Such automated decisions were then verified manually by conventional biomedical experimentation. Many of hypothesis produced by the robot scientist shave been confirmed, therefore, robot scientists automate not only production of valuable biomedical data, but also automate the analysis and repetition."
DSM100-2022-OCT,Topic 8: Robot Scientist part 2,Lecture 7: Meta-analysis agent,https://learn.london.ac.uk/mod/page/view.php?id=96412&forceview=1,"[music]So far, we have considered the very core agents that enable the robot scientist to carry out a full cycle of scientific investigation. The knowledge agent that encodes all the underpinning knowledge for the robot scientist, the hypotheses generation agent to formulate new hypotheses to test, the planning agent to produce an experimental protocol to test those hypotheses, the automated robotic lab agent to execute the experimental protocol and record experimental results, the experimental results analysis agent to decide if the hypothesis were confirmed or not. In this mini-lecture, we will talk about additional agents that can enhance the performance of the robot scientist. For example, a meta-analysis agent. A meta-analysis agent can collect information about all previous runs, results, and compare performance of other agents in what situations they work best. For example, what methods the hypothesis generation agent used and what methods work best in what situations. If, at the next round, the meta-analysis agent can recognize the situation, then it can advise what method to use. We have implemented such an agent for the robot scientist, Eve. It is called Meta-QSAR. The main technique that the meta-analysis agent employ sis meta-learning or learning to learn. It is a powerful approach for making learning more efficient. Meta-machine learningis learning about machine learning algorithms, the parameters, and what algorithms perform best on both datasets. The robot scientist Eve uses QSAR learning to predict chemical compounds by activity from its structure. Please refer to our open-access paper Meta-QSAR Large Scale Application of Meta-learning to Drug Design and Discover yin the Journal of Machine Learning for full detail. The QSAR task is given a target, usually a protein and a set of chemical compounds, small molecules, with associated bio activities, for example, inhibiting the target, to learn a predictive mapping from molecular representation to activity. The idea behind Meta-QSAR is the no free lunch theorem. There is no universal machine learning algorithm that performs best on all datasets. Almost every form of statistical and machine learning method has been applied to learning QSARs. There is no agreed single best way of learning QSARs. Meta-learningis to learn what learning is better in what scenarios. A motivation for itis to understand the performance characteristics of the main baseline machine learning methods currently used in QSAR learning. Meta-QSAR learning should be successful because although all of the datasets have the same overall structure, they differin the number of data points or tested chemical compounds, the range and occurrence of features, compound descriptors, the type of chemical/biochemical mechanism that causes the bioactivityThese differences indicate that different machine learning methods are to be used for different kinds of QSAR datasets. The task is, for any given QSAR problem, select the best combination of QSAR and molecular representation that maximizes a predefined performance measure. We can see that over 8, 000 QSAR datasets. Features of these datasets are like number of examples, mutual information, and some other features. We considered 18 base-level learning algorithms, including linear regression, support vector machines, neural networks, regression trees, and random forests. We measured the performance of each algorithm on each QSAR dataset. Before carrying out meta-learning, we have to carry out QSAR learning or baseline learning. We extracted nearly 3, 000 targets, drug targets, from ChEMBL. It is a curated publicly available database. The number of chemical compounds per targe twas from 10 to about 6, 000. There were several associated bioactivities:IC50, EC50, Ki, Kd, and their equivalents. The bio activities have been normalized by our collaborators from the University of Dundee. All compounds were represented by so-called fingerprints. It is binary tables showing presence, absence of each chemical or group. Among all algorithms, random forest was the best performer. It shows the best accuracy of prediction sin more than 1, 000 targets out of nearly 3, 000. The second best was SVM. If there is no any additional information about datasets and drug targets, then the best option is to employ random forest. Often, it would yield the best predictive results. However, in over 50% of cases, random forest would not be the best choice. Information about datasets and target properties can help to choose the best one, and this is meta-learning. To do meta-learning, we need to prepare meta-datasets. To form meta-datasets, we need to define their meta-features. We use characteristics of the datasets considered in the base study and drug target properties as meta-features. We encoded meta-features in the form of an ontology. The Meta-QSAR ontology describes features of datasets, like number of compounds, mutual information, average minimum of the input, and features of targets because each dataset is associated with a specific drug target. To form a meta-dataset for training meta-QSAR learning, we described every data set used at the base learning stage with the defined meta-features. We calculated number of instances, mutual information, kurtosis, and so on, and other descriptions of the targets corresponding to each dataset, totaling over 2, 000 of meta-features for over 8, 000 of datasets. We split the meta-dataset on training and test meta-datasets, trained and tested our meta-QSAR learner. Then we evaluated this performance by comparing the performances of the best-suggested algorithm with the default. The default or baseline was random forest because it performed best at the first stage. Most of the algorithm predicted by our Meta-QSAR learning perform better than the default. Meta-learning can be successfully used to select the best QSAR algorithms for a given dataset and drug target to formulate hypotheses about potential drugs. The same meta-learning approach can be applied for other problems, not only learning QSARs. In this mini-lecture, we can see that a meta-analysis agent for the robot scientist, Eve. The agent was constructed on the basis of analysis of 18 machine learning algorithms applied to over 8, 000 of drug datasets. It was trained to predict what machine learning algorithms will work beston what type of dataset and molecular representation. Such an approach is called meta-learning, and it can be used for other types of learning."
DSM100-2022-OCT,Topic 8: Robot Scientist part 2,Lecture 8: Communication agent,https://learn.london.ac.uk/mod/page/view.php?id=96414&forceview=1,"-In this mini-lecture, we'll consider a communication agent for the robot scientist. It was developed for Eve to enable communications mainly between machine learning tools and automated robotic lab, and also start to work on communication between human and robot scientist. Why communication is important in multi agent systems?In multi agent environments, agents can act independently, they can act as a swarm, and then they don't communicate. Or, they can be competitors, for example, playing a game. In this case, they will benefit from knowing about another agentas much as possible, or they can compete without any such knowledge, but the more they know, the better strategies they can choose. Or, they may cooperate. Again, they can cooperate without knowing what others are doing, so it can be quite swarm-like, but if they know what shared goals are and what other agents are doing, then it will be more intelligent behavior. The best way to cooperate on achieving a shared goal is by knowing the shared goal and to share knowledge, not only about the goal, but also the work where they operate. Again, each agent can have its own knowledge model of the word, what they know about the word, and they can act based on it. They can exchange this knowledge model, so other agent know what other agents assume and what they know, or they can have one single knowledge model representing join knowledge about this word, so different scenarios as possible. They also need to know what other agents are doing. The more they know, the better strategies they can choose and more chances of achieving this shared goal. It is especially important in dynamic environments because everything can change. The goal can change, the world can change, knowledge about this world can change, and what agents are doing also can change. They need to regularly update this knowledge. The best way of doing this, by communicating. What communication involves?What is important to consider?Of course, content what they're communicating, so what knowledge they need to exchange, then, an agreed protocol for this exchange. What steps?What information?What they need, and mode of communication, how they're doing it?Is it via text messages?Is it voice or some other means?It is a hot area of research. There is a lot of work on development of such protocols, languages, and whole series. In this mini-lecture, I will tell you about our work on development of a communication agent for the Robot Scientist, Eve. This is a very simple agent and because it's of a scientist, we know what communication is about. The communication is about these basic steps of scientific discovery. We call this protocol Sci Com, Scientific Communication Protocol. It's in current minimalistic form. Yes, it requires a lot of extension and future, and I will tell you about future plans. What we developed so far, it's a protocol that is focusing on exchange information about the most critical aspects of scientific discovery. It is all about request for experiment sand communicating results of that experiments. We define syntax and semantics for such communication. It is based on the ontology Eve, that we specifically developed to support the robot scientist Eve. I will introduce you with this ontology, so what is there. Eve ontology provides formal logical descriptions of most essential entities that are important for experiments, experiments that Eve can run. It defines typical experiments. For example, the most typical experiment, what we call Optical Density Measurement Experiments. It's experiments that takes these input yeast strains, add different chemicals, possibly other factors like glucose, maybe change oxygen concentration, and by the end, it measures optical density. Optical density is measured by light going through the plate where this culture is being grown. If it cannot penetrate much, it means that there is a lot of growth, a lot of yeast cells, so measurements will be different. If yeast doesn't grow well, then measurements will show it. This is the most basic measurement and the whole experiment we called Optical Density Measurement Experiment. Eve defines all essential components of such an experiment. Then, Eve ontology also defines possible experimental factors, so what can be derived in such experiments?For example, metabolites or these chemicals, what can be added?How much?Then, what else can be changed, medium?It's quite important on what this yeast culture is growing. It can be rich medium, it can be defined, it can be minimal, so growth depends on this medium. For example, duration, it can be grown for two days, for four days, and this experimental factor, and our control can decideor Eve can decide, so robot scientists can decide. What can be changed?These changes can be done for the whole plate or it can be done for each individual well on this plate. If using 384 wells on one plate and each well can have different metabolites, and it can have different yeast strains, so it's essentially different experiment in each well. Something can be changed on well level, something on plate level. It all needs to be described and defined, so Eve can then make these changes. What is considered as input?What is input to experiment?The main, what it experiments with is yeast strains. What are outputs?Optical density regions, I already explained what it is. It also can take fluorescent measurements or it can measure concentrations of chemical or glucose. It all needs to be defined and described. What are capabilities of the lab?If we will talk about different labs, they can be different equipment, and they can be different measurements and different capabilities. It needs to be defined and encoded in machine-processable form, so then experiments can be planned. This ontology was used to define all important entities about experiments. Here's example of experimental factor at plate level. What is important to know?For example, plate can have a lid, and if it can put the lid on and it can remove lid, it's actually quite important. If lid is on, then less oxygen, and yeast is growing differently. If it's exposed to oxygen, then measurements can be very different. Also, liquid can accumulate on these lids. This is important to record and also to make a decision if lid is on or off, or maybe for some part of experiments, lids are on and when they are off. Then, shaking. Especially when metabolites are added, so it's quite important to distribute them evenly and incubators where yeast is growing [?], so you can put shaking. Then, how strongly to shake?For how long?It all needs to be decided then recorded, and also shaking can be different. For example, we have serious troubles when experiments couldn't be repeated for a long time. Every time the results were different. It's because this shaking moment was changed, so if it's not orbital, then yeast was forming pellets on the bottom and all measurements which is completely different. Every parameter is very important and it needs to be defined and it needs to be recorded. Then, how frequently to read. Again, there are some minimum and maximum. How often, it's possible, or when it's already doesn't make sense, if it is one to two hours, so too many things can change during this period, so it's possible to miss something important. Again, the Scientology defines it. Then, temperature, what is minimum temperature the lab can have or incubator can have?What is maximum?What is recommended temperature?The same duration and even plates, it can be very different. Different plates, different culture, and obviously, everything, every minor detail can affect the experimental results. Here, example of, what should be recorded and defined at a well plate?At each plate, for example, concentration of glucose, so it's food-free yeast. It can be zero, no glucose added or you can just have only glucose, recommended is 10%. Then, medium. This is standard, the medium YPD. There are many other medium where yeast can grow. As in any good experiment, we will include positive and negative control. One way of including the control is to add wild-type, and even wild-types, they can be different. Here is recommended strain to include as control. Then, volume. How much is possible to put in one well?Again, minimum, maximum and also this increment is important, how much it can be changed. What equipment is capable of handling?Then, other factors, so here are examples, but I think you got the idea that any experiment is complex and it's important to record about it as much as possible. Implementation of the Sci Com protocol. As I explained, it's based on Eve ontology that defines a standard experiment. The protocol actually enforces recording only differences, deviations from this standard protocol. On the right, you see implementation in YAML. YAML is a data serialization language and it's quite easy to understand, not only to robot scientists, but also to human, so it's quite readable. Each experiment is assigned to these, an ID, and then just if it's standard experiment, yes, and then only differences, of course. Like strains can be different and then it's specified with strain sand what outputs are, so what to record. Outputs, again, so standard recording will be optical density measurement, and so it's like default. Here you see output glucose and ethanol. It's like additional optical density recordings will be done and if additional outputs required, and this is what this message will convey. Most likely it's some simulation, machine learning simulation with simulation done with Moodle and they would like to check something in real lab, some predictions. Will it be confirmed or not?They need to make this request for an experiment, grow these real strains and observe what has happened, and then if predictions were confirmed or not. If not, then we'll go back and think again. Something was wrong, take this new data into accountand run simulation again. This is how it's working. This communication protocol supported the real projectis Ada Lab project, your IPN project. Five universities were involved and the communication was between this various machine learning tools that were developing computational model of complex biological system to understand some biological phenomena. There was a lot of computational simulation and then the simulations need to be tested if they're correct or not. It was like a cycle, get more data, and do better simulation, update model. Again, make prediction, check in the lab if it's true or not. We also attempted this communication between human and robot scientists. Would human who input and say, ""Recommend it, "" would the controls to select some experimental settings like concentration durations are sometimes hard to better represent experimental results. This was input from a few months back from robot scientists. It's mostly results of experiments. Of course, this is not enough and in future if think seriously about robot human scientists working together as a team, then they need better communication. In these scenarios again, communication is easier because you know what they're going to communicate about. They're going to communicate about hypothesis. They're going to communicate about experimental design, about maybe final models. We develop this very foundational cycle as was the duration, but it is obvious that it needs extending, especially if it can handle communication about hypothesis. It would be good not only to pass this hypothesis, how it's done now, but also explain why this hypothesis to test. What will it give?Why not others?How it was selected?The more information, the better. Again, this hypothesis maybe can bechanged like first results started to show. You don't need to wait two days. For example, yeast usually is being grown for two days, maybe you can updraft experiments early if results are not as expected and shortens the cycles to amend and start something new. There are many possible scenarios. It is something for the future work. In this mini-lecture, we looked at the communication agent. Such agents are important to support communication between different components, between different agents. [?] come together on a shared goal or even if they're competitors, they still benefit from knowing more about the charts. The particularly interesting aspect is to support communication between humans and intelligent agents. In this mini-lecture we can see that the communication within the robot scientist Eve, is between agents in Eve, machine learning and robotic part of Eve, and robot scientists but also between human"
DSM100-2022-OCT,Topic 8: Robot Scientist part 2,Lecture 9: Robot Scientist: new projects,https://learn.london.ac.uk/mod/page/view.php?id=96418&forceview=1,"[music]-In this mini-lecture, we will consider the state-of-the-art of robot scientists. Of course, there are many projects in scientific discovery, automated discovery, but I will tell you about our projects. Projects that I know inside out, I know what we are targeting. I will tell you about something which is not published yet, and what other people still have no opportunity to know about. I will tell you about two new projects. The first project is Action on cancer. Action stands for a decision support system for the treatment of cancer. Several organizations are involved to this projectis the University of Cambridge & Goldsmiths College, the University of London. The project is funded by Research Council UK, Engineering and Physical Sciences Research Council. It's £1. 4 million for this project and it's four years duration of this project. I am Principal Investigator in Goldsmiths. The aim of this project is to develop a prototype AI system for the design of personalized cancer treatments. There are many therapies for cancer, but this project is focusing on design of drug cocktails. I will tell a little bit more about it in a minute. Longer-term aim is to perform equally or even better than human oncologists. There's AI systems that will make recommendations for treatment son the level of experts, oncologists, or better than them. The motivation for this project is that canceris one of the world's greatest killers. 13 million deaths and 20 million new cases per year is estimated by 2025. According to World Health Organization, it's every second person on the planet will develop cancer at some stage of their life. Of course, research programs for investigating cancers, biological projects, and the AI projects, the research programs are well funded, they're substantial, but they still cannot address all the challenges because it's a very complex disease, and we are far from finding really good treatments for cancer. We are very far from solving this huge problem. However, over the last decades, our knowledge about cancer has increased enormously. We now understand the fundamentals of this disease. The major factor is malfunctioning of information processing in cells. Something is happening with signaling, it is disrupted, and cells start behaving differently. They no longer obey commands, they don't behave how they should behave and it is how tumors start to gro wand then how cancer develops further. We started to understand the biological side of it. It's still not enough to find good treatments of this disease. The treatment is complex. There are many therapists, for example, radiotherapy is very aggressive therapy. Immunotherapy it's less aggressive and very promising. There is a lot of research in this area. One of therapies is chemotherapy when it's treated by drugs. Traditionally, a single drug is a sign with attempt to kill these tumor cells, but our metabolic network is very complex and cancer can easily find ways around and develops resistance to these drugs. Action project focuses on finding a combination of many drugs, something between 6 and 12 is the recommended number. This is a more effective treatment because it's harder to develop resistance at the same time to so many drugs. Even if cancer would develop resistance to one or two drugs, it means it will be weakened against other drugs. Cocktail of drugs working better than single drug, but you still want to find such cocktails that is quite individually tuned to individual patients because cancer it's very individual disease. Even if you take twins, their cells have acquired significant differences that can dictate different development of cancer. The focus of Action project is first, personalized treatments or genetic information is taken into account. We still don't have access to medical history data, but if we will get such data also we want to include history with medical treatment with reactions. At least we now have access to genetic information. Existing cancer model can be amended, taking into account this personal information, genetic information. Also, it's personalized and because it's combination of drugs, it's more effective because it's harder to develop resistance. In such cases, because we talk about combination of drugs since there are thousands of drugs, and if we want combinations, we have combinatorial explosion. Computational simulation is absolutely essential. Then most promising combinations should be tested with cell culture to see if it really makes sense or notif cancer cells are being killed and at the same time healthy cells are not killed so it's not toxic to them. Arguments here is fairly simple. While computational approaches by AI approaches are vital importance here, it's simply because we don't have enough blood cells to experiment with all possible combinations of drugs. Before running any experiments, we first have to narrow down this search space. This is an overview of the project. On this diagram you can see the key components of the systems that we are working on. It's the heart of robot scientists. It is automated labs that can test hypothesis. A hypothesis will be about this combination of drugs [?]. In other projects, we were using yeast strains. Experiment in this yeast. In this case, is mammalian cells. Cells from biological organism and test on them if this combination of drugs it doesn't kill cancer cells. It's not toxic, but it can kill cancer cells. This is the idea. This is how it will be tested and how to come up with suggestions. First, you need to combine all data you have, all knowledge you have because there are cancer models. We have some understanding how cancer works, but it's all generic models. They're not tuned for individual patients with genetic information. You see on the slide, example like there are public databases, there are clinical trials, the Cancer Genomic Atlas, Human Phenotype Ontology, National Cancer Institute [?]. There are many resources. Plus, we have agreement with some companies that are working with patients and they're doing screening of the tissues. We have an agreement that we are doing data analytics for them and they're giving us this data. After they're putting it all together, we do a lot of reasoning and we try to come up with these suggestions. What combination of drugs achieves the desired effect?If everything works well, then such AI systems that can advise these cocktails will be able to output this personalized cancer model and also these recommendations. Then on some next stage, then it can be tested not only in lab settings but on some may be starting from mice. Whatever future developments will be here. Another project I would like to tell you about is called Ambition. It's a completely new project. It's also with the University of Cambridge & Goldsmiths. I'm a Principal Investigator in Goldsmiths. It also with lab automation companies that this named Arctoris. It is located near Oxford. This is a relatively small project, £300. 000. Also funded by Engineering and Physical Sciences Research Council UK. It's just for one year, but it is the first stage. We hope that it will be extended for further stages. Within this first phase of the project, the main proposal is to harness the power of AI and automated experimentation to provide researchers in the UK and beyond with continuous uninterrupted remote access to AI/robotic augmented biomedical research capabilities. The motivation for this project is that during the pandemic, it was clearly demonstrated that biomedical researchis of vital importance, and it's critical to maintain research continuity because many labs that could work on development of drugs to fight this disease, including our labs, they were closed during pandemic. The lock down and social distancing pose serious threat to research community. Many laboratories were closed and years of research were lost because of that. What we are proposing in this project is to develop a demo lab to show that it's possible to carry out all these experiments remotely. Not just that experiments but enhanced by AI technology and to have this closed cycle of robotic experiments. This slide shows an overview of the system. It's a modular system. In the middle is APIs that can connect to other components. The components that we already have like hypothesis generation agent, planner agent, data analysis, experimental results analysis agent, they all can interact with API with this interface. Also, robotic pieces of equipment also can interact with this API. If necessary, there can be lab technicians. It can be semi-automated or it can be fully automated. The main idea is that it is remote system and users, scientists, they can access it from anywhere anytime so they can just send their task sand this lab will be working towards this task. Design experiments, run experiments, record everything. Hopefully, also provide explanation and feedback. It can be just experimental results, or it can be models. This is what we will be working on. What I would like to stress is that such automated laboratories on particular Ambition doesn't aim to replace humans. The aim is to empower them by reasoning and data processing capabilities to support their decision making, and especially if it's possible to do remotely. It will enable a paradigm shift towards real-time treatment supported by continuous testing of novel drug combination son patient-derived tumor materials. Because this robotic lab can work non-stop, the scientist or practitioners, they just can send this task, test this, test that and see if we'll have continuous results so decisions can be made and changed quickly. Essentially, this bridging the bench and bedside, where patient-derived tumor material can be tested in real-time and treatment decisions can be adjusted accordingly, reducing side effects and improving efficiency. It is longer-term vision. Hopefully, we will be able to continue this project. Even as the first stage, if we can achieve this to show that the research labs can workin fully autonomous and remote fashions, that will be a big step forward. In this mini-lecture, I presented two new projects that involve robot scientists. Of course, there are many more projects on knowledge discovery, but I talked about our projects because I know them inside out. I know what our aims, I know where we are with this. What I presented, it's not published. You will not find it anywhere else. This is an opportunity for you to learn about the real state of the art. What scientists are doing, what they're working on, what they're aiming, what published papers in five years will tell about. Hopefully, they will tell about the results of this project. Of course, there will be many more publications telling results of other ongoing projects."
DSM100-2022-OCT,Topic 8: Robot Scientist part 2,Lecture: Topic 8 summary,https://learn.london.ac.uk/mod/page/view.php?id=96420&forceview=1,"[music]-In this topic, the concluded considering the case study of the robot scientist system, it is a complex system, possibly the most complex system in existence. We represented it as a multi-agent distributed system and considered the functioning of the key agents, a hypothesis generator, an experiment plan, and an analysis of experimental results agent. All these agents require access to underpinning data and knowledge. The agents have to communicate with each other to pass input-output information and also, they have to communicate with human scientists to output experimental data and report their conclusions. The robot scientists are becoming more and more sophisticated, capable of running more experiments and discovering more new knowledge in several research areas."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Game player case study introduction,https://learn.london.ac.uk/mod/page/view.php?id=96425&forceview=1,"[music]Welcome to the Game Playing AI case study. Now I'm really excited to bring you this content, because for me, it's been a really fascinating learning journey, learning all about the current technology that's used to play games using AI, and applying my past knowledge around neural network sand reinforcement learning and other things to this new topics. I've really enjoyed learning about it. I hope you enjoy learning about it from me. First of all, what I want to say is, what are we doing here?We're going to be looking at the 2015 research paper from Deep Mind, where they develop this system called Deep Q Networks, and they built a playing agent using this technology, which was able to get human competitive levels of game play technique across a range of different retro video games, specifically Atari. Now, if you don't know what Atari is, the Atari 2600 was a video game console from the 1980s, which was really popular. One of the first consoles that had games like Space Invaders, Asteroids, Breakouts, and all very classic, very well-known iconic games. It had simplified versions of the arcade game sand loads of people have them, and its great fun. Some of the games are really challenging and very difficult to learn to play using AIs. The thing that made this paper interesting was that actually they only fed to the AI with the graphics on the screen. The idea is that the AI would learn how to play the game just by looking at what was on the screen. This is part of the research trajectory that's in process in AI research at the moment towards generalized AI, where you can get things which can just learn how to do a task without being given really specific detailed information about that task. That's exactly what we're going to be developing in this course, a generalized AI system, which can play a range of video games without being told anything about how you actually go about playing those games. Yes, it's really interesting. I'm going to go now through the different weeks of content, which is of modules or weeks of content, which we have for you. The first module is a history module. We're going to be looking at, of course, Deep Blue, the chess playing system from 1997. We're going to go further back than that, back to the 1950swith Shannon and Turing working on very early chess playing systems. We're not going to go into huge amount of detail. We're just going to get an overview of some of the milestones that are reached, and we're even going to get into the current century. Wow, and look at some of the latest work around video game playing, and actually the chess playing and the checkers playing and stuffis ongoing work, and also poker playing as well. Lots of interesting systems to look out there. That's the first section of the course. In the second section of the course, I'm not going to try and scare you off with a horrible equation [chuckles]. Don't worry, but what we're going to do, you see that equation up there?If you're like me, you might see that and find that a little bit intimidating, or at least I did when I was doing my master's course, and some of you are going to be doing bachelor's or whatever different courses. You might find that equation looking pretty gnarly to you. Now what I'll say is when I studied neural network sat the master's level, I had a tutor, who's a brilliant tutor, really encouraged me to dig deep into my knowledge and learn a lot more. He used to throw equations right on the board. The whole lecture was just like a massive set of hand-drawn equations on the board, and now I'd just be sitting there saying, I don't know what any of those equations do, but the way I learned what they did do is I got the textbook, and I went home and I spent ages picking all the equations apart and finding out exactly what every term in the expression meant and how that it was actually a neural network that could learn something. What I'm going to do is I'm going to show you partof that process in this course. I'm going to take this, for example, which is the loss function. This basically says how an AI agent can look into the future and predict what's going to happen and then therefore know whether it's doing the right thing or not, that kind of thing. We're going to take this and break it apart and loo kat every little element of it, and then further work into the code and see how each bit of this equation ends up as a mechanical piece of code, if you like. That's what we're going to be doing in the second section of the course, formalizing how an AI can go about learning how to play a game. In the third section of the course, what are we going to be looking at is tooling. What did we mean by tooling?These are the tools that we're going to be using to implement this DQN agent. The first tool we're going to be looking at in detailis the OpenAI gym environment. This is a system which allows you to essentially present your AI system with a unified interface with which it can interact with a whole bunch of different tasks. For example, there's a 2D physics engine task over there, which is trying to learn how to walk along with that ED-209 Robocop creature. Then that one in the middle is actually our own custom gym, so you can make your own environments. That one we programmed in Pygame because we couldn't show the original Atari games because of copyright reasons, so we made our own one, but it was fairly easy to wrap that upand make it visible through the OpenAI system, so we learned all about that. That third one up there is a Lunar Lander, which is another example. There's loads of ready-made gyms in OpenAI gym. It's going to be a really great tool for you to learn about in general, anyway. I'm sure you'll enjoy it. The second tooling, now time for the flashing lights. Watch out if you're sensitive to flashing lights, but basically what we're looking at here is a neural network. What we're going to be learning about is the Keras neural network library. Now this isn't a neural networks course. We don't have time to teach you everything you need to know about neural networks, but I'm going to give youa crash course in neural networks. Specifically, I'm going to go into detail on the convolution al neural networks, which are a particular type of neural network which is very good at processing images. What we're looking at here is an image of what a real neural network. Once it's been trained, what it sees when it's playing a game. We're feeding the neural network with screenshots from the game, and this is what it's turning them into. It's learned how to extract salient information about the game and use that to then decide what to do. You're going to learn all about how that works. Then the fourth section of the course, we're actually going to take the formalism which we developed in week two or the second section, and we're going to express that into code. We're going to see how equations like this one can be broken apart and turned into mechanical Python code, so you can really geta good understanding. I don't know about you, but often when I see equations, I prefer to go and look at the code because I read the code, and I can understand mechanically, oh, those numbers are going over there, and those numbers are being turned into those. I can see mechanically what's happening so that helps me understand the equations. What we'll do is we'll be going backwards and forwards between the equation sand the code, and getting a really good understanding of exactly what this thing is doing. Also in week four, we'll actually see the final neural network and the agent system playing a game. In this instance, it's not playing it brilliantly well, but you can see, it's certainly tracking the balland getting to bounce it off the bat and scoring a few points there. I'm going to give you some pre-trained neural networks, because it takes up to 48 hours or even more to train one of these networks, and actually you may not have access to sufficiently powerful hardware. What I've done is I've trained a few different example networks, including one that you can run to play the original Atari Breakout game from the research paper, which is very good at playing, but also one that plays our clone of the Breakout game, which didn't go quite as well, but it still certainly knowshow to play it a bit. We're going to see both of those, and I'll give you the pre-trained model, and you will be able to run it on your own machines without needing too much power to do that. It should run on regular type of laptops. We're going to give you that, but also show you how you could have trained at yourself and talk about how I went about training it. In the final section of the course, I'm going to be fighting with the machines, because we're going to be talking about the state-of-the-art in game playing AIs, especially thinking about how good are these things at playing games, and then we're going to be thinking about the ethics of this. We're going to be thinking, is it ethical?What are the impacts that building superhuman game player AIs?What is the effect of that on people who play games?For example, Lee Se dol, the Go player, who was beaten by the Alpha Go system. He stopped playing Go after that. Was that positive or negative? I don't know. We're going to be thinking about those ethical question sand thinking about what we're doing when we're building these AI systems, whether it's the right thing to do, or whether we should be taking certain things into consideration. That is the end of the course. Just to summarize, really looking forward to seeing your reaction to this course. This material, I think, I've really enjoyed making it, hope you enjoy it, and hope that you learned a lot about how it's possible to build real game playing AIs using Python and Kerasand OpenAI gyms, and also hope that you gain some real confidence about looking at these research paper sand breaking those equations apart, because I'm going to show you how I go about doing that. I hope that really is effective in this course, and that you enjoy all of the learning that we have for you."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Week intro,https://learn.london.ac.uk/mod/page/view.php?id=96429&forceview=1,"[music]Welcome to week four. It's finally time to build and train this AI game-playing system. Let's just look at the learning objectives, and then we'll get some previews of what we're going to be seeing this week. First of all, at the end of this week, you should be able to explain how the DQN agent architecture and learning systemis expressed as a working program. Secondly, you should be able to evaluate the performance of the DQN agent. You need to be able to know how you can test whether this thing is actually working or not. Finally, I'm going to show you how to deploy a pre-trained neural network and use it to play games in real-time. That's important, because you may not have the facilities to train this thing, but I do and I've trained this thing for you. I'm going to show you that once someone's trained the network for you, it's easy to implement that and deploy that in your code. Okay, so what about the previews? Well, first of all, the big thing we're going to be doing this weekis we're going to be looking at code. But, really importantly, it's really vital that you make the connection between the sort of maths equations that are in the paper and the actual implementation in code. I'm going to very carefully make the connection between this complicated ""loss"" function and exactly how that's implemented in the code, which line relates to which, so you can see that the code really directly implements exactly what's in that equation. Then secondly, what we're going to see is the AI playing a video game. You can see herea reasonably well-trained AI playing Break Wall, which is our version of Breakout. Yes, it's not doing too badly. It's scoring a few points. Okay, so that's what we're going to be looking at, and eventually you will have this running on your system at the end of this week. Okay, great. Welcome to week four. I'm looking forward to seeing how you get on with the actual implementation and see if anyone can train a better model than I can to play Break Wall. I hope you enjoy seeing the full implementation of the DQN agent."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Overview: playing games with AI,https://learn.london.ac.uk/mod/page/view.php?id=96430&forceview=1,"[music]In this video, I'm going to try and lay out some of the theme sand flavors of the AI research field, which relates to playing games using AIs. We're going to cover the question of why we should play games using AI sand what that means?We're going to think abouta few different angles on it. We're going to consider the commercial angle. I'll give you an example of an interesting commercial requirement of doing this. We'll also look at various research theme sand consider certain trend sand things like that in the field, especially looking at Moravec's paradox, and how that's developed into this idea of general game-playing bots. What do we mean by a game playing AI?Essentially, obviously, maybe AIs that play games. It could be a program that can play chess, a bot that can play Doom, but what is it not?For our purposes, we consider AIs playing games that a human would play. In other words, we're mainly interested in AIs playing games where they can potentially compete against human beings, or their games that human beings find really interesting, or that human beings find very challenging because it gives us this opportunity to test the progress. Where are we at with AI?That kind of thing. Another reason or a reason why we want to play games?This is from Russel land Norvig's Artificial Intelligence, sort of a classic textbook. They say that games are interesting because they're too hard to solve. That sounds like quite a simplistic statement if you like, but actually, they have a whole chapter in the book about itand what they're opening up to us hereis the fact that games contain all kinds of really interesting, really hard to solve problems. Things like, if you're playing a fast action game, how do you analyze the visual input quickly enough so that you can actin an effective way?If you're playing a more long-term strategy-type game, how do you make sure that you can developa strategy in it and adapt it dynamically as conditions change?There's all kinds of classic problems from AIto do with hidden information, planning and visual analysis, all kinds of things that come out when you try and play games with AI. Yes, it's really an interesting area for that reason. Now, this is an interesting paper. Before we look at the quote, I'll just tell you what the paper's about. It's called, When Are We Done with Games?What they're doing in this paper, is from 2019, they're making the argument that we're not done with games yet. Because over the last very few years, very recently, it's become apparent that video games can be played very wellby AIs in particular board games, as well Alpha Go, playing Go, playing chess. AI is brilliant at playing games, and so what they analyze in this paperis how fair those competitions actually are. How fair was it that the OpenAIs Dota Five was able to beat a team of professional players?How fair was that competition?Was it actually as fairas a human-to-human competition typically is?They argue that many of the competitions are actually not that fair. It's a really interesting paper, it gives an interesting perspective now, but in terms of a motivation for playing games, what did they say there?They say, ""Researchers have consideredStarCraft to be the hardest games for computers to play, which ultimately suggests that when the game is final grand challenge for AI in games before tackling real-world problems. ""I said that really fast, but you can summarize it as this. They're basically saying, ""When we're done, if we've solved Star Craft--""and as we're building on what I was saying before from the Norvig and Russell quote. Once you've solved these games, if you can solve playing Star Craft, you'll have to have solved all kinds of other problems because you ha veto act in a complicated, changing situation to achieve certain goal sand collaboration and competition. There's all kinds of things going on in there, which if you solve them, you've definitely solved a load of problems, which would be very useful in the real world once you try and apply those AI algorithms outside of the game. Next, how about this paper?This is another angle on this. This is a paper from 2018, which bravely claimed--so asked the question, when will AI exceed human performance?Now, you should never ask scientists to predict when things are going to happen. They don't like doing itand if they really confidently predict something, they probably don't know what they're talking about or it's already been solved, and then they're hoping you haven't realized yet. Take this with a pinch of salt, but it's really an interesting question. Maybe the most interesting part of this paper is, what are the areas of AI that they're particularly looking at?Because it gives you an idea of what were the interesting areas of AI at the time that felt that were being pushed really hard or weren't being pushed at all. It does give you a map of the field. They asked experts for estimates when AI would meet human levelin various tasks. One of the tasks was, when will AI outperform professional game tester son all Atari Games using no game-specific knowledge?It's from 2018, a lot of people said 8. 5 years. I think the mean prediction was in 8. 5 years from 2018, we'll have a general Atari Game player. I find that a little bit unusual given that in 2015 Deep Mind had already published the Atari DQN player, which we are going to be looking atin the next few weeks, that was able to play quite a few games pretty well. I'm surprised that people said eight years from then, but there you go. What's telling is that in a general paper about AI, Atari Games came up as a key thing to be looking at. It just shows you that playing these retro games has become a really important benchmark in the field in the 2010s, through to the present day. Actually having said that, in 2020, Deep Mind appeared to have solved that exact problem. Exactly 2 years after a prediction of 8. 5 years was made. That's why you should never predict anything. Moving on, what about the commercial interestin AI game play as well?Of course, if you know video games, you know that there's loads of automated systems in video games. There's non-player characters which react to what you do. There's teammates and there's enemies and all kinds of things, which appear to exhibit intelligent behaviors. Those are certainly examples of AIs playing games, but typically they're adversaries, or they've quite limited. What we're more interested in, as I said at the beginning is, AI's doing things that humans would do in the game. Instead of the AI being a part of the game, it's more to do with the AI actually playing the game as if it was a human. Like triggering their control, the controls that a human might send. What about that?This is another interesting paper, all these papers are interesting. That's why I've picked them. This one is a case study from people who work at EA, programmers from EAshowing how they're using AI to play test games, and the quote is, ""Relying exclusively on play testing conducted by human scan be costly and inefficient. Artificial agents could perform much faster play sessions, allowing the exploration of much more of the game space in much shorter time. ""Basically, if you can train an AIto play a video game, you can then run it offline. It can play, you can have a thousand instances of it all running at the same time, all playing the game and trying to find all the bugs. It's really an excellent way of play testing games, and you'll see that the AI can find all the little flaws in the game and exploit them. Also, actually, in this particular paper, they were talking about a role-playing game and they wanted to make sure that the balance of difficulty, depending on which role you chose at the beginning of the game, they wanted to make sure that as you worked through the game it was equally as difficult or easy to win, and that there weren't some roles that were disadvantaged compared to others. In fact, they did find that some of them were, and they were able to fine-tune them as a result. That's an interesting one to read as well. What about the role of competitions?Competitions are really important in video-game-playing AI systems because they allow people to compare the performance of their system. Now, let's look at these quotes again, this is from Togelius et al, where they were talking about the 2009 Mario AI Competition, which is kind of an early competition for playing real video games using AIs. They said that competitions have the role of providing software interface sand scoring procedures to fairlyand independently evaluate competing algorithms. Fairly and independently comparing that's great, and the competitions motivate researchers. Motivation is good too, because--excuse me [coughs]. They motivate researchers because existing algorithms get applied to new areas and the effort needed to participate is less. Basically, competitions make it fair because everyone's playing the same game. Everyone's plugging into the same simulation, so it's really just comparing how good their AIs are. If everyone's got their own version of the AI of the simulation of Mario, then it's difficult to compare. It's just like benchmarks in any other area of AI and machine learning. Also, it's very motivating It is easier to get into because if someone's already built the simulation, you just need to plug into itand build your AI rather than mucking about trying to get a simulation to work. I now want to switch modes a little bit and talk about an important trend in the research, which is more of Moravec's paradox. Came from Hans Moravecwho's a very interesting writer on AI. The things that humans find hardare often no problem for AIs, well, not a problem, but they can be solved by AI sand things that humans find easy, not so much. I'll give you an example. I can't beat Garry Kasparov at chess, but in 1997, IBM were able to do that with this amazing, super customized computer that we're going to talk a little bit about later. Whereas when I get up in the morning before I'm even awake, I've already got the kettle on and I'm already brewing my first cup of tea. The IBM machine didn't stand a chance of doing that. It had no arms, it had no visual apparatus it just, it was so specialized it couldn't do anything but play chess. All it could do is evaluate gazillions of chess moves really quickly. That's what we're getting at here is this idea that the problem is that if we make these incredibly specific algorithms that can only do one thing really, really well, then we're not really creating an AI that's anything like a human-AI or a general AI. That's led to this trend in the field of having game-playing AIs that can play multiple different games, so and they're generalists. That's trying to address Moravec's paradox. Here's an example of general arcade playing games. The DQN which is the one we're going to be looking at, is one of the earlier kind of retro video game general AI, so the same system has to be retrained for each game, but the same basic constructs, the same neural network, the same algorithm can play lots of games. Agent 57 is a state-of-the-art example of something that could play all those games. Just to say, there's a whole community of people who've been looking at this for many years. It isn't just Deep Mind, although it's tempting to reference them because they have these high-hitting nature journal papers but actually there'sa whole community that had been working on these ideas and developing them for a long time and I want to acknowledge those with those two links. Retro arcade games are okay, but what about long-term planning?Actually, if you look into it, some of those Atari games do involve quite long-term planning because you might have to search around for ages to find something and then come all the wayback again to somewhere else to use it like a key or something like that, but nevertheless, compared to something like a Dota or something like that, or Star Craft, then there is much more planning involved, and those are maybe more challenging games. The question is when will it end?Let's bring up another quote from the, when are we done with games?""An ultimate goal that would demonstrate that an AI system can fully master a game, beyond the extrinsic factors of human versus human competitions would be to allow anyone to play against itover a long period of time. ""That's what they're saying. They're saying, when AI will be able to play games as well as humans, when we pretty much try and matchas many of the factors as possible, we basically say, okay, there's a game-playing AI online, and you can just go and play it whenever you like. Then that's when we'll be confident enough to say we've solved it and you can play the game under any conditions against any other player and it will still win. Indeed that's exactly what the open AI team have done. They put Donta 5 online and people were able to go in and play against it but again, it was a bit constrained. The reason they're saying that it was constrained, it could only play in certain circumstances, certain versions of the game, that kind of thing. Also, Deep Mind, the Alpha Go, or Alpha Star online so that people could play against that as well. Anyone could log in and play against the game player. There have been efforts to do this, but according to these researchers at least, they haven't been entirely fair. That's just giving you a bit of a flavor of this field of game-playing AIs. I've thrown a whole bunch of different idea sat you there just to get you chewing on this and thinking about it. First of all, we're thinking about why we play games with AI?What's the motivation and then we're thinking about the commercial aspect to this, just briefly mentioned that idea of automated play testing. Going beyond just having automated AIs to play against, but more to do with game testing. Then I pulled out a whole bunch of different research themes, things like the idea of more of Moravec's paradox where we're looking for generalized AIs, and competitions, and benchmarks, and things like that. Thinking about when we're going to actually decide, yes, we've done this, we've solved this. When do we say we've finished and what do we do next?In this video, I've just been giving you a nice wide intro to the area of video game and game-playing AIs."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Some AI game player milestones,https://learn.london.ac.uk/mod/page/view.php?id=96432&forceview=1,"[music]-In this video, I'm going to be presenting a few milestones that have happened in the AI game, playing research. In summary, we're going to talk about the early years and then we're going to go through various things, Computer Olympiad and we're going to look at a few things that have happened in various decades just to see where we're at with AI game players. Now, I'll just say at this point that I'm going to show you loads of different papers. What I want you to do is, if you play any of these games or you're particularly interested to find out how any of these systems work, then I encourage you to go off, find the papers, read around, find out how the algorithms work and follow your interest really. I'm just laying out some of the milestones here for you to follow off on yourself. Okay. Right, so the early years. Now, some of these things I'm talking about in this video are derived from this really nice research paper where Schaeffer, who actually worked on game plays, AIs that play checkers or drafts as we know it in the UK. He talks about some of the early years and history of early game players right up to quite recently, actually, it's the 2001 article. He covers that first 50 years really of research. I've found some of the research articles that you'll see in that article. The early years, in 1950, Shannon actually defined what's one of the very first formal descriptions of a chess playing algorithm. Okay. That's Shannon. If you've heard of the name Shannon before you might remember a very famous information theorist who talked about how it's possible to represent information in binary, how much resolution you need to represent different types of information and different types of data. A really important figure in this who also happened to just knock out the first chess playing algorithm. Apparently, Alan Turing was aware of this algorithm and implemented it by hand and hand executed it because he didn't have a machine or resources to run it all of the time and was able to show that it could play chess, but it wasn't very good. Well, probably Alan Turing is pretty good at chess though, let's face it. Okay. Moving on, in 1952, we saw one of the first checkers or drafts playing systems, which as I said is Schaeffer's area. This one, as you can see from the title, logical or non mathematical programs in proceedings of the ACM. This is, I guess, it's differentiating itself from all these mathematical algorithms saying, ""We've got an algorithm that actually does something, plays a game. ""Okay. Moving on. 1979, so I've actually skipped quite a few years there. Feel free to go and find out what happened in between, but I'm just trying to find some of the key milestones here. This is 1979, Backgammon. Now, I've always found backgammon quite challenging to play and rarely win. Apparently it's quite easy for machines because in 1979, they were able to beat a world champion using a computer program. Maybe I should go and read that article and I found out a bit more about how you can play Backgammon. Obviously, that's a game that has more chance in it as well, which is interesting. Then just, I wanted to mention The Computer Olympiad because this is an interesting thing that popped up in 1989, created by Daniel Levy in London. It's still running today. This is a computer versus computer competition where people submit algorithms, which will play a range of different games against other computer algorithm sand they rank them. It's quite an interesting thing to go and dig into and potentially, look at some of the algorithms because a lot of them are open source and lots of different games are played. Going back to checkers again, in 1992, this is where Schaefer comes in and published a world championship caliber checkers playing game system. That's another important milestone, so that you can see we're transitioning into this idea that we are playing and beating world champions as of the 1980s and 1990s. Let's go on to the big one, which is Deep Blue. 1997, Deep Blue beats Gary Kasparov. There is a picture of the Deep Blue Computer as far as I'm aware that I found on Wiki comments there. What can we say about this?Well, it is worth just saying it was highly custom hardware. It had lots of processes. It's a highly parallel system, but it also had specialized chess move computation processes. It could just run through all kinds of different moves really, really fast, massive search algorithm really to search for the optimal and search through the massive tree of possible moves depending what the other player did as well. Yes, it's a really demonstration of super computing power and what you can do with specialize hardware and really a very important and famous milestone. Okay. Moving on. Scrabble was solved, if you like, or at least world championship player, standard Scrabble AIs reported in 2002. What's interesting about Scrabble?If you don't know Scrabble, you have to make place words onto the board and you get given a set of letters and you have to make up words. You don't know what the other players have. You don't know which letters they have, you might have some idea based on what's gone out on the board. As the game progresses, you have more idea about what letters they might have. There's also lots of different ways to score points as well, so it's a really varied point scoring pattern. That's really quite a complicated game, which is probably whyit was until it wasn't until 2002 that this world championship player was made. Then, going back to Shafer's work on drafts and checkers, 2007 is completely solved checkers, which means that it plays the best possible game every time. I think what the definition of solved is and you can go and read the paper if you want to find out more, but there's no point in playing against an AI checkers player anymore. In 2012, just bringing this in as a slight variation on the theme so we've seen a lot board games. At this point let's just think about video games. 2012 or 2009 was actually the first Mario AI competition. It was a small community of people that were getting together at a conference and competing their AIs against each other to see which one could do the best game of super Mario Brothers. I've got a picture of plumbing there because I'm probably not allowed to show anything closer to an actual image of the Nintendo Mario game, but there are plumbers and that was the theme, this graphical theme of the game. This ran from 2009 to 2012. They give some reasons in a later paper as to why it ended. One of the reasons, I think they were getting a bit of heat from Nintendo. They weren't really able to use the graphics and things on their websites, and also other competitions were inspired by competition, more general competitions developed. Actually the community developed as a whole of people building AIs to play games. That's an interesting thread to follow up on as well if you're interested. Okay, let's skip forward to 2015 and look at a big milestone in the Atari video game playing systems. In 2015, this is the article we're going to be looking at in lots of detailin the next few videos so we are going to take a really deep dive into this, look at all the equations in the paper, try and pick them apart, and then look at an implementation in code where we can see exactly how it was done and match that to the paper. We should get a really strong understanding of how this one works. This was interesting because it was published in Nature, which is a really important science journal where a lot of the big articles across the range of science are published. You can see that it's can consider very important topic. As we saw in the previous video, I talked about motivations for this. One of the motivations for doing this work is that is considered a stepping stone to general AI systems. If we can make a single AI that can play a whole range of different games or learn to play a whole range of different games, then we are then in a good position to then start looking at general AIs, maybe looking at more real world problems and solving other problems. It's really a stepping stone, but it's a very important one. You can see Deep Mind is a multibillion dollar company as I understand it, which is part of the Alphabet Group along with Google. People are spending a lot of money on this and a lot of computational resources and brain power to try and solve this problem, so clearly it's definitely seen as a very important problem, 2015. Moving on, then the same company, Deep Mind solved go if you like, or not solved it, but certainly came up with a world championship standard famously player in 2016. There's a great video documentary about that, which you can find on YouTube. Then, switching back to some other types of games, so I don't play a lot of poker, which is why I've still got the house. This is considered the hardest version of poker apparently, so heads up, no limit Texas Hold'em Poker, 2018. That's when an AI was able to play this game to the standard and to beat world championship standard players. This is really interesting because you think about poker, there's a lot of hidden information in poker, so you don't know what cards the other people have. You don't know which cards are still in the deck necessarily. You don't know whether someone is bluffing, when they put their big bet down, whether they really have good cards or whether they're just trying to beat you out. It's a really interesting game for AI's to solve and it's a very detailed description in that paper that you can read. Again, look at the journal, it's science. It's science, one of the big journals in science. Clearly, this game-playing AI stuff is considered a very important element of science as a whole. Let's just skip on to almost present data. 2019, OpenAI reported on their Dota 5, OpenAI Five bot, which was able to compete with world championship Dota players in constrained circumstances. Remember, that research paper called, When Will We Be Done With Games, where they talked about the fairness of these competitions, and suggested things that could make it fairer. Interestingly, yes, they do follow some of those suggestions in this research, for example, putting it online and letting anyone play against it. Moving to 2020, and this is the final paper we'll look at. This is another paper from Deep Mind, where they actually were able tonot just play unlimited, I think they played 47 or 49 games in the 2015 paper. In this one, they played the full set of 57 Atari Games, which is considered the benchmark. They were able to show that their agent could out-compete the scores or beat the scores of human play testers, the professional game players who they basically got to play all the games as well as they could. This system was able to out-compete them. It's above human performance across all the games. Some of the games are really difficult, have very long-term rewards for the play. You don't get any points until you've done a whole load of really complicated actions, exploring a load of cabins or whatever. There's some really difficult games, which don't indicate much to you as to how well you're doing, unless you really have some understanding. It's a very important paper. As I say, an important stepping stone towards this idea of general AI solving this more of a paradox or at least addressing it. Okay. That was just a bit of a brain dump of loads of important milestones, interesting milestones in game-playing AI. We've been through various decades, and talked about a whole bunch of different games, and when AI met world standard or World Championship standard in a lot of those systems and when the first systems appeared, as well. Okay, so in this video, we've just been looking at some important milestones in game playing AIs. As I said at the beginning, I encourage you to follow your interests. You don't have to go off and learn about every single one of these game-playing AIs. What you should do though, is if you play a game and you think, ""Oh, how do they build an AI that plays that?"" Go and read the paper and try and make sense of it, try and pick those equations apart. That's what I'm going to be doing for the Atari 2015, Atari DQN agent in the next few weeks for you."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: How might we build a game playing AI?,https://learn.london.ac.uk/mod/page/view.php?id=96436&forceview=1,"-In this video, I'm going to be asking the question, how do we go about developing a game-playing AI?I'm going to start out by giving us the context, which is our big aim over the next few weeks of what we're trying to achieve. Then, we're going to be thinking about how a human might play a game and then try and lay that problem out for an AI system. Then we're going to find out about what reinforcement learning is as we get to the end and that's the technique we're going to be using. What's the context?Well, we're going to be looking in-depth at the DQN agent, which is the deep mind agent from 2015, which can learn to play Atari video games. The question is why am I doing that?Why am I using that agent which is from 2015?Why not the 2020 one?Why don't you show us the newest one?Well, the reason is there's a certain purity to the DQN agent, which means we can probably fully understand it in the time that we have over the next few weeks. We're going to take a really deep dive into this agent and find out formally how it's been specified looking at all the formulae and the expressions. Then, look at code implementation of that and hopefully get it to work and see how it operates. Whereas the later versions, I've found out actually they've bolted on a load of extra features which make it perform much better, but it loses some of the purity of the original agent, which I think is a really impressive piece of work. That's what we're going to be looking at. We're going to be looking at the DQN agent. It plays retro video games. Let's go back and say, how do we go about doing this?How do we go about writing an AI game player and especially one that can play retro video games?What I might do is start out by playing the game myself. We're going to play this game, which we've built, and I'm going to reflect on what I'm doing as I'm playing it. Just to say that we've built our own version of Breakout because we can't really show the Atari one for copyright reasons. Here is the game, which we programmed in Pygame. There's a ball zipping around the screen there. I hit the ball on my bat and every time I hit it, it gets a bit faster. Eventually, I may have to break through the bricks. You get the idea and it's game over. Let's reflect on what I was doing there, and just to say, I'm not sure if the original Breakout actually accelerated the ball in the same way and whatever, but it just gives you the general idea. The questions I was asking myself, how do I get points?Obviously, games like this, you're trying to get points. That's the first thing you're thinking about. Then which way is the ball going?I was looking at where the ball was zipping around on the screen so I could get my bat underneath it. Then I started thinking about the bricks like, ""Oh, where are the bricks?Where do I need to get the ball to go?How can I get the bat in the right position so the ball bounces off in the right way and I hit the bricks?"" These are the things I was thinking about. The question is that that's me, but what about the AI?What is the challenge we're setting the AI here?Well, the way it's built in DQN in this general Atari game-playing world, so I have my AI and the idea is to give it the minimum, most realistic information if you like. You want to make it fair, so it's given the similar information to what a human would have. I think that that's one of the ideas. Let's just see. We have the game, which has got the bricks and laid out there, and then there's a ball and a bat. What we could do, we could say, what we want to do is we're going to tell it exactly where the ball is. We're going to tell it exactly where the bat is, and we're going to give itall the X and Y positions of all the bricks, but really the human being, they don't do it like that because the human doesn't really have that. The human just sees the screen. They just got the visual stimulation of the screen. That's what we do with the AI. The first thing the AI gets is an input, and literally the pixels of the screen. So the pixels on the screen. Because we don't want it to have to find the score because how's it going to know what the score is, we cheat a little bit and do tell it the score and if the game is done. If it's lost or not. Then the output is just a left, right, or nothing. I'm going to use the Northern English expression for nothing, which is nowt. It's basically the AI as its input, it has a view of the screen. Literally, the RGB values are the pixels on the screen, and then it has the score. It has a numerical score, plus a true, false whether it's lost or not. Then it outputs a left, right or nothing, or nowt. That's the idea. We've tried to create something that's fairly similar to the way that a human might play the game if you like. Now, how are we going to design this AI?Well, yes. Maybe what we could do is just create a big input-output data set and train a big network, but what does that data set look like? What's it going to look like?Is it going to going to have every possible screen that it could ever see?Is it going to say exactly what the correct move is to make in every possible screen?How do we create that data set?How would it know that in the first place?Do we meticulously label all the millions of possible screens and different pixel combinations and label them all as, ''Okay, you should go left here, you should go right here?'' It's just not really feasible to create that data set. That's one of the features we're going to be seeing in the algorithm. Look, what I'm going to do now is I'm going to give you a definition of reinforcement learning, and hopefully, now you thought about the game and what the possibilities are, you'll see that it makes sense. Reinforcement learning is how agents can learn what to do in the absence of labeled examples of what to do. That's exactly what I'm talking about now. I'm saying it's not really feasible to create labeled examples of all the possible screens plus the correct move to make in every screen. We can't really create that data set. It means that reinforcement learning with this definition, how agents can learn what to do in the absence of labeled. That's great. Secondly, imagine playing a new game whose rules you don't know. After 100 or so moves, your opponent announces you lose. This is reinforcement learning in a nutshell. That's Russell and Norvig's chapter on reinforcement learning. It's pretty much what we are looking at. We're playing this game and these pixels are flowing into us and there's this score, which may or may not go up. Every frame, we get a new shot of the screen. At some point in that, suddenly the game ends because we moved the wrong way. That's what reinforcement learning algorithms are aiming to solve. That kind of situation where you've got this mysterious game, you make a bunch of moves and then you lose most of the time. The conclusion is we do need reinforcement learning. It seems based on Russell and Norvig's description, it seems like a very appropriate set of algorithms to be investigating. Next week, what we're going to do is we are going to be formally describing the reinforcement learning problem in terms of the actual equations that are in the paper. For now, before we head off, we're going to think about states, actions, and rewards. Those are the key elements, if you like, of a reinforcement learning algorithm. The states are, think about that, if you're looking at the screen going back again to this, the states are really the pixels coming in and the score or the done. That's the state, if you like. Then the actions are those three actions we have there: left, right, or nowt. The rewards, I suppose those are the points or the done, actually. The state is really the screen. The rewards are the points or the done coming inand the actions, the left, right, and done. What we're going to find out is how we can formalize that in terms of reinforcement learning and then how we can build algorithms, which can operate on that formalism to solve the problem. That's kicking off us thinking about what game playing AIs and how they might operate, especially reinforcement learning type of game-playing AIs. We've seen the context, which is that we're aiming to get a really good understanding of this DQN agent, which can play Atari video games. We've looked at how a human plays a game and thought about what they can see, what they can do, and what they're trying to do. Then we've tried to outline the same problem, but for an AI where it has limited access to what's going on in the game based on just having the screen and so on. Finally, we've made an argument of why reinforcement learning looks like an appropriate set of techniques to use in this context. In this video, we've just been introducing the concept of how we might go about building a game-playing AI for the first time."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Weekly outro,https://learn.london.ac.uk/mod/page/view.php?id=96440&forceview=1,"Excellent. You've reached the end of week one of our game playing AI case study. Because you finished this week, you should now be able to describe historical examples of game playing AIs, you should be able to evaluate the motivation for researchers investigating game-playing AIs, and you should have started to consider how one might implement a game playing AI. Well done for reaching the end of week one. Heads up for week two, we're going to be digging deeper into how we can formally describe game playing and game playing systems so that we can later develop that into a working system."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Introduction to week 2,https://learn.london.ac.uk/mod/page/view.php?id=96443&forceview=1,"[music]Welcome to week two, you came back [chuckles]. What are we going to learn about this week?Well, this week we're digging into the actual mathematical or formal underpinnings of how this Atari game-playing agent is implemented. Here's what we're going to be looking at. First of all, we're going to learn how we can describe a gamein the formalism of a Markov Decision Process. We're doing real AI here because we're using words, formalism, a Markov, so we definitely know that we're doing real AI. Then secondly, we're going to describe what Q-learning isand why it's necessary in the context of playing Atari video games. Finally, we're going to explain the purpose of the key elements in the DQN agent and the loss function. We're going to break down this deep mind, Atari video game playing agent and we're going to understand exactly what it's made of and how it all works in a formal way before we go into the coding later in the course. Just some heads up on what we're going to see. This is the agent architecture. We're going to be understanding how the agent architecture works, how this simple diagram here is able to learn how to play video games. Then here's the sort of doom we're going to be looking at this. Don't worry, don't worry. I don't expect you to absorb that all now. I'm going to make my best attempt helping you to understand fully what everything in that equation means. I'm going to break it all apart and explain all the different elements of itand why they're there and how that means that we're able to learn how to play video games. Later we're going to actually break it apart and look at the actual code where we implement each element of this loss function. That's it. Welcome to week two. I hope you enjoy this kind of formalistic dive into the implementation of a game-playing AI."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Formalising the problem,https://learn.london.ac.uk/mod/page/view.php?id=96445&forceview=1,"[music]In this video, we're going to formalize the problem of playing games with an AI into the construct of states, actions, and rewards. First of all, we're going to find out about states and actions. We're going to find out about rewards. Then we're going to find out what a Markov decision process is. Let's start out with a little reminder. How can we create an AI that can play games?Most games involve an iterative process of observing and acting. Most games involve some positive or negative result. Think of an arcade game where you've got observations, you've got actions, and you've got a result. Remember if we've got the idea of an arcade game, we've got our observation, which is, in the case of breakout where the bricks are, where the ball is, where the bat is, basically, the pixels on the screen. We take actions, which are things like moving left, moving right, or doing nothing. Sometimes we get rewards, which might be a brick disappearing. We then get some points, so +1 point, or maybe the ball goes off the screen and we die, which would be maybe a negative reward, something like that. That's just a quick heads up on what we're talking about. The state is the current view of the game, in breakout, and the actions are move left, move right, or stay still, as I just said. The reward is one point each time you hit a brick, or game over if you miss the ball. Actually, there's a pretty heavy negative reward if you miss the ball and it goes off the screen. What we can say is that we can formulate the game in terms of states and actions. The player observes state S, and they take action A, and then they observe the next state; primals, next state. What we say is, the state, so basically that's the observation, that's state. Then the action is the action we take, so that's the action. Then it leads to a subsequent state which we, basically the next screenshot, which is maybe the ball is further down or whatever, so that's the next state. That's what we mean, so state, action, and next state. Sometimes it's a probabilistic thing. We can say, there's a probabilistic relationship between one state and the next. You can take the same action in the same state but it would lead to two different neck states. What do we mean by that?How does that work?Well, let's say we've got a view of the screen. Let's just simplify it now, the ball is there. It's basically going that way, which we don't know by just looking at the screen. We don't know which direction it's going in just by looking at the screen. The next state on that one would be that the ball is further that way. Let's say the ball happened to be going that way and in that case, then the next state would be the ball being further that way. That's what we mean. We could say based on our observation there'sa 50/50 chance because it's equally likely to be going either way. We can say probability is 0. 5 there and 0. 5 there, that and that is prime, that is prime. That's what we mean by it being probabilistic. We express that formally by saying probability of state prime given state and action. That's what we call effectively a state transition matrix. Okay, what about rewards?What we can say is, when we've got our state, action, and next state construct, there's a reward associated with that, which we give the letter R, and state which says taking action A in state S leads to the following state and a reward of R. Sometimes we formalize that as a reward function, so we say there's areward function that operates over the state transition matrix, if you like. What we mean there is, let's say just for the sake of argument, I'm just going to say the bat is there. If I've missed the ball, basically, the ball's heading off that way, then I might get a reward of -1 because I've basically lost the game. If I happen to hit a brick, then maybe I get a reward of +1. Then if I don't do anything, which is what happens in most-most frames nothing's going to happen, most times the reward is going to be 0. If you don't achieve any points in a given frame, maybe you have anegative reward for that as well, so that could punish inaction, if you like. If the thing isn't gaining points all the time, periodically, then it's going to gradually lose its reward, something like that. There's different ways to model the reward, but essentially it's either areward function or it's just an R that are part of the observation, if you like. That means what we have, with those parts, with the probabilistic state transition matrix, and this reward function, that's a Markov decision process, which is a well-known process in AI really. What we say is that a Markov decision process is a way of formalizing a stochastic sequential decision problem. Let's go through those words and make sure they're really clear. Formalizing is basically expressing something in a clear mathematical way, if you like, that can then be used to build algorithms and so on. A stochastic sequential decision problem; stochastic means that it has aprobabilistic element, and that's where the probabilistic state transition matrix comes in. It's stochastic because there's different-- based on what the current state is, it might go to other states probabilistically. Then it's sequential, and that's really important. It's sequential because we're going to have to make a whole series of decisions. Because we see a sequence of frames of what's going on in the game and we have to makea decision, each frame, as to whether we're going to move left, right, or stay still. It's sequential because we have to make a sequence of decisions. Then sequential decision problem, obviously, is a problem of making decisions. That's what we mean by stochastic sequential decision problem. A Markov decision process is another name for aformalization of a stochastic sequential decision problem. Just in case this helps you visualize that, the idea of a state transition matrix where we're moving around and we're gaining reward. You can see, sometimes a little reward pops up, and sometimes we just transition between states. That's what we mean by Markov decision process. The decision and this is what we're going to cover in the next video, the decision is, which action are we going to take?That's the decision we have to make, is what are we going to do. We've just been going through the basic elements of the Markov decision process, which is the states, actions, and the rewards. We found out that it's essentially a stochastic sequential decision-making problem and that's what we're dealing with. Now, in the next video, we're going to be looking at the action policy and Q-learning. In this video, we just introduced the concept of the Markov decision process."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Value functions and DQN,https://learn.london.ac.uk/mod/page/view.php?id=96447&forceview=1,"[music]In this video, we're going to continue with our discussion of how to formally model this decision-making process. In summary, we're going to be thinking about action policies no wand what does an optimal action policy look like?Then we're going to think about the concept of future reward and how we can express that in terms of a Bellman equation. We're going to see a big gnarly equation at one point in this video. Then we're going to think about what Q-learning doe sand what Deep Q learning does. Lots to cover, so let's get started. First of all, remember, we were talking about this sequential decision-making process. Well, the action policy tells us what to do in a given state, and it's normally denoted with the symbol pi. We say, ""Pi acts upon a given state to yield an action. ""In other words, so if the state is S, then we need to take action A. The policy is simply a mapping between states and actions. What does an optimal policy look like?Well, because it's a sequential decision problem, not just a one shotreward might come in the near or far future. That's a really important thing to understand. Normally, when we're doing machine learning, we consider the reward to be instantaneous. If you're training an image classifier network, so you've got a bunch of images and you've got a bunch of tags, like dog, cat, et cetera, you essentially train it to know, ""Okay, if you see this image, say dog. If you see this image, say cat. ""It's a one-shot thing. It doesn't have to then say, ""Well, you're going to see this and then you can see this. ""It's sequential problem. When we consider what the best action to take is, we have to consider, are we unlocking a load of possible future, other actions by doing this action?For example, think about breakout. If I engineer it so I can get the ball on the top of the bricks, it will bounce along and I'll unlock loads of future reward. In order to do that, I need to knock out the bricks on the side firstand I need to hit the ball on a certain part of the bat. I'm not getting much reward until way, way in the future. That's the problem with a sequential decision problem. When we're formalizing it, we have to take account of the reward we're going to get in the future. The Bellman equation allows us to formalize that. Let me bring this up on my editor because that is a pretty gnarly-looking thing. I'm actually going to explain why this doesn't really help us that much. Let's go to it. I think I've got it over here. Yes, there we go. Here is the Bellman equation. There are lots of different versions of the Bellman equation. This is the one that is presented in the DQN research papers. I've recreated it with my own LaTex code. Let's go through it and say what each of the bits means. Well, essentially, what that equation means is, it's saying that the value of a given action in a given state is based on the reward we might achieve in the future. That's what it's saying is if we assuming we're taking actions based on a certain decision policy. If we have a certain action policy, and we take the actions in that action policy going into the future, then this action we're taking now will yield this value. Value is reward going into the future. How is it expressing that?Let's go through each of the bits. This is the first bit. This is what we're looking for. This is the value function, it's saying the value of action A in state S is?Then what does all this stuff mean?Let's break it down. First of all, we've got this rt. That is the immediate reward we're going to achieve by taking this action. Remember, we've got this state, action, next state, and reward. That's what we're talking about there. It's taking account of the immediate reward, first of all. That's the first thing. That's what that is there. The next thing is, let me just get rid of that highlight, the next thing is this, these things. This is, again, processing some reward. We've got an r in there. What does it mean?That's a gamma. That's letter gamma. What does it mean γrt+1?What do you think rt+1 means?Bearing in mind, we're looking into the futuret+1, t, time, plus 1 into the future, right?What we're saying is, we're going to say, ""in order to work out the value of this particular action, we're going to look at the reward in the next time step, but we're scaling it by gamma. ""Gamma, this sign here, gamma is less than one. Imagine gamma is 0. 5, what we're going to say is, ""Well, that one, t+1 is times naught 0. 5. ""That is half of the actual reward. The further into the future it is, the smaller the scaling on the reward. If gamma is 0. 5, then gamma squared is 0. 25. We're only taking a quarter of that reward. We take half of that reward, quarter of that reward. What we're doing is further into the future, we're saying, ""Well, we're not that confident of what's going to happen in the future. ""We're only going to take a little bit of the reward that comes in the future. That's my interpretation of it. You can see that we're taking account of t+2 there. That's t+1, t+2. We're looking two steps into the future. Now, we've got the dot, dot, dot, so that tells you it's going to continue into the future to a certain amount. Then, this bit here is saying, essentially, you can translate thatas assuming that we're using the action policy pi to select actions. It's saying, ""Well, here's the reward you're going to get if you operate using the action policy to select your next action to lead to the next state. This means that, essentially, we're making a decision, which is we choose the most valuable thing and the action policy every time. Pi is the action policy, max is just saying the highest. Basically, we're making the best decision. It's saying, ""If you make the best decision you can according to the action policy, going into the future, this is the value that you'll receive. ""The problem is it still doesn't really tell us what the action policy is. [chuckles] How do we get this value function?That is the big problem. We're actually missing a couple of things here. We're missing, so, the state transition matrix, so we don't actually know what the state transition matrix. Think about our breakout game, we don't know what the state transitions are, because we don't have a massive data set which tells us what the state transitions are. That's the first thing we're missing. The second thing we're missing is the action policy, which tells us what decision to make. We're missing most of these things. If we don't have those, then the value function doesn't really help us. This is the problem. Luckily, there's a thing called Q-learning. In essence, before we look at the details on that slide, what Q-learning is doing is it's saying, ""Well, I'm not going to have a perfect value function. I don't worry about that, but I'm going to try and get my best approximation of what the value function is. I'm going to somehow, through observation, learn what the value function is. ""The Q-function in Q-learning, the Q-function is an approximation of the value function. What we do is we just take the original equation and we just swap out the V for a Q, and then that isour Q-learning expression. What we're saying is the best Q-functionis the one that maximizes value over time using the action policy. Instead of having to know all this stuff, what we do is we somehow learn the mapping between states and action, the value of a given action in a given state. All we need to know is the state and the action and then it will tell us what the value is. That's the problem. That all sounds great. Q-learning is a type of reinforcement learning and there are various methods to do it to approximate what that function is. By the way, most of them don't work for real problems, especially they don't work for Atari video games. This is the crux, this is the problem with Q-learning. It sounds like a brilliant way of doing it. Basically, somehow you can figure out what the value function is without the need to know all that stuff. You just learn it somehow. They don't work. That's where the DQN system comes in. DQN is a way of approximating the Q-function using a Deep Q Network. We learn the Q-function using a deep network, of course, because that's what we do all the time in machine learning and AI. We throw a deep network at it. Yes, luckily, in Atari video game context, it appears that with the right type of neural network, you can actually approximate that value function. That's why it's called Deep Q Network. That's all I want to say for now on that one. What we've been doing is we've been learning about a few new things. We've been learning about what an action policy is, or an optimal action policy is which is where we maximize reward going into the future and how we can express that using the Bellman equation. We then learned what Q-learning is, which is essentially a way of approximating what the value of a given action isgoing into the future by learning it somehow. We found out, finally that Deep Q Network is a type of neural network which is used to learn the value function, in our case, in retro video games. We've been learning a few new things here, action policy, Q-learning, and Bellman equations. Now, we're ready to continue learning about the Deep Q Network agent."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: DQN agent architecture,https://learn.london.ac.uk/mod/page/view.php?id=96451&forceview=1,"[music]In this video, we're going to get started looking at the DQN agent architecture. In summary, we're going to think about the overview of the architecture for us of all. Then we're going to think in detail about the replay buffer and how it's filled up using Epsilon greedy exploration. First of all what is an agent?I'll start referring to this thing as an agent and it's an entity that can observe an act autonomously. Just trying to come up with a really simple description of an agent, it just means some entity which can act in the world and observe and have some sort of autonomy so it's making its own decisions. You could roll into that, well it has some goals, it's trying to maximize its reward going into the future as well. That's another feature of it. What's the agent architecture for?Essentially we need the agent architecture to solve these problems. We don't have a state transition matrix and we don't have an action policy. We need some way of gathering the data that we need in order to develop those by basically learning this Q function. You can see that it's got a whole bunch of mechanical bits in the agent which allow us to do this. I'm going to explain how they all fit together and work. This is an overview of the agent architecture and I'm going to split over here to my drawing program so that I can show you that. What have we got?Let's just go for the high-level stuff first of all. First, on the left here, that block that I've highlighted there, that is the agent. Then this is effectively the game simulation or the game environment. The first thing to think about is the data that's flowing into the game environment and out of it, back out to the agent. What we've got is the agent makes observations of the game and what's happening in the game. Also, it knows about the rewards. We've got states and we've got rewards. Then the agent acts in the world by taking actions. That's really the flow between the two. Then, within the agent, we have a replay buffer over here. If we zoom into that you can see that it consists of exactly what we'd expect. It's basically gathering observations of state sand actions taken in those states, what the next state was, what the reward, was and whether the game was done at the end of that or not. The replay buffer consists of many hundred thousands of observations of what's happened as the agent is taking these actions in the world. Then, the other key element of the agent is the Q function. This is the thing we're going to be training. This is basically the deep neural network which is being told, ""Oh, here's the state. What is the action I need to take?""It's essentially this value function which tells us in the given state, what are the value of all the available actions so that I can choose the highest value action. What happens is the behavior of the agent is, it's going to becollecting observations and I'll talk about how that works in a minute. Then, periodically, it's going to stop collecting observations and it's going to go off and it's going to train the Q function up on its current set of observations or some batch of that. That's essentially what the agent's going to be doing. The replay buffer; let's think about the replay buffer. The replay buffer this is really how the agent gathers, learns or gathers information about the world. The question is how does it do that?How does it know, how does it take actions in the world?Essentially, just to clarify what's in there just in case we've forgotten. State S is the state now, A is the action taken in that state. S' is the next state. R is the reward and done or D is true or false is the game finished. That's what we mean by the replay buffers, it's basically a massive list of those. The data might look like, for example, ""I was in state 1. ""You see over there. ""I was in State 1, I took action 1 and I saw state 2 and I gained reward 1 and done with 0. ""Just to make that really clear what's going on there. What's Epsilon greedy exploration?This is the method that the agent uses to fill up the replay buffer. This is how it gathers observations. Epsilon greedy exploration sounds really complicated but it's not. Let me explain how it works. If I go back over here. What's happening is I've just explained, we've got this cycle where the agentis essentially taking actions in the world and making observations. The question is what are the actions?At the beginning, we don't have a Q function. There's no Q function. We don't have this magic function that tells us what to do. All it does is it actually has another block here which we can just call random. Instead of choosing to do something according to the Q function, most of the time, at the beginning, the agent just says, ""Well, I'll just choose a random action. ""Random actions. It just doesn't really even worry about the state. It just takes a random action every time. It's the equivalent of just you're watching the game, just moving the joystick at random. Every frame you're going, ""Oh left, right, nothing. Left, right, nothing, "" whatever. It's just randomly acting in the world. That exploration yields a load of observations in the replay buffer. Periodically it's going to take those observations and use them to train the Q function. Now, the thing that makes it Epsilon greedy, the greedy bit is, I guess, the random. Greedy means using the Q function to choose your action because that's more expensive. That's my understanding of the word greedy. Greedy means you're using the Q function and Epsilon greedy means Epsilon is just a value that's changing over time. It dictates how greedy you're going to be. At the beginning you start off with zero greed. Let's just draw a little graph here. Time and Epsilon with is a wiggly E like that. That's our greed. The greed level starts off at 0 or very low and eventually it goes up to 100% or somewhere thereabouts. What do we mean by Epsilon greed?Epsilon is increasing over time. That's how greedy we are. Remember greedy means you're using the Q function to choose your move. You can see here at the beginning we're not greedy at all. That means we're just using the random function to generate our moves. As Epsilon increases- and this may be many thousands or millions of iterations of playing the game- as Epsilon increases we more and more start relying on the burgeoning growing learning Q function to select our moves. What happens?Typical pattern would be at the beginning we are going, ""Hit the random, keep hitting the random. Keep hitting the random. ""Then we do some training. Then Epsilon goes up a bit. Then we're hitting the random. Occasionally, we're going to use the actual Q function to make a decision. Then we're going to hit random again. This is the pattern. As Epsilon, the greed increases, we lean more and more over to that Q function. We start using our Q function which is improving over time, of course. The beauty of that is it starts off giving the agent a random exploratory behavior but asthe agent learns over time, so as the Q function improves over time, it uses it more and more. That allows it to basically learn how to play the game by randomly exploring and then starting to use an actual functional policy which is actually going to give it reward. Because, of course, behaving randomly doesn't yield a reward generally but it'll find roughly patterns of sequences, maybe at random that allow it to yield a reward. Then eventually the Q function will allow it to put those together into bigger sequences. This is the idea. It's a really elegant solution, the idea of changing the behavior agent overtime to be more and more reliant on what it's learnt about the environment. That's pretty cool. That's one of the main features of the deep Q learning. That's all I wanted to say for this video. I wanted to give you that idea of these Epsilon greedy exploration and give you an idea of what the overall agent architecture is. In this video, we've just been finding out about the basic features of the agent architecture and how it uses Epsilon greedy method of exploration."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: The loss function for DQN,https://learn.london.ac.uk/mod/page/view.php?id=96453&forceview=1,"[music]In this video, we're going to be learning how the DQN network can be trained. In summary, we're going to, first of all, think about where Q fits in the architecture, where's the Q function in the agent architecture. Then we're going to be thinking about how we train neural networks, and then specifically, how we're going to train our new networkin this deep Q learnings scenario, okay. Here's a reminder of the architecture. We've been looking at the replay buffer, but now we're going to be looking at the Q function and how the Q function could be trained over time. The first thing to know is that the in putto that training is the replay buffer, that's our data set. The second thing to know is, looking at that Q network over there, you can see that what comes in is the state sand what comes out is the action. That's when we're using it, essentially, it tells us the value of all the actions that we could possibly take in a given state. The state comes in, which is basically the screenshot of what's on the screen, and then the action is that the value of all the actions, sort of say, ""The left has a value of one, right has a value of 0. 9, and doing nothing has a value of two, "" whatever. It basically tells us what the value is, the value function. Remember that we're approximating the value function, which tells us the value of a given action in a given state. Okay. Stepping back a bit, as I said, we train it on the state transition data, or in other words, the replay buffer. Problem is, we don't really have a ground truth, so we have to come up with a clever way of doing it. Normally when we train a neural network, we have a ground truth, which is we have this labeled data set with outputs and inputs, so it says something like. . . Let me jump into my editors to show you this. Normally we have, say, a set of inputs and we say, ""Okay, when we give you this input, we want you to do that output. ""Then, what we do in training is we say, ""Okay, we'll give it the input. ""We know what the correct outputis and we look at which output it actually generates, and then we look at the difference between the two and that's our loss. Say, we're training it on these three and then we knew the three correct things. We compute the loss over the training set and the trick is, the way that the trick with neural network sis that you can then take that loss, the mistakes, the errors that it's made and you can feed that back into the weights on the network. You can adjust the neural networkso that it comes up with a better solution next time. That's exactly what training is. That's what back propagation does basically. The error function is dictating what is the loss or the error between the two, what it should do and what it actually did, and then the back propagation allows us to then train that back into the network so we can figure out how to adjust the network such that that gives us a better result next time. That's the basics of how neural networks work. What's our input and output on this one?Well, we don't really have a correct output, [chuckles]so we we don't really know what the correct output should be, because the correct output is the correct value for each of the moves and each of the states. We don't really have that because the only thing we have in our replay buffer is the immediate reward that a given action will give us. We don't have the reward going into the future to get that. We would need to somehow analyze it and build a state transition matrix, but it's just too big to do that and analyze it properly. This is the problem. We don't know what the correct answer is. How do we do it?Well, the trick is this thing. Okay, so the loss function, it tells us what is the difference between what we've produced and what we were expecting to produce. If you see an equation like that, there may be some magical people in the world who see that equation and go, ""Yes, I know exactly what that means. ""There are not many of them. [Chuckles]What most people have to do, is they have to look at the equation and break it down and work out what each little bit means. Even if it's computer size, often you would then want to loo kat the code implementation of that equation to verify. Especially with research papers, you want to verify that the equation in the paperis actually the thing that's done in the code, and often it isn't actually. There's a little things they missed out or details, so the equation is not the be all and end all. Often it's a work in progress. You look at that, you have to read the code and figure out the details. Anyway, we're going to try and understand this equation because this tells us how we know what our thing has done wrong. Remember, with this image here, it's obvious what we've done wrong. That's what we were supposed to get and that's what we got, but we don't have that, okay?The reason we've got this fancy loss functionis because we have a proxy for what the correct answer is. This is the trick, the proxy is the previous version of the network before we did this round of training. What we do is we keep two versions of the network. Now, one, is used to estimate the value of something and then when we break this equation down, you see how that works. The other one is the one that we're training over time. There's a slower one that we update occasionally and then the training one, we keep updating all the time. That's the idea. You're going to see that when we break the equation down into its different component parts. I've actually got it over here. Let me bring that up for you. Yes, here we go. I'm going to get out of the way. What does it all mean?Well, the first thing to identify is that these thetas here, I got a few different thetas going on, those thetas, they're the weights in the network. In case you're not super familiar with neuro networks, the idea is that you've got a bunch of nodes connected together and signals go through them. The weights are the way that the signalis transferred between the units and the network. The training neural network weights is what is all about. You want to get the correct weights in there because then when you put an input in, you'll get the correct output. It's all about the weights, that's what we want to do. These theta expressions, the theta expressions here, here, and here, that's referring to the network weights. What we're saying is, and θi is the network now. Let's write that there. Θi, current network, right. Current network. Saying, ""The loss of the current network is this. ""[chuckles]That's the first thing to know. It's basically telling you what is wrong with the current network. What we do, let's break down these other terms. What it does is this bit. Let's get rid of this bit next. This bit is simply saying, it looks really complicated, it's simply saying that we take a random sample, so that's what the U means. Uniform random sample from the replay buffer. D is the replay buffer. It's basically saying, ""Uniformly, randomly sample from the replay buffer to yield a set, ""our training set, ""which consists of, "" we zoom into that. You can see it consists of states, actions, rewards, and next states, which is exactly what's in the replay buffer. Right?That's just referring to the input to the loss function. It's saying, ""The loss function is calculated and loss over a random""sample from the current training set, "" if you like. Basically saying, ""Here's a massive replay buffer. ""Sample at random from that, "" and then we're going to calculate the loss. How good is the network at guessing the rewards on that?Let's move on. The next bit is, here we go. Now, this is the bit where we're saying, ""This is our estimate of what the network should output. ""Similarly to my previous example, where is it?Down here, where I was saying, here, we definitely know for sure what the correct outputis and we just compare that to the actual output. Well, here we have to estimate the output because we don't have the correct output. The way we estimate it is using this magic thing here. What is that magic thing?That is θ-i. That's the weights of the network. That's an old version of the network. Okay. Old network. It's essentially saying, we take an old version of the network and we basically ask what the values would be. We ask it to come to run its Q functionon it to calculate the value of this, plus we use the actual known reward. We've got the predicted future value of this particular action versus added to the actual reward that we know. We know the R because we've got it there. The R is part of the dataset. We've taken our known reward that we got in this exact state and we're adding that to the gamma modified, so this weighted max of what we think the value function should be. We got our best guess at what the value function is, plus what we know the reward was in this frame. That's basically saying, ""This is the correct answeror the best correct answer that we have at the moment. ""We compare that to the answer that the current network yields, which is our current Q function, which is using the original, the current trainee network, and then we do mean squared error, Euclidean distance if you like. What have we said there?We've gone through and we've said that what we do is because we don't know what the correct answer is for the network, we take an older version of the network, plus the reward signal that we've definitely that's correct that we know because that's in our Action Replay buffer and we combine those two. That gives us an estimate of what the value function should be, an estimate of the best Q function we can come up with. Then, we compare that to what the current network is actually generating. Bear in mind, your current network doesn't use the actual reward that we've got. That's how we do it. Over time, this will improve because periodically, we take our training, our current network, and we copy it over to this one. This one gets updated occasionally. It takes on the weights of the best network that we got. We retain an old version and we keep using that, eventually, we update the old version to a new version, and we start using that. We've got this cycle going on. Over time, the value function just gets better and better, it gets better and better, because it's comparing itself to the previous version, plus a known signal, and then it's improving itself based on that. It's a very elegant and clever solution to the problem of not knowing what the correct output should be. In this video, we've just been going through the loss function for the Q network in the DQN. It's a pretty gnarly equation, but I hope that by my enthusiastic explanation, have tried to pull it apart and explain what it all means. How we can go about training a neural network when we don't really know what the correct answer should be, by gathering examples of data and using our best guess to compare what we're doing as we progress through the training. In this video, we've just been learning about the loss function in the DQN system."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Visual processing and states in DQN,https://learn.london.ac.uk/mod/page/view.php?id=96455&forceview=1,"-In this video, we're going to be talking about some of the little changes that are made, well, quite big changes actually, to the data that comes out of the Atari game emulator and into the DQN agent. We will think about some of the details of what you have to do when you're building a real AI that can really do something in the real world as it were, or at least the real world of video game playing. First of all, we're going to talk about some secret sauce they add to the state to make the state a little bit more informative to the network. Then we're going to be talking about how they deal with a certain issueor technical limitation with the Atari platform, which meant that the programmers use little hacks, which cause flickering, so we're going to talk about that. Then we'll talk about the final processes, which is really just data reduction, and we'll see that that's clearly a sufficient amount of datato know what's going on, in this game at least. First of all, my observation. There's no sense of time. If we look at these two screenshots, you can see on the left one the ball is there and the right one obviously you can tell which direction the ball is moving in because you've got that sense of time. If the neural network is only being passed a single frame as its state, if it only ever knows, ""This is the state now, "" and it has to make a decision as to what the value of that state is, value of the different actions are for that state, then how does it do that if it can't even tell which direction anything's moving in?Especially with these arcade games, it's all about things moving. Somehow the network needs to know which direction things are moving in. Otherwise, how's it going to know which direction you should move in?That's really one of the problems that came into my mind as I was reading the original version of the paper before I got right into the details. Now, typically, if you're modeling sequential data over time, for example, audio or video, or text even, because text is sequential data, typically people use some sort of recurrent system like an LSTM network, you feed it the sequence and it has some memory so it knows what the previous sequence was, it remembers that over time so it can build up this knowledge over time. The network that gets used in DQN is not recurrent, it doesn't have any of that sense of retaining the pattern over time that's been fed in. How do they do it?Well, the answer is, the state that gets fed in is, in fact, three frames so it looks at the last three frames. It's not just a single frame, so by ""state"" in DQN we mean actually three frames of what's going on in the game. You can see an example here. We've got three frames where you can see the ball moving from the left over towards the bat there. That means that it has a sense of time because the state that you're giving it actually includes information about three frames. I won't call it cheating because it's not cheating. It works. Nothing's cheating here. It's just that there is a way that they feed some sense of time into the neural network. You might think of it as being a state which is second order or something because it retains two previous states in a certain sense. That's the first thing of the secret sauce of the three frames of state. The frames thing gets more complicated still. Have a look at this quote from the paper. ""We take the maximum value for each pixel color value over the frame being encoded and the previous frame. "" It's looking at two frames. Forget the three frames thing. Let's imagine we're just trying to get one of those three frames. How are we going to get it?Well, actually, one of those three frames that gets fed in as the stateis compounded out of two frames. They take the maximum value. Why was this necessary?Because they needed to remove flickering that is present in games where some objects appear only in even frames while other objects appear only in odd frames and artifact caused by limited number of sprites Atari 2600can display at once. Let me just draw a picture to explain that. What we're saying is, we've got two frames. The problem with the Atari 2600, the original hardware, is it could only display a limited number of sprites. Sprites are the graphics on the screen. A brick would be a sprite probably or the player's bat would be a sprite or the ball would be a sprite If you want to draw loads of sprites on the screen, like in Space Invaders, there's loads of sprites on the screen, what you'd have to do is go draw whatever you could on one frame and then the second frame you'd draw the other set of sprites and then on the subsequent frame you'd have to draw back your original set of sprites again, that may have moved. What that means is it's flickering. The sprites are flickering on and off in between frames but as a human player, you don't really notice this. Actually, Ataris used to be a bit flickery. You notice it but it works for you because your persistence of vision that you have in your visual apparatus basically means you make up for it with your brain so you can deal with it. If an AI is looking at this it just says, ""Oh, it flicked it off, ""What they do is they essentially combine these two frames into one by taking the maximum value of a pixel. We're assuming that it has a black background and if it has a black background when the sprite disappears it goes black. That means that you always choose the data that is when the sprite's appearing so if you stick those two together you'd end up with that. You'd see all of the sprites. Of those three frames that we're feeding in as the state each of those is actually made out of two frames where we've taken the max value to deal with this flickering problem. That's the next fiddly thing that they do. This is a bit more straightforward. Basically, this is where we get into the idea of data reduction. The next step is instead of having RGB values, they convert those into a luminance value which is a bit like gray scale really. The luminance value gives you a single number per pixel instead of having three numbers per pixel. That just means we've got less data to feed into the neural network but you can see from that screenshot, clearly you can still see exactly what's going on in the game. The next data reduction step, the final one, is to take the number of pixels down from 216 wide down to 84. It basically rescales the image and there's the code that does that in the [?] example we'll be looking at. We'll see that again later, don't worry about understanding that. Essentially, you can see from this screenshot up here that it's still really clear what's going on. You can see where the ball is, you can see where the bat is, it does look a bit blurry or whatever, a bit pixelated, but that's fine, because as long as you can see what's going on it doesn't really matter whether it's pixelated or not Again, you're throwing away loads of unnecessary data, again there. Gray scaling and resizing throws away a load of data but, of course, the other steps are adding more data. We're having effectively six frames fed in as the input although we're combining two of those into three. That's it. In summary, we've just been looking at some of the interesting processing steps that they had to do in order to, firstly, retain some sense of what happened in the past and secondly, to solve the problem of flickering which is a of an artifact of the necessarily hacky programming they had to do to switch the sprites on and off because of the limitations of the hardware in the original system. Then we talked about some of the data reduction steps, the gray scale, and the resize step. In this video we've just been getting a heads up on how the raw data from the Atari emulator is processed to be ready for training the neural network."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Weekly outro,https://learn.london.ac.uk/mod/page/view.php?id=96458&forceview=1,"[music]Well done, you made it to the end of week two. That was probably the most mathematically challenging weekin this case study. If you found that really dense and difficult to get into, don't worry, we're going to do some coding next weekand we're going to be seeing the details of how we implement all that maths into code. Certainly for me, I sometimes find it easier to start with the code and work to the maths, but this time we've done it the other way around. What have we been looking at in this rather challenging week?First of all, we've learned how to describe a gamein the formalism of a Markov Decision Process, and it wasn't as complicated as we thought, it's basically states, actions, state-transition matrix, and making decisions iteratively over time. Secondly, we know what Q-learning is and why it's necessary. Q-learning, remember, is the ability to understand what value different actions that we might take have, so that we make a decision. Thirdly, we can explain the purpose of the key elements of the DQN agent and the loss function. We've seen the agent architecture now and we've even dug into that loss function, breaking it apart and try to really understand what's going on and what's the magic going on in the DQN agent, which isn't magic at all, of course, it's mathematics. Well done, you've made it to the end of week two, this was probably the most mathematically challenging week. I hope you come back next week to see come code."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Welcome to this week,https://learn.london.ac.uk/mod/page/view.php?id=96461&forceview=1,"[music]Welcome to week three of the game-playing AI case study. This week, we're finally going to dig into some code and start building some AI systems which can play games, well, simplistic ones anyway to start off with because really the focus this week is tooling. We're going to learn about the basic tooling that we're going to be using to build this DQN agent. Here's the learning objective. First of all, we're going to be using the open AI gym library to implementa game simulation environment. We're going to learn all about how we can createa correct environment with which our agent can interactin simulation, and secondly, we're going to developa simple random game-playing agent that can operate in an AI gym. We're going to create if you like the most simple possible agent that does something in the world and learn how to do that. That will show us the whole thing, the whole process of building an agent. Finally, we're going to learn a little bit about the Keras library. I'll just say that it's a whole course to develop, to learn all about neural networks and Keras. I'm just going to give you some highlights just to give you just about enough if you haven't done any neural networks before, just about enough to grasp what's going on in the neural networkin the DQN agent and how that's done with the Keras library, especially focusing on the convolution al aspect of the neural network. We're going to talk a lot about convolution in this week. Let's just have some highlights of things we're going to see. Here we go. We're going to be seeing some--these are some of the open AI gym environments. You can see we've got these kinds of examples like Box2Dsimulated physics-type environments, we've got our custom one over here which we've built for you because we can't show you the Atari ones for copyright reasons but we've done our own version of the Atari game. Then there's another one up there which is the Lunar Lander example. Here's the exciting thing. Personally, I think it's the most exciting thing that you'll see this week. You are going to see what a neural network sees when it looks at the game. We are going to show you what a convolution al neural network actually sees when it looks at a video game. What you can see here is is the output of 32 filters which the neural network has learned how to configure such that it can pull out the relevant information that it needs from the view coming in from the game. You're going to find out all about how that work sand what that means this week. Welcome to week three. We're going to roll the sleeves up and learn all about the tools of open AI gym and neural networks this week."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture:  What is openai gym?,https://learn.london.ac.uk/mod/page/view.php?id=96463&forceview=1,"[music][music]-In this video, we're going to be talking about OpenAI Gym and having a quick demo of how we can use it. In summary, we're going to first of all ask the question, ""What is a gym?""Then, we're going to be looking at OpenAI Gym in particular, finding out about all the different environments that are available or some of them anyway. Then, learning how we can create an environment. How we can act in the environment and Then, access all of those things which we know about from looking at the Markov decision process way of modeling reinforcement learning. First of all, what is a gym?I've tried to come up with three points here. Number one is, it's a way of providing standardized environments for reinforcement learning algorithms to operate in. It's a standardized API that allows you to plug your reinforcement learning algorithm inand it will have a standard way of doing things which we'll see in a minute. Number two, it allows the developer to focus on their agent instead of the simulation. We saw that brought up in an earlier discussion where we're talking about I think the Mario AI competition, and the idea that as a developer who wants to work on AI algorithms, you might not want to spend all your time building the best simulation of physics or whatever it is. This gives you a bunch of ready-made simulations or environments. You can just plug your algorithm inand work on that rather than working on the simulation. The third point is that it includes versioned standard set of environments enabling easy comparison. The fine point is that because everybody is using the same simulation, if you like, it means that it's very easy to compare algorithms. If you have a simulation of a video game and your algorithm gets 27 points, someone else's algorithm gets 30 points consistently, you can see that it looks like your performance is going is comparable, first of all, and you can prove that it's better. There are various gym-like systems out there. The OpenAI Gym is not the first or the last, probably. I've just picked out a few here. You've got the Arcade Learning Environment from 2013, which predates the Deep mind DQN paper and I think that's what they used. Then, you've got Viz doom, another example from 2016. I've got all the citations up here, but don't worry about those. I'm going to give you those as direct links in the reading. Then, Viz doom allows you to basically design agents to play the Doom first person shooter game. Then, you've got OpenAI Gym, which is the one we're going to be looking at. So, what is OpenAI Gym?Well, this is what it aims to do, anyway. It aims to combine the best elements of these previous benchmark collection sin a software package that is maximally convenient and accessible. The design goals areto have as many of these other benchmarks in there as possible, but to make it easily accessible. Indeed, you'll see from the Python interface we're going to be looking at that it is pretty easy to access. These gyms, and specifically OpenAI Gym comes with a whole bunch of different environments. We've got three here and I'm just going to show you the webpage, because it's probably easier to see them there. This is the OpenAI webpage for gym. You can see over that side that we've got the various different categories of environment. There's the algorithms one, the Atari one, which I can't show you because of copyright concerns. Box2D which is a 2D physics environment. Let's look at a few of those. You can see that the first couple are these Walker ones. You can see there's this two-legged ED-209 if you know Robocop, it's like an ED-209 type creature. No machine guns, though. That's walking across an environment and there's two different versions of that. The one on the right has a more challenging environmentas I understand it. Then, there's a driving game. You can see the green one there in the top left is car racing, so you're driving this car. Then, we've got lunar lander up there as well. Those are some environments that come with OpenAI Gym. Then, what else have we got?These are classic reinforcement earning problems. So, the Cart Pole is the absolute classic. I can actually remember my tutor lecturer when I did my master's at Susex, Adrian Thompson, who is one of the inventors of evolvable hardware actually. He was demonstrating CartPoleby running it backwards and forwards with the pointer for his blackboard, a big tall stick. He ran backwards and forwards across the lecture hall to illustrate how difficult it was to balance the Cart Pole. Of course, reinforcement learning algorithms are very goo dat learning how to do that. Anyway, that's that one. What else we got? Then, we can jump into these ones here. MuJoCo is a 3D physics engine, a really precise physics engine. You can see they've got various ready made robots which you Then, have to plug your algorithm intoto try and teach them to walk. Then, there's various different robots. There's a humanoid one there and various other bits and bobs. There's a snakey one there. So, that's that. Then, interestingly, at the time of filming it's just happened that Deep mind have actually just bought MuJoCo. Mujoco was previously a commercial product, a commercial physics engine which has just been bought by Deep mind in 2021. Then, they're going to open source it. That means that these fancy physics engines are going to become available. I don't know how it compares to Bullet, which I've used in one of my other case studies, but apparently it's pretty good. Then, we've got a whole set of robotics 3D environments as well. There's all kinds of really cool environments available hereto train algorithms in. That just gives you a little insight. Back to my slide here, you can seein the middle I've got this break wall one. We had to create our own environmentso that we could show you something learning how to play a game without having to show you the Atari games. What we did is we were able to fairly easily create our environment by creating a game using the Pygame library, which is a very common Python gaming environment, it lets you build a game engine. So, it's a game engine that lets you build games very easily. We built this game and then, we were able to wrap it upin the OpenAI Gym system such that it could just be a new gym. So, you can create your own environments pretty easily, because I did it [chuckles]. I can tell you it's not that difficult. Moving on. How do we create an environment?Well, first of all we need to know which environments are available. What I've got here is my Python environment. I can import gym and then, these commands here allow me to just show you how many different environments are currently installed in my system. I think I've installed the Atari ones on here as well. So, we do gym. envs. registry. all. I've now stored all of the names of all the environments into x. Let's just do length of x. You can see I've got 859 available environments. Now, they're not all totally different. They might be different versions of the same environment. Let's just print them out. For i in x print i, let's see what happens. It's now printed out all those games, so i know that certainly Zaxxon, for example, is a Atari games. Obviously, got lots of Atari games on there. Wizards, pinball, tennis, Star Gunner, Space Invaders, all the favorites there. No Robotron. Robotank, though. You've got all these ready-made environments on there. Also, as I say, you can easily write your own. That's how you get access to all the different environments. Let's say I want to create an environment. Well, here's the workflow for doing that. Here, what I'm doing is I'm creating env2. gym. makeis the way of creating the environment, and I need to give it a name. Let's follow that along. I'll do env. Let's call it Lunar Lander env to be clear equals--Actually, let me just do this. I'm going to cheat and go--Here's one I made earlier. I'll call it Lunar Lander environment. That's the command. Run that. Then, the first thing you'd normally do with a new environment is reset it. I do ll_env. reset. It passes me back some stuff there which we're going to talk about in a minute. Then, the next thing you probably want to do is actually render it, because that lets you see the current environment, what it's doing. I'm going to jump into that, because that's quite small. That's pretty much it. You can see at the top there that's there's some Apollo Lunar Lander capsule coming down. You can see the leg sand that's going to be trying to get down to that place there. So, this is the task here, in this particular environment. Then, what I would do next is I'd want to step. I'm going to do ll_env. step. We wouldn't normally call render when we're training because we don't need to see it. More about that in a minute. Then, I'm going to pass it just some value sand I'll explain what they are. I just passed in a zero zero, and no wand if I keep rendering it again. Sorry, ll_env. render. Then, I step. That's basically it. You can step through. It's not really moving much because the frame rate's quite low. Let's say I do a quick quick iteration on this. For i in range 100, and I'm going to do-- sorry, ll_env. step, and I'm going to tell it what to do and ll_env. render. Now, look carefully at the thing up there. Let's just pull that over there. Okay, hopefully, you saw that. It basically plopped down. Actually, I managed to land it, which is pretty remarkable. Yes, so that's an example of the environment iterating and rendering. I don't know if you saw that plopping down. You can always rewind if you didn't. That's the basics of operating an environment. The question is, what was that zero zero I was passing in?The answer is that as the action. Okay, so if we go, we get back into memory, the Markov decision process, where we basically tell the agent to--we take action sand as a result, the state of the environment we receive, the next state, okay, so we know the current state. We act, and we get back the new state. We also get back other thing sand we'll talk about the other things in a minute. Let's talk about acting first. The available actions are called the action space. If I do ll_env. action_space, okay. Then, that tells me it's box two, which is a bit mysterious. What I can do is I can call sample(), and that will basically generate a random action of all the available ones. You can see that it's a 32-bit float with two values, it's basically a two-dimensional array there. Each time I sample I'm getting a different random action. I'm not taking this action yet, I'm just sampling the action. Now, if I create another environment. So, if I do this, and I'm going to call it break wall environment. This time, I'm going to create something called break wall. This is the one we've created. Again, if I do bw. env. action_space--sorry, bw. _env. action_space. sample()and I should get examples. This one has got a much simpler action, it's just an integer. I could just see what it is. It says Discrete 4, that's basically four possible values. You can see that different environments have different action spaces. Okay. Let's say we want to build our simplest possible thing to play to operate in this environment. Then, what I can do is I can sample a random action and then, pass that into step. Okay, so, let's try that. I'm going to do for i in range(1000). I'm going to do it with the break wall environment here. I'll doa = bw_env. action_space. sample(). Okay, so that's my random action, Then, I'm going to take my random action. I'll do bw_env. step(a) that I've just generated at random. I'm going to do bw_env. render(), just so we can see what's going on. I'll just make sure it's on the screen because otherwise, we might not be able to see, you haven't rendered it yet. Okay, right. Are you ready?Let's get out of the way. You can see the bat twitching around at random, because it does not do anything sensible, well, it's a bug in the game that allowed itto somehow magically not die. Okay, so that was the 1000 random steps taken in break wall. What about lunar lander?Well, I'm going to reset it first, because I noticed that the lunar lander has actually landed. If I do the same thing, but do ll, here. I'll take a random action, the lunar lander 1000 times. Where's my lunar lander environment? There it is. Okay. I think it's crashed already. It's obviously not--It went a bit fast, didn't it?Let's quit that one, let's try again. If I just reset it first. Okay, Then, let's try again. Okay, it just randomly crashed. Okay. That gives you the idea of basically what you can do, how you can select random actions. We've done that. Now, the next thing to note is that when we step, it returns some stuff and this is not surprising. You can see here, it's really the MDP, right?They call it the observation, but we can call it the state. The O, so it's o, r, d, i equals ll_env. step. Okay. It's giving us--we can actually find out what this is. If we do ?ll_env. step like that, that'll give us a description. This is it, it says, ""Run one time step of the environment's dynamics. ""Then, it will tell us what it returns. We get back. The observation is the first thing. Okay, the observation, the reward, done, and info. Okay, so the observation is really the state of the environmentas far as our agent is aware. That is exactly the stuff that we need for doing Q learning or whatever. We need to be able to act in the world and we know what the result is of our action, which is the next state of the world. Plus the reward we got, whether the game's over, which is the done thing, and then, some extra info as well, in case that's useful. Let's say I doll_env. step(ll_env. action_space. sample ()). Okay, so I'm just going to take a random step. Let's just see, what is that?That's the state that's coming back. The first thing is the state. That first block of stuff up there is the state. Then, the second bit is whether is the reward, which is this -100, which doesn't look like a good reward. The next thing is, whether it's finished, the game is over or not. Remember, we saw it crashing down, so it must be over. Then, something about time limit truncated. Okay, so what about if I reset it, and then, do it?Then, do another random action. Okay, so you can see now the game is not over. I've got some different values back, which I'm not totally sure what those arebut they're obviously some information about the state of the game, which we can use to train on. Now, what about the break wall environment?Let's try that, bw_env. reset, let's reset it first. Then, do a step and see what we get back from this one. You can see, first of all, when I called reset, you can see I got back, basically loads of numbers, lots of them are zeros because they're basically black pixels. It's given me all of the RGB values for the pixels. Then, you can see, similarly, I'm getting the--That's the observation, that's the state, that big thing. That's the reward, which is zero in this case. That's whether the game is finished or not and there's no info, any extra info. Okay, that is, we're looking at there, the different environments give us back different types of observations, and they take different actions. Yes, different environments give us different thing sand that's really it. That's the end of my introduction to OpenAI Gym. There's going to be a worksheet where you get to play around with this and build your own random agent and explore and look at the different environments. That's something you can look at. In summary, the OpenAI Gym provides the workflow for reinforcement learning, of acting, observing, and getting rewards. It's basically giving us everything we need to build one of these Markov Decision Processes, and then, do some learning on that. In summary, we've just been finding out all about gyms, find out what the basic idea of a gym is at first. Then, we looked at the OpenAI Gym in particular. We looked at some available environments. We looked at some different types of environments that it has. We looked at how you can create an environment, how you can take different action sand that different environments have different types of actions, how you can make observations of the state of the environment, and how you can look at the rewards. Yes, in summary, we've just been looking at OpenAI Gym."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Introduction to keras and neural networks,https://learn.london.ac.uk/mod/page/view.php?id=96468&forceview=1,"-In this video, I'm going to give you a real crash course in neural networks, just to give you an insight to some of the key terms that we're going to be looking at as we discussed the DQN neural network. First of all, I'm going to answer the question, what is Keras?Then we're going to be talking about all the different elements of neural networks. I'm going to try and give you a kind of quick intuitive grasp of what's going on with neural networks because I'm not going to assume that you have done an extensive neural networks course in this course. What's my motivation?As I say, we need some basic neural network terms to be able to understand the DQN network. I'm telling you all of this because in order to get a good grasp of how the Deep Q learning works, we need to understand a bit about how neural networks work because when we look at the code, we'll see what's going on. What is in your network?It's a set of processing units connected together into a neural network. Let me show you a picture. Here's a bunch of processing units, and they're connected together into a neural network. When we connect them all together like this, there's many different ways you could connect them together and different connections that could even be connected to thereif you want but there's all kinds of different--That is a neural network, a bunch of processing units. What do you do with it?Well, you feed in some numbers at one end, and other numbers come out of the other end. Okay, so back to the drawing. We put some numbers in at the top, so some numbers come in here. Well, maybe 0. 1, 0. 2, 0. 1 or something, so we're feeding numbers in at the top and then at the other end, those numbers they flow through the network. That one's going to go down to there, down to there, and then that one's going to go all over the place. Basically, the numbers go through the processing network, and eventually, other numbers come out the other end. It might be a 0. 0001, you might get there. Then maybe you get another number there. Numbers have come in the top and other numbers came out at the bottom, that's the basic idea. Then, the other thing to know is that training in neural network involves incrementally adjusting the settings of the processing units until they cause the correct outputs to come out in response to a set of inputs. The whole point of training is that when we feed in those numbers, we have a bunch of examples of inputs and outputs typically, and we say, when you see this input, we want you to produce a specific output. We feed the numbers in, we have a look at what comes out, and we've got our idea. Well, actually, we didn't want that. This is what we wanted. We wanted some different numbers. We wanted to say 0. 1 to come out and 0. 2 there and maybe 0 there. You know what came out, you know what you wanted to come out, and then training involves going, ""Okay, well, it was wrong, ""so training involves feeding that error. We look at the loss between what we got and what we wanted and we feed that back through the network, feed it back through and the error, and we adjust all the settings on the network. I'm going to talk a little bit about what those settings are in a minute but suffice to say that training involves looking at that network and reconfiguring it slightly based on what the error was so that it gets slightly closer to the correct answer next time. That is training and basics of neural networks. What is Keras? Well, Keras is a library, which allows us in a high-level way, it's a high-level API library that lets us program TensorFlow neural networks basically. It gives a nice simple Python interface on the TensorFlow library where we can build networks. The DQN example code uses Keras. I forgot to say on the previous slide, that DQN uses a neural network to learn the Q function where the input is game state and output is value of all actions. If this is DQN network, the thing coming in at the top there, the thing coming in here, remember, is the state of the game, which is basically all the pixel values for the last four frames, I think it is. We're basically passing in loads of-the number of inputs is like one for each of the pixels on the screen, so it's got a huge input layer coming in there. Then the output, what it's giving us, is the value of each of the possible actions. Those numbers coming out the bottom in the DQN network are like how much is going left?If I'm playing break out, I've got to move my bat left or right, it's telling me how much is it worth to go left now, how much is it worth to go right, and how much is it worth to do nothing?It's telling me what is the best action to take given what we've told it about the current screen. It looks at the screen, it tells what--That's what we're doing and the idea is to learn what the best actionis for every possible screen ultimately in DQN. Keras is the neural network library. What are layers?Well, neural networks are made out of layers, and deep neural networks typically have quite a few layer sand that's what we mean by deep. There's different types of layers with different nodes in them which process the signals in different ways. Back to the drawing. That's the input layer, for example, and that's what we might call a hidden layer because it's inside and that's the output layer but we have loads of layers in there and the more layers you have, the more processing is done to the numbers as they flow through the network. As those numbers are going through the network each layer, each node is doing some sort of processing on the numbers. A deep network has loads of layers, and therefore it's quite difficult to train because you have to give it loads of examples for it to learn how to configure the network correctly. There's lots of different types of layers, and this is the Keras documentation for the different types of layers. What I say, when I first saw this webpage, it really blew my mind because I felt like I've just gone into a sweet shop for neural networks, the equivalent of the neural network sweet shop because look at all these things it's got. You got all these different types of core layers. You've got input layers, dense layers, activation layers, embedding layers, masking layers, lambda layers, that's just the core layers. Then we've got all these different types of convolution al layers. We're going to talk a lot more about convolution in the next video, so don't worry about what they are for now but there's loads of different ones. We've got pooling layers, we've got recurrent layers, long, short term memory networks, GRUs. We've even got attention layers, which are the trendy new thing for doing transformers and that kind of thing. It's all there. When I was doing neural networks first in the late '90s when I was at Sussex doing my masters, we didn't have all this stuff. We had a really basic neural network library in MATLAB, or whatever, and that was a lot. A lot of people were hand-coding these things in Cto get them to work fast enough and so on. This is exciting times to be working on neural networks that's what I'll say about that. Where am I? That's the layers. There's lots of different types of layers. Keras makes it very easy to build, lots of layers together, and those are the core layers. I just mentioned those. Those are some of the quotes from the documentation. A dense layer is just your regular densely-connected layer. For example, a dense layer is a type of layer where everything is connected into it. You can see this, I guess, this is actually a densely-connected layer because everything from the previous layer--Actually, it's missing one there. It actually go there. Each of the sort of inputs to that layer, if you like, is connected to each of the outputs. That's what we mean by densely-connected. There's different ways. Depending on the layer, things are connected differently, and also, there's different mathematical functions embedded in that layer to process the numbers coming through. As I say, there' more of these convolution al layers, and DQN uses convolution al layers a lot so we're going to talkin a later video much more about what convolution does. Then you've got this idea of an activation function. What's an activation function?Well, back over here, what happens is the neural network, each of these nodes is going to do some sort of processing. It's going to say-- The numbers are going to come in to that node. In this case, we're going to have three numbers coming in, and the activation function it's really based on a biological metaphorso it's bio-inspired computing, remember. It's bio-inspired computing. It's the idea that in your brain or the way natural neurons work, you've got lots of neurons wad into another neuron and signals come in, and if the sum of all the signals coming in is high enough, then the neuron will fire. We have a similar concept in here. Let's say we were to draw a simple graph of it. We would have the input coming in there, and that's the output. Typical biological one would be as the input goes up, and up and up, eventually gets high enough and it fires and an output comes out. That's the, if you like, the most basic activation function. Then you got kind of linear ones, as the input increases, the output increases. Then you got kind of sinusoidal ones, all kinds of different shapes, [chuckles] whatever you want. You can have whatever activation function you like. There's loads of different ones. There's lots of built-in ones, or you can probably write your own ones as well. Okay, that's activation functions, and the purpose of an activation function is to control when the neuron outputs and how much outputso it can combine lots of inputs and respond to those. The next concept is weights. The weights interact with the activation function. Each of these lines, basically, is a connection. Each of these margin has got a number next to it 0. 1, 0. 2, 0. 3. You scale the numbers as they flow through. They get scaled. It's another powerful sum. You add up all of the inputs, multiply them by each by their weights, and then that dictates how much is flowing through the network. Then those weights, then feed into the activation function and depending on the type of activation function you have, you'll get a different output from that node, depending on the weights and the activation function, and the activation coming in as a result of the inputs. All works together to basically set up this whole design for how signal flows through the network. Right. Then you have this loss function. The loss function is when you feed in, as I referred to it a bit earlier, but you feed in a given set of inputs and you know what you want, you know what the output should be--Okay, well, I want that output, but maybe you only got this output, and that's the output you got. The loss function is, I know what I wanted, but that's what I got. The loss function is the difference. It's like the difference. There's lots of different ways of calculating the difference between the two. Just different distance functions, if you look at that, it's like a 3D vector, isn't it?There's lots of different ways to calculate the distances between two places in 3D spaces it were, and that's what the loss function is doing. The idea is that you feed in the inputs, and you calculate how wrong it is. The loss function is how wrong is the neural network now. It gives you a value. Once you've got your loss function, I'll come back to that, you then do training. The idea of training is that you feed a bunch of examples, you calculate the loss, which is how wrong it is, and you feed the loss back into the network weights. That's the key thing. Once you know how wrong the neural network is, using basic calculus, you can feed that loss back through the network, back up into that. You got your loss, and you feed that, that dribbles back up to the network, and you can adjust the weights. That's what we mean by training. We basically change those numbers. We change the weights slightly so that it gives a slightly better result. What we can do is we keep feeding it examples, we check the loss, and then we feed the loss back through the weights, and the weights improve over time till eventually the network actually outputs the thing that we want, or at least as close as possible to the thing that we want over a wide range of inputs. It comes out with the correct outputs for each input. That's what training is. It's really just feeding things in, seeing how wrong the network is, and feeding that wrongness back into the network, and adjusting the weights so that it gives a slightly better answer next time. We keep doing that iteratively until it's really good. Okay, now, I'm going to come back to this. Custom loss functions. You don't have to use the built-in ones. In fact, in DQN we use a custom loss function. We've already seen the equation for this, we're going to revisit that and see how that gets implemented in code. Remember the loss function in DQN is fancy because it basically uses a slightly less trained version of the network to partially calculate the loss. It's a bit fiddly, but we'll talk a bit more about that later. Okay. In summary, the complete picture is that we build a model by sticking a bunch of layers together. We set the weights normally to random values or there's various ways you can initialize the weights, we feed in a bunch of data, we look at what came out, we calculate the loss, and then we back propagate the loss into the network, and then we go back to three, so rinse and repeat. We just keep feeding in batches of inputs, looking at the output, feeding the loss through, feed another set of inputs inand we keep training again and again until the loss is low enough over our data set that we're happy that it's being trained. Okay. In summary, we've just been answering the question, what is Keras?Then we've dived in to a bit of a crash course in neural network. We've talked about layers. We've talked about transfer functions. We've talked about weights, loss functions, custom loss functions, and finally what training is. Hopefully, at this point, you should have some idea, the basic idea of what a neural network is. It's this data processing thing where we can learn how to process the databy feeding it examples and feeding the error back into the network to improve it over time. In this video, I've just given you a crash course into our second major tool that we're going to be using to build DQN which is Keras, the neural network library."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Convolutional layers,https://learn.london.ac.uk/mod/page/view.php?id=96470&forceview=1,"[music]In this video, I'm going to be talking about convolution and convolution al neural network layers. In summary, first of all, we're going to think about what is convolution, and what convolution does to images, and then we're going to be thinking about how we can embed this processof convolution inside a neural network and use it to enable the neural network to learn how to process images to extract the salient information. We're going to be talking about how we parameterize these convolution al layers and the neural network, and then at the end, we're going to be seeing a visualization of how a real neural network sees a video game while it's being played. I wanted to start out by just reminding you what the agent architecture is because this kind of motivates what we're doing. What we've got here is the agent architecture, which we've seen in a previous video. This is the agent and this is the video game, and the basic idea is the agent makes observations about what's going on in the game, which are essentially the screen grabs of the pixels on the screen, and then it makes a decision about what to do in the game. It makes moves. It might move the bat right or left, or it might do nothing. What's interesting for our convolution al layer sis these inputs, which the video game screenshots are fed into this neural network which then outputs what the appropriate action is based on what it can see. Inside here, we're going to be putting convolution al layers because they're commonly used to process images. That's a just quick review, where this fits into the agent architecture. As I say, the motivation is that the DQN network architecture uses convolution al layers, and it does make sense that it uses convolution al layers, because they're commonly used for image processing tasks. Lots of the image classification models, pre-trained neural network models that you'll see are based around convolution al layers because they're extracting information from image data. First of all, what is convolution?It's a filtering technique often used for images, but it's also used for other signals like sound. If you're on the B Sc, and you've taken the graphics programming course, then you will have seen convolution already there, and if you're on the B Sc, and you've taken the signal processing course, then I talked about audio signal processing in that course. The idea is that you've got your image or your signal. Let's talk about images. The task of convolution is to calculate a new version of that image by applying some sort of filter to the images. Essentially, you go in and you pass a filter over the image, and that generates a new image. Let's look at some examples of that. I don't know how well you can see this, but the idea is that the left-hand imageis fairly sharp and the right-hand image is blurred. Hopefully, you can see that there's it's slightly blurrier depending on what kind of thing you're looking at this video on. That's one example, and the way that one works, and we'll look at the details in a minute is essentially, what we do is the filter is placed over each section of the image and it mixes together the various values of the pixels into the new pixel value. It's just the equivalent of essentially an artist havinga bunch of paint on a canvas that hasn't dried yet, and they're smearing that paint around. The paint gets mixed together, so you get this kind of averaging effect. That's one example. Here's another example that's perhaps more striking, and this is called edge detection, which is very similar. It's exactly the same process. It's just we have different ways of processing the image. In this case, the filter instead of averaging out over the pixels to make a blurred version, it almost super sharpens it. It blocks out any pixels that don't change compared to their neighbors, but pixels that do change relative to their neighbors get emphasized. What you see in this image is that the bits of the image where there's rapid change between neighboring pixels get emphasized so they go brighter, and the bits of the pics, the image where there's not much change gets de-emphasized. That's what we mean by edge detection. How does it work?I've got an animation for you here, which I've prepared in P5. The gray thing is the image, and then the green thing moving across is the filter. We don't filter the whole image at once. We basically apply the filter for various positions in the image, and we calculate the new pixel, the center pixel of that moving filteris calculated, and the reason I've got a square over top of other square sis to show that the new pixel value is based around some sort of calculations that's done on multiple pixels surrounding it, and that green thing moving across there is called the filter kernel, and it can be different sizes. If I just edit the code a bit, we could havea 9 by 9 filter kernel as well. If I reload it and play that, that was obviously huge. Each new pixel is calculated based on all of those other pixels that's almost taken account of the whole image. Obviously, with a real image, it's going to be 1000s of pixels by 1000s of pixels, but we could do a 5 by 5, which might be a bit more subtle. In this case, it's taking account of quite a few of the pixels around it, but as I say this is just a demo. Okay, so how does it work?The image kernel, which is that box that we saw moving across, has coefficients, and the idea is that you multiply each of the pixel values in the kernel by these coefficients. The left-hand one is the edge detect. What we're doing, you can see that we're basically negating the surrounding pixels and emphasizing the center pixel. It's almost certain that that gives you that edge detect effect when you apply that in that way over the whole image, and then the blur effect is done by essentially calculating an average. If you think about what you see in that blur filter, each thing is a 0. 5. It's taking a little bit of each of the pixels or half of the value of each of the pixels around it, and mixing them together into the new value. It multiplies all the pixels by those values, add them together, and that gives you your new pixel value. Those are two classic convolution filter examples. What is a convolution al layer in the neural network?As you might imagine, it is a neural network layer which applies trainable filters to images or other data. Instead of hard coding the filter parameters, like we did in the previous slide, we've got question marks. We don't tell the neural network layer what the filter is going to be. We don't start off with these filters. It has to learn a filter, and the way it learns the filteris by training. We pass it a bunch of examples, and then we train it, and it feeds back the error back into the configuration of the filters. There are a few different properties which define how a convolution al layer operates. Firstly, how many filters does it apply?In the examples we've been seeing before, like the blur and the edge detect, we're only applying a single filter, but a convolution al layer can apply multiple filters to the same image. Then secondly, it's the size. That's what we saw when I was adjusting the number, how big the box was. That's the size, how big is the filter kernel basically. Then finally, the stride. In our example earlier, it's moving one pixel at a time. If we just look at that one very quickly again, you can see it moved across one pixel at a time. The stride allows you to jump more than one pixel at a time. You can basically make a smaller image. If you only, let's say, you've got a 10 by 10 image, and you're stepping through it with a bigger stride, you might end up with a 4 by 4 image at the end, because you've made it smaller by leaping through, and only calculating a certain number of pixels coming out of it. It can actually shrink the size of the image. It filters and shrinks it typically, and does that multiple times. Now, don't worry if that's a bit abstract, we're going to seea very concrete example shortly. The reason we can see a concrete example is that we can actually plug into the convolution al layer. Remember, basically, we just remind ourselves what it looks like, so it's that neural network again. Imagine that this is the convolution al layer, and it's applying a bunch of filters. We can actually hook into it. We can hook in, and we can visualize it. You can see what's coming out of it in terms of an image. You can see what that convolution al layer is actually doing to the image. This is really cool. I based this code on-- I've just cited it here, because my code has kind of inspired by this code that I use to generate the video and the images you're about to see. Yes, this is it. What we fed in is the image you see at the top left there, which is the break wall game. You can see we've got the bat at the bottom and the ball, and then the bricks and then what we found was coming out of a 32 filter convolution al layer was this. You can see what it's done. Each of these kind of rectangles is a different filter. I've got 32 different filters. This is 16 by 2, and each of those filters is showing what that filter output when we fed in the image, and this is a trained network. This is what it learned by training to see that image. As I say, it allows the neural network to learn what are the most important things it needs to knowin order to reduce its error, because that's what it's basically trying to do. You can see if you look at those images, that in some of the images, it's managed to hide the bricks entirely. These are all of the same screenshot coming in. In fact, it's being fed multiple screenshots, and a block of screens. You can see, in some cases, it's removed the bricks. It manages to invent its own filter that can remove the bricks. In other cases, it's come up with a filter, which only shows certain bricks or certain layers of the bricks. What it's being able to do is that it emphasize different bits of the screen that the agent might be interested in. What I'm going to show you now is actually an animation of this happening in real-time. I've re scattered it, but this is basically a 4 by 8 grid of filter outputs from the first layer in the DQN agent. I trained the DQN agent to be able to play this game, and this is what it could see after it'd been trained. Let's play the video. Look out for the bat and the ball moving around. You can see that the bat's moving around and the ball's bouncing. The reason there are multiple balls is because it's actually being fed the last four frames of the game. It knows the last few positions of the ball, but it could also be using some convolution al techniques to shift, to spread the ball out a bit as well. That's really cool. That gives you an idea of what the neural network actually sees when it's got a convolution al layer. It's learned these 32 different filter kernels to process the image with. That's it. What we've been talking about is convolution. I gave you a very quick introduction to convolution and the idea that you can essentially filter an image using a filter kernel that slides across the image and that the output can pull out different features of the image or process the image in different ways, blurring, edge detection, there's all kinds of other things. Then we talked about how you can have a convolution al layerin a neural network, which actually allows it to learna bunch of different filters, which it applies to the image, which allow it to extract more useful information from the image that is available just in the raw image data. That's the whole point, it's to extract useful information which help it to train better. It's got a feature extraction technique. You've got image feature extraction embedded inside your neural network. This is exactly what DQN is doing. In the end, we just saw a visualization of what a trained neural network is seeing when it's playing a game. In this video, I've just given you an introduction to convolution and convolution al neural network layers."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Weekly outro,https://learn.london.ac.uk/mod/page/view.php?id=96473&forceview=1,"[music]Excellent. You've made it to the end of week 3. We've been learning all about the tooling that we can use to build artificial agents, which can play video games. We've been learning about these things. Firstly, we've been able to use the OpenAI gym library to implementa game simulation environment, we've been able to developa random game playing agent that can operate in an OpenAI gym, and we've also been learning all about the keras library and how we can use it to build neural networks. Especially, we've been learning quite a lot of detail about how convolution works in neural network sand even gaining that insight to see what a neural network actually sees through its convolution al layers when we show it a game. Yes, well done. You've reached the end of week 3. Next week, I should say, we're going to be looking at building the actual agent in code and then running itand seeing how well it plays the games."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Weekly introduction,https://learn.london.ac.uk/mod/page/view.php?id=96476&forceview=1,"[music]Welcome to week four. It's finally time to build and train this AI game-playing system. Let's just look at the learning objectives, and then we'll get some previews of what we're going to be seeing this week. First of all, at the end of this week, you should be able to explain how the DQN agent architecture and learning system is expressed as a working program. Secondly, you should be able to evaluate the performance of the DQN agent. You need to be able to know how you can test whether this thingis actually working or not. Finally, I'm going to show you how to deploy a pre-trained neural network and use it to play games in real-time. That's important because you may not have the facilities to train this thing, but I do and I've trained this thing for you. I'm going to show you that once someone's trained the network for you, it's easy to implement that and deploy that in your code. What about the previews?Well, first of all, the big thing we're going to be doing this weekis we're going to be looking at code, but really importantly, it's really vital that you make the connection between the maths equations that are in the paper and the actual implementation in code. I'm going to very carefully make the connection between these complicated loss function and exactly how that's implemented in the code, which line relates to which, so you can see that the code really directly implements exactly what's in that equation. Then secondly, what we're going to see is the AI playing a video game. You can see here is a sort of reasonably well-trained AI playing Break Wall, which is our version of Breakout. Yes, it's not doing too badly, it's scoring a few points. That's what we're going to be looking at. Eventually, you will have this running on your system at the end of this week. Okay, great. Welcome to week four. I'm looking forward to seeing how you get on with the actual implementation and see if anyone can train a better model than I can to play Break Wall, and I hope you enjoy seeing the full implementation of the DQN agent."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Overview of the DQN runtime,https://learn.london.ac.uk/mod/page/view.php?id=96478&forceview=1,"-In this video, we're going to take our first look at the DQN implementation, which we're going to be working with this weekand see how it operates at a runtime level to see what the steps are giving a general flowchart of the algorithm. Okay, in summary, I'm first of all going to give you a citation because the code that we're using is based on an example code, which I found. I'm going to explain why that is in a minute. We're going to have a quick demo of the code running just to see exactly what it's doing and see some printouts there. Then, we're going to review the agent architecture, check out the flowchart of the agent runtime, and think about episodes and frames along the way, and also the replay buffer. Okay, here is the citation to start off with. This code is really the cleanest implementation of DQN I could find, and also the one that works on current as of the time of filming in 2021 versions of TensorFlow and Keras. I'll just say that in order to prepare this course, I investigated all kinds of implementations of DQN. There's a lot of them out there, and this is the one that I found, which I felt was the most elegant and clean, and most easy to understand, and that's why I chose it because I figured it was the best one to teach with. Well done to Jacob Chapman and Mathias Lechner, I want to give a shout out to them to just say, ""Really very nice, clean code. Super clean implementation. ""It has an Apache license, so it means we're allowed to essentially distribute itand make edits to it, and so on. That's the reference there and I'll give you a link to this. Let's dive in, start off with a quick demo. I've got in my editor over here. What I want to do is just run my slightly edited version of it. I've edited it. It prints out a few things to tell us what's going on. The first thing that will happen is that it's going to pump outall of this usual stuff. If you've used TensorFlow before, you're familiar with pumping out all that stuff. What's happening now is, the agent is playing the game. In the background is loaded up that open AI gym, loaded up break wall. It's observing, it's taking actions, and then what it's doing is it's collectinga history of all the observations, and actions, and so on, as described previously. It's putting all those in, it's periodically using that to train the network. You can see, every time it's printing out the network training thing there, you can see it's basically taking a random sample of what it's seen and feeding that into the network. Then, you've got these less frequent outputs, which is the end of an episode, which we'll talk a bit about in a minute. You can see, and that's telling you how many frames it's played. For example, it says it's played 1, 572 frames. That means, really the game has, we've called step on the game, 1, 572 times. If it's running at 30 frames a second or whatever, that gives you an idea of how many seconds of real-time game play it's been doing. It's running at much faster than real-time. It's playing the game faster than real-time and it's collecting experiences, it's training the network. I've also got this other output, because it builds up the memory buffer over time and that's just printing out how much of that memory buffer is built up to. I'll just say that, I've been able to get reasonable performance using this implementation training on our version of break wall. The original papers, obviously, playing the direct Atari games, and so we had to re implement our own version of the Atari game for this. I've got it to play pretty well. It can keep going for a few turns and it knocks out quite a few bricks. Got an average score of about 20 over time. It's pretty good. I think just because our version of brick wall is different and I had to train it on my laptop, which has got a special graphics card in for the 24 hours or 36 hours to get decent performance. I'm going to give you that pre-train networkso that you can play with it and see what it does. Because it takes a lot of power to train it, it takes a long time to do it, but the network can run in faster than real-time. We can actually play the game faster in real-time with the network. Typically with machine learning, the inference where you're running numbers through the network to get it to actually use it is really fast, but the training is the slow bit. Okay, that's just showing you that. I'm going to stop that because TensorFlow just uses every bit of power it can find in your machine. I'm going to stop that now. That's a quick demo just showing you that it runs and that it does stuff right. What about the agent architecture, which I said we're going to review?Well, we've just been watching this whole thing running in real-time. We've been watching the agent over here, basically, generating actions somehow, and using that to play the game, and then making observations and storing that into the memory, which it was gradually filling up. We saw that percentage going up and up. It's filling up the memory and periodically training the network. Actually, quite often it trains the network every few frames. What we didn't mention here is the epsilon greedy thing, which I'll talk a little bit more about in a minute when we look at the workflow. Okay, that's the agent architecture review. Now, what about the runtime?Okay, over there, you can see the runtime. There's a lot of steps in here and we're going to go and look at those in the code in a minute, but I've got a zoomed-in version of this, hopefully. Up here, let me just find that. Whoops. Yes, there we go. Sorry, about the scrolly screen. I've got a zoomed-in version here. Let me get out of the way. What we're doing here is, we start off by, basically, creating the gym environment. Okay, that's the first thing we're going to do, and then we create the neural network. We define the key or the layers are in the network. We literally define what the network structure isand create that and then we get into the business, which is, while the running reward is less than 40. Remember, I said I was able to achieve like 20 as a running reward. The idea is, it remembers, of the last few games that it's played, quite a few of the past few games. It keeps a running score of what the average scoreis to make sure that it's consistently learning. It doesn't stop until it's consistently able to play to a good level. In this case, in the original script, for the Atari breakout, the consistently, good enough score is 40. That's, basically, if it can knock out 40 bricks consistently, the whole thing will exit and that's the end. That isn't really going to happen very quickly. The next thing it does, it carries out an episode of 10. 000 actions, okay. Each episode is a lot of actions, and actions, remember, are like one step in the game. Basically, choosing what to do each frame in the game. Then, what we do, is we do the epsilon greedy thing, which I think I mentioned before, which is the idea that most of the time, it takes random actions. At the beginning, it takes random actions. In fact, if we look at the code, let's go back into the code to see what those properties are. At the beginning, you can see that this is the actual code here. Epsilon starts off at one. Beginning, the chance of it doing a random action is 100%. Okay, and basically, over time, that reduces. Over a certain number of frames, epsilon reduces from its maximum of 1 down to a minimum of 0. 1. We still retain some random action, but it only goes down to 10%. Over time, it switches from taking random actions into using the network. As it's running, the neural network's getting better and better. It starts off rubbish, it gets better and better, and it gradually shifts on to making decisions using the neural network. That's the epsilon greedy process we talked about before. That's the thing it's doing there. That's what we mean by reducing the chance of random. That's basically epsilon being reduced. Then, the next thing it does is, yes, either selects with random or the Q function. Basically, either uses random or it uses the Q function, which is the current neural network. Based on that, that reducing chance that it takes is action, acts in the game, and then it stores the results. We saw earlier in that last weekend in the open AI gym, that we get the states, and so on, back. We store all that into the replay buffer, and basically, if we've done a certain number of actions, then we're going to update the network, we're going to retrain the network. We saw those printouts coming out. We saw their training network prints out coming out. That's what's happening there and then after a certain number of frames, what it does is it updates the Q network, the Q dash network to the Q network. I'm not sure if you remember back, but when we were going to look at the loss function in detailin the next video, but it's actually maintaining two neural networks. One of them is the current one that it keeps training and the other one is used for the loss function, and that one is has got an older version of the weights. Okay, what it does is, every often, it updates the Q dash oneto be to have the latest weights, but not very often. That gives it this stability because if the network that it was using for the loss function was the same as the currently trained network, they found that it didn't train very well. What they do is they keep a network static for a while, and then occasionally update it to the latest weights. Okay, that's how that works and then at some point, we basically start rewriting the memory. Once it fills up the memory to a certain size, it starts writing back at the beginning again, so it wipes over. It's like a looping buffer thing. Okay, that's a general idea. That's what it's going to do. How does that work in the code?Well, let's look at this code here and find out a bit more about that. I want to go through. Here's the Create Model Code and we're going to look a bit more detail at that in a later video. That's the basic idea. That's the function that creates the model, then there's a bunch of configuration parameters to do the memory size, how long epsilon takes to reduce down to 0. 1. How often you're going to retrain the network, how often you're going to update the secondary network to the latest version. Then it defines a type of loss function, which is used to calculate the loss between the input and the output. Then, we got the main loop here, which is an infinite loop, which will only exit if we get that continuous score of 40. Basically, you can see it starts off by resetting the environment and then we then iterate over max steps per episode, which we can find up here somewhere max steps per episode it's 10. 000. As I said earlier, it runs those 10. 000 steps. In this case, we're calling that an episode, but typically an episode is playing the game until you lose. Okay, if at some point we lose the game, then we need to reset the environment. Okay, but otherwise, it will try and keep playing the game10. 000 frames if it can. Then, we're obviously counting the number of frames. In this case, this was actually rendering the environment every time. We should be able to see the window, the environment there. Then, what it's doing is then--that's the epsilon greedy stuff. Basically, if we want to choose a random action or if we want to use the network to generate an action, and then we update epsilon, as we said. That's changed. Over time, it gradually reduces the epsilon. The chances of using the network for an action increases over time, then that's where we actually do the steps things. You've got step, that's where we actually act in the environment and we're calculating how much reward have we gathered in this episode. We store the reward, the total reward before we die. Okay, that's really how many points have we gained. Then that's managing the Action Replay buffer. The action replay buffer is actually in this implementation multiple lists for simplicity, you could make it a class that takes in the state action rewardin next state. In this case, we're just maintaining some simple arrays there and just sticking the latest things on the end. We, basically, got for each of the elements of the replay buffer, we've got a separate list. A certain point, we basically do our--What's this?This is the training. This is training the network. To train the network, which I'm going to look a bit more at this in one when I cover the neural network, but basically, we select a bunch of random examples from the replay buffer and we use those. We run those through the network and we then calculate the output of the network and eventually, that gives us the loss over here. That's probably the most gnarly bit of code really to just calculate the loss and then we feed that loss back into the network there. That's basically saying, ""Okay, you're updating the network and if we've done a certain number of frames, then we update our Q dash network with the latest network. ""As I was saying, anyway, maintain two networks here. Finally, that's the bit of code that wraps around the replay buffer. That's really it. If you trim this one down, it goes down to like 189 lines, basically. It's like not a lot of code to read through and figure out exactly what's going on. Yes, as I said, the replay buffer is actually a set of lists containing all of those thing sand that's clear and we saw that in the code. We'll cover more about the training in the next later videos, but for now, that's giving you the general idea of what the runtime of the agent is. We've run the agent with a quick demo there. We've seen who's written the code. We've been through the architecture, been through the runtime. Finally, we just thought a little bit about episodes or episodes, is this thing of running, playing the game until you die, basically, up to 10. 000 frames, and then the frames of one step in the game. The replay buffer is this set of lists, which remember what's happened in the gamein terms of states, actions, and rewards, allows us to build that kind of state transition matrix stuff for the MDP. Okay. Right, in this video, we've just been taking our first kind of deep dive into the code of the DQN agent and trying to match a sort of flowchart of the runtime into the actual code. I've highlighted the blocks of the code where each of the key things that it's doing are happening. Hopefully, you've got some idea of this cycle that it's doing now of, basically, playing the game for a bit, training the network, occasionally swapping, updating the long term network, maintaining this buffer of experiences, and eventually exiting if the performance gets high enough over an average time."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: The DQN neural network architecture,https://learn.london.ac.uk/mod/page/view.php?id=96480&forceview=1,"In this video, we're going to be looking at the actual neural network architecture that is used for the DQN agent. We're going to start by just having an overview of the architecture, then we're going to go through each of the layers, and just look at inputs, convolution al layers, looking at the different features of the convolution al layers. Then we're going to look at the dense layers at the end. Finally, the output layers, and we'll look at a summary and see how many parameters there are in the whole network because it's quite a sophisticated piece of kit. There is the overview of the network. You can see at the top, we've got the inputs coming in, and then we've got three convolution al layers. You remember from my videos previously that a convolution al layer can actually apply many filters. In fact, we'll see how many filters each one of them applies. For example, you'd have 32 filters being applied in a single convolution al layer, so they're pretty sophisticated layers. We've got three of those, and then we've got a flattening layer, which I'll talk about more in a minute. It basically takes the really complicated output of a convolution and turns it into a single layer of nodes. Then we've got two densely connected layers which are layers where everything's connected to everything. Then finally, the output. Just to remind ourselves, this is the Q-function. In Q-learning, the problem with Q-learning is getting that Q-function, which tells us the value of all of the available actions so we can choose what to do given the input. The input is the state and the output is the value of all of the actions, and that's exactly what a Q-function's supposed to do. That's the purpose of this network. There's that citation again. Chapman and Lechner, just noting that they wrote the code which we might look at in a minute. Let's go through the layers. First of all, we've got the input layers. You'll notice that we've got 84, 84, n, there. In the DQN paper, what they do is they take the screenshot of the game, which in our case in Break Wall I think is 800x600or something, and they resize it to 84x84. There's actually some code in the Atari Gym. Environment which resizes it down to that size, so before it even gets into the neural network, the image has been resized. Not only that, but you see you've got this n thing there, n is the number of frames. In this example, we've got 4. If I look at the properties of that layer. Here, I've got my network summary. With Keras, you can call model. Summary, and it will tell you all about the model you've just defined. You can see that we've got. . . That's the size of the input. It's 84x84x4. This is quite a common thing for an image input. You have your XY frames of the image, and then the XY define the frame of the image, plus you've got multiple channels. It's not a black and white image. It's typically got red, green, blue, and then an alpha channel. We're cheating a bit. Instead of having all the different colors, we've got just a black and white image, but then we've got the last four frames that we've seen of the game. It's sneakily feeding in a bit of history to the networkso it can maybe see where the ball has already bee nor what's happened in the last few frames. That's the first thing. That's the input, and that's the simplest layer. You can conceptualize the input layer as just reallya bunch of nodes 84x84x3 which means it's a tensor. It's 84x84, so we've got 84 that way, 84 that way, and then it goes that way 4 as well. It's a kind of cuboid structure, which is what we mean by tensor. That's what's coming in. Incredibly complicated input layer there. That allows it to receive the image. Yes, said that. Then convolution. We know about convolution. I talked about that in a previous video, so I'm not going to repeat myself. It's basically this idea that you can filter images, and we know all about that. The first convolution al layer has this specification. If we look in the code, we'll see exactly that. It's a Conv2D, which makes sense because it's a 2D image coming in. You might use a Conv1D if you're, for example, processing a time series like audio. You might use a Conv3D if you're processing a time series of video frames or something like that. What is it?It's Conv2D. You've got 32 filters. That's the first thing. Remember the visualization that we saw before where I showed you the different things. We've got 32 filters being applied to that image. The size of the filters is 8, so it's an 8x8 filter, and the strides is 4. It's skipping four steps each one, so it's not actually processing every pixel individually. It's jumping through the image missing out some of the pixels. If we look at the size of that over here, you can see that it comes out actually 20x20x32. 32 is the number of filters and 20x20 is the size of the output of each filter. Actually, it's going down. The convolution is scaling it from 84x84, so that image is being scaled down to 20x20once it's gone through each of those filters. Then we've got 32 instances of that image, one for each filter, so it's got 32 images coming out of the other end but they're scaled-down. That's it. Then the next thing is. . . Oh, where's the. . . Sorry, I'll just goand look at the code for the other convolution al layers. Yes, there we go. Here are the other convolution al layers. The next one is 64x4. That's a 4x4 filter being applied. 644x4 filters being used to process the image. That's got a stride of 2, so that's moving more carefully through the image if you like. Instead of skipping four pixels each time, that's going 2, but then given that they're only 20x20, you want to be stepping through those fairly carefully. If we look at the size of that one, it's actually then giving me a 9x9 image coming out of that filter but 64 of them. You then got 649x9 images filtered, so double processed. Then we've got another layer again which is another 64 filters with a 3x3 filter, and that scales it down to 7x7. That now, coming out of there, you've got really tiny images coming out. 7x7 but 64 of them. Remember, the whole idea of these convolutionalfilters is about reducing the amount of data and just pulling out the salient features. That's what they're doing. They're just trying to just compress that image down. It's a form of compression. We're taking out just the most important features of the image that allow the network to train properly. What's next?We've got a flatten layers. A flattening layer. Because this convolution al layer is this complicated thing where it's actually. . . What was the shape again?It was 7x7x64. We've got this 7x7, so 7, 7 by 64. We've got this quite complicated structure coming out of that. What we do with the flattening layer is we basically wire all of that. We flatten that back out into just a flat layer of nodes like that. You can think of that as being7x7x64 numbers. Each of those circles is a number, and so we're unpacking them all into a single list, basically. That's what the flattening layer is doing. Hopefully, it makes sense that 7x7x64. . . If I just do a quick calculation. Python, 7x7x64gives us 3, 136. Does that make sense?Yes, because you can see that the flattened layer has got exactly the same number of units in it if you like, but it's a single flat layer, which makes it easier to connect up. Then we connect that to a densely connected layer of 512. What we do then is we go down from that 3, 000, so that's 3, 136 of those. It goes down to a 512 densely connected layer which is smaller, basically. What do we mean by densely connected layer?Well, that means that each node in the 3, 136 layeris connected to each node in the 512 layer. That's what we mean by densely connected. That means that each node gets to contribute to the activation on the next layer of every node, but obviously, all of the weights, all of those lines have a trainable weight. That's what we mean by the number of parameters, which you can see if I just disappear. You can see that the number is telling us how many parameters each level has. That first convolution al layer has this many trainable parameters, and then even more, even more, and eventually, the whole thing has 1. 6 million trainable parameters. We've got two dense layers, the second one. Finally, we've got a densely connected one to give us the outputs, which is 4 because there are four possible moves, which I don't know what exactly. . . I thought there were three left, right, and do nothing, but there must be a fourth move on the Breakout game. That's why you've got four moves at the end there. The idea is that finally, at the bottom of this network, we get four simple outputs which are for each of the moves, and we just get a probability. That is what the key function outputs. That's the value of each of the possible moves that we can make. That is what we want in the bottom of the network there. That's it. Before we go to the summary, let's just view that overview again. Just to review what we've done there. We've got the inputs coming in, we've got three convolution al layers, we've got a flattening layer, and we've got two dense layers finally leading to the four output points at the end there. It gives us these 1. 6 million trainable parameters. All those parameters rule those filters, all the weights connecting everything up, all the weights in those densely connected layers, all of that stuff. You can see why there's so many parameters. Let's summarize now. In summary, we've just been going through the network architecture for the DQN agent and sort of referencing the code and just looking at how many parameters there are and everything else and just reviewing convolution a little bit and talking about densely connected layers and flat layers and so on. Finally, that essential output layer that gives us the four values for the different possible moves that we can make, which is what we want to make our decision about how to play the game. In this video, we've just been going through the neural network architecture that's used in DQN."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: DQN loss function and training,https://learn.london.ac.uk/mod/page/view.php?id=96482&forceview=1,"[music]In this video, we're going to be talking about the DQN loss function and digging into the code to see exactly how that is used to train the neural network. Summary. First, we'll review the loss function briefly just to remind ourselves what to expect in the code and then we're going to see how we select items for training, how we compute the loss, and then how we compute the gradient and apply that back into the network in order to train it. Quite a lot to cover but let's dig in. There's the citation just to note, the code we're going to look at is from Chapman and Lechner and the loss function. I'm going to jump straight over to my drawing program here just to show you the loss. Do you remember this from a couple of weeks ago, the loss function?This is derived somewhat from the Bellman equation sand the idea is that what we're saying is, what is wrong with this network is the difference between what the network predicts we should do, and what our best guess predicts we should do. Normally, when you're training neural networks, you have a ground truth. For example, if you're training it to recognize images, you have a load of labeled images of cats, and then you can tell whether it's got it correct or not. The problem is, we don't have that data. In this example, we don't know exactly how much each action is worth. What we have to do is you have to make our best guess about what we think the action is probably worth, and then try and get the network to approximate that. How do we get the best guess?Well, let's break down this equation. Just remind ourselves. We've already done this, but I'll do it again. First of all, when I said the best guess, that bit is our best guess. This is the training data. That is what we do. That's a sample of items from the replay buffer. We got some states actions, rewards and next states from the training buffer. That is a uniform sample of random examples from the training buffer, and the point is, in that sample, we know what the reward is going to be. Part of our best guess, this is our best guess, what we might call the ground truth, or as close to a ground truth as we're going to get. Part of that is actually the reward, the known reward. That is ground truth. We know what the reward is but the thing is, this thing needs to know what's going to happen in the future. It's not just the immediate reward that we'll get from doing this action, it needs to be thinking a few moves ahead, to gather what is the best action to lead to a future reward. That's the trick. We can't just look at the reward, we have to also look at some reward going into the future. How do we know what the reward going into the future is?Well, we use the Q-Network, which remember is this old version of the network, which we're updating every 10. 000 frames, or whatever it was. That is the previous version of the network. What we do is we got this gamma thing, which is a discount factor applied to that. We're saying, what is the known reward?That's the real reward we'll definitely get in the next step, or at least in the past observations, and that is the discounted guess at what the reward will be iteratinga few frames into the future to what we call the horizon. That's our best guess of what is the right output for the network. The network should output something like that. What we do is we say, well, that's our best guess because that's Qand that's using Q-, remember, which is the older version of the network, and then this one is using, sorry, it's theta dash. Then this is the current version of the network. There's no dash there and it's saying compare what our best guess is to the whatever the current version of the network is calculating. Then it's a mean square squared error deal. Although we're not actually going to use that in this implementation, we use the Huber error. The way that we calculate the error between those two, that's the error function if you like. That is the loss function. The loss function is basically telling us of the latest version in the network comes out with this value and what is our best guess of what the value should have been the ground truth, we compare the two and that gives us the loss. That's what I wanted to say about that. We've kind of already covered that. I just wanted to remind you because it was a little while ago. What does the code look like for that loss function?Let's go through. First of all, we want to know, let's look at the code to get this bit. That's the first bit we're going to find in the code. How do we sample from the replay buffer?Well, let's go in and find the sampling bit. There's the code, here's the training bit. That is it there. What do we do?We basically take, this is essentially saying, if the whole replay buffer were in fact, remember the replay buffer is multiple lists, but we can just choose one of them. We take the whole list, and we take a random set of samples, the size is batch size, which is I think, 32. We basically choose 32 items from the replay buffer, and then we basically sample. That's the state bit. That's the S bit, that's the S dash bit. That's the next state. That is the R bit, and then that is the action. That's our random sample. Those are the bits, which we see here. That's all of that, randomly sampled from the available replay buffer. Good. We've got that bit and then the next step is how to. . . that's selecting the 32. The next bit is then basically calculating what the network thinks the reward is. That's calculating this bit. We've got to calculate that bit and we've got to calculate. . . well, we know what that bit is, because that comes with the dataset, because that's the R there. We're going to calculate that and we've got to calculate that. Let's find that in the code. Let's see what we got. First of all, we have the. . . yes, there we go. That bit see if you can guess what that is, future rewards. Model target is your Q-Network, right?That is your Q-Network. What's that?The future reward is exactly this. That's calculating that bit. We literally just pass in the next state and we use the network to predict. Model target is the neural network which has the old version of the weight sand we could predict on that, which essentially feed sin the sample of next states and it gives us back the rewards, which remember are going to be for each of those states that we're passing in, it's going to give us the predictive reward that that network thinks we're going to get. That is that bit and then we need to discount it. There we go. We got the gamma's in there. That's the gamma. It's all there. There's the gamma, gamma, gamma, gamma. That's there and then what else have we got?That's all that and then we basically do one hard encoding. We choose the. . . I think we're basically selecting the action and then we've got the bit here where we actually calculate the Q-values using the model. That line there is doing it on the model. That is. . . sorry to keep jumping around, but I want you to be really clear on which bit of the equation we're looking at. This is the bit where we calculate what our very latest network is generating. That's what we think we're training on. That is there. You can see we're passing in models. We get the Q-values, and basically filter them a bit, process them, and eventually, we call loss function. Oh, did we add the reward?Where do we add the reward?Just going to see where we added the reward?Oh, yes, there it is, it is there. To calculate the ground truth, we did add the reward and we then do the gamma scaled version of the original network, and then we recalculate the difference. That is where we do the whole thing. That line there is doing this. That's where we do this subtraction and it's calling. It's passing then to loss functions. What is loss function?Where is loss function?See if we can find that. I'm going to search for it, loss function. There we go. Loss function is just one of the standard losses available. In fact, they're using the Huber loss, which is a way of calculating the distance between two sets. Oops, I delete it. There is and that's it, and then that gives us basically the loss. Then because we're using a custom loss function, this custom way of calculating the ground truth versus the predicted truth, we then use this tape gradient technique to essentially workout what the gradients are in that loss, which are basically the distance between what the network predicted and what we were hoping it would predict if you like, the direction of those distances. That's your calculus bit. It's like calculating the gradients between where we are and where we want to be. Then that line there, essentially just back propagates those gradients back into the network. That's where the network weights are actually updated. That's more or less it. I'm going to stop there. I just wanted to show you exactly the mapping between the code and the loss function. You can see there's a direct mapping. Every bit of that equation is implemented in the code, there's a line of code that does it. I missed out a little bit of detail there but you can goand read further exactly what all those Tensor. Processing functions are doing but that's essentially it. In summary, we've just been looking at the loss function. We found out how it selects items for training and then we've made a mapping between the original loss function equation from the research paper into the code and seen how we compute the loss by having the known ground truth, which we extract from that Q-Network versus the current network. Then we compute the gradient and roll that, back propagate that back into the network to update those 1. 6 million weights in the network, accordingly so that it'll get a better prediction next time. In this video, we've just been deep-diving into the loss function code in the DQN implementation to find out exactly how every bit of that equation is implemented in code."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: DQN code review,https://learn.london.ac.uk/mod/page/view.php?id=96484&forceview=1,"[music]In this video, we're finally going to see the DQN agent playing a video game. Let's look at the summary and see what we're going to go through. First of all, I want to give you some information about how I went about actually running the training sessions because it wasn't as easy as you might think. It's not just a case of running the script. I'm going to show you some plots of how well the training went in a couple of examples sessions. We're going to talk through the way that I can then use the agent to play the game in real-time and view that, so we're going to see how that script works. We're going to have a demo of it actually playing the game, and we're going to reflect on the experience. First of all, when I was developing this course, before we get into that, my initial phase, it was just really trying to figure out what's the best way to show you the DQN agent. I looked at loads of different implementations of the DQN agent in different languages and things like that and I settled on the one that I chose because it's a very clean code. My first step was to verify, ""Does this actually work?There's no point in me going through it if it's not going to work in the end. The first thing is I verified that the DQN agent could train to play the Atari Breakout game. Indeed, it does train very well. We'll see a graph that proves that shortly, but unfortunately, because I'm not able to show you the Atari Breakout game itself because of copyright reasons, we then created a clone of Breakout called Break Wall, which is kind of similar but it's different in certain ways. I attempted to train the same agent to play that game instead. It wasn't quite as successful but it still clearly learned some technique on how to play. We're going to be seeing the demo of that agent playing it shortly. How do I go about training it?The thing with the Keras script is that, it has this one problem which I encountered, was the size of the replay buffer. In the original nature paper, the replay buffer is a million units. They have a million states and after-state sand everything else stored in the replay buffer. That takes a lot of RAM. In the Keras script, they've reduced that to 100. 000. I found that on a 16-gig machine, I still couldn't actually maintain that size of memory buffer. It would just crash out the script. My plan of, basically, training on my learning machine here didn't work because it only has 16 gig. I, first of all, tried it on Co lab and on my regular Mac. I've got a 10-year-old Mac that has 24 gigs of RAM on it. I was able to run the script on both Co lab and the regular Mac. If you haven't seen Co lab before, it's Google's Jupyter Notebook system, which is really good because you can access it for free and run training sessions, and it has a GPU backend. You can actually get access to an accelerated training environment for TensorFlow. That's pretty cool. I was able to train it on Co lab, and also on my unaccelerated Mac, but, obviously, that was quite slow. Finally, I've got some RAM for this machine. This machine now has 64 gigs of RAM in it, and this is my learning machine. It's got a Ge Force 2070 chip in it, and that allows me to train much faster. I did a lot of my training on that machine. One of the models we're going to see later the results of only just finished training it. Let's look at some training here. This is the agent being taught how to play the original Atari Breakout. The graph is going up. Graphs going up is generally good, in machine learning, means it's getting better. What does the graph mean?The graph is the running average score that it's achieving across multiple episodes?Remember, an episode is basically the agent playing for either 10. 000 frames, or until it loses the game, so the ball goes off the screen. This is the running average score. You can see over many 1, 000s of games. It is eventually getting some pretty decent scores there. It's clocking in a regular 10-point score, which means it's knocking out 10 bricks very consistently. Probably, most of the time, it's getting more or less, but averaging out 10 or 11. That's the first one. That's my proof that it is able to learn how to play Atari Breakout. I'm going to give you this model. I can't show you this model playing the game, but I'm going to give you this pre-trained mod eland a script that lets you run it on your machine. You can see actually an AI playing Atari Breakout on your own machine, because whilst the training does require loads of RAM, and, ideally, a graphics card that accelerates the training, actually running the model's very fast. It doesn't require as much RAM, hardly any, and it is very efficient. Thisis the slightly more disappointing training of Break Wall. You can see it is going up. There are some points when it got pretty high, but it's not quite as consistent. The thing is, I might need to train it for a lot longer to really get very good, but what happened is, while the training was going on, it actually fell off the cliff. You see it just dropped down. The laptop didn't literally fall off a cliff but, certainly, the running training performance did. This one, I had to really abandon. I will try a few more runs, see if I can get a better model for you. I've got a reasonable model. We're going to see it later, which I did in another run, but you can see that clearly Break Wall is not training as well. We're going to talk a little bit about why that might be later. Now, I want to just actually run the script and show you, but before we do that, I'll just remind you of the agent architecture. Actually, we're not going to use that bit, so we cross that out, cross that bit. We're only interested in this bit for now because this is the bit we're going to be running. Essentially, we're going to be running the game. The game is going to be running. We're going to take those screenshots, the four screenshots. We're going to be sending them through to the Q function, which is essentially the trained neural network. The Q function, remember, is our trained neural network. It's going to get the status. It's going to tell us what the value of each of the actions is, and then we're going to take the highest value action and do that, and then keep going. That route goes round and round, and it'll play the game. I've got a script that does exactly that. Remember, I've removed those two bits because we're not remembering what happened and we're not training in this version. We're just running the model. I've got a script that does that. Essentially, what it does is, it creates the model. Just like we did before, it creates the Keras model. I can just quickly show you the code for that. It's really the same code, the create model code. It creates all the layers and returns the model, and then, it loads the weights. The next step is that it loads the weights. I'm just finding where that is. There we go. It creates the model there. Loading the weights is actually super easy. You just call model, load weights like that, and then you pass it the weights file. Where did I get the weights file from?The training script I have, what it does is, every time it hits a historic high in the running, training performance, it will just save the weights down to a file. I've got a folder over here with all of my pre-trained models here, and some of which I'm going to give to you. That's the Atari Breakout one. Those are the best ones that it achieved, and then, that's the one. There's a few different snapshots of it playing the Break Wall, and there's another training session I did on my Mac. Basically, I've got loads of different pre-trained models. I basically load one of those in and call load weights, and then, it loads those 1. 6 million parameters into the model. Once you've loaded the weights, you create the environment. I've got a little function here that creates the environment. The core of that is this bit. It isn't just as simple as creating a regular type of Gym environment because this Gym environment doesn't just give you the raw state. It actually pre-processes the state a little bit. If you remember back to quite a few videos ago, the image processing, so remember, where we turned it into a gray scale image, and then, we store the last four frame sand we also resized the image before it even goes into the neural network. That's creating a specialized version of the environment by wrapping it in these special functions which come from the open AI baselines package. That basically gives us the same environment as they're used in the paper. That's the idea, more or less. Once I've created the environment, I've got my model, I've loaded my weights, I've got my environment. Now, I just need to do the runtime. That is pretty straightforward. Let's just go through that. This is the function. We start out by resetting the environment and storing the state that we got, and then, we call render on the environment. That pops up a window to show us what's going on. We convert the state that we receive from the reset function into an appropriate format, the tensor format that we can pass into the model. We basically pass that state tensor into the model which will give us back a set of probabilities of really the values of all of the actions. We select the highest value action with this tf. argmax function there. That gives us the highest value action, and we then take that action in the world by calling env. step, passing it the action we've chosen, and then, it receives the results of taking the action, the reward, and the done state. If the thing's done, if basically it's lost the game, it just resets the environment again, and then, it all starts again. That's essentially it and that is the script. What we can do now is run it. I'm going to go over here and run it. This is the one that I trained that we just saw the graph for. You can see it's missing sometimes, but it's clearly sometimes. It's not too bad. It's actually getting to it. It knows where the ball is. It's figuring out where the ball is and getting the bat over to that. You see it got it there and it's going to go. It got all the way up to the side. It managed to predict where the ball was going to go. Clearly, it's learned. It's not exactly the same. When you run the Breakout model on your own machine, you'll see that it's much better. It gets a much higher score, but this is not bad. Clearly, it's not just random, is it?It's definitely following the ball and getting the bat in the right place. Clearly, this has learned how to play the game. The reason that's interesting--You might say, ""Well, it's quite a simple game. It's not that interesting. ""The reason it's interesting is because we didn't tell anything about how to play the game. We only told it when it gets a point, and getting a point can happen quite far in the future, can't it?It might actually only get the point after several runs. I'm going to get that out of the way. You might have to wait for age. There's the point. Imagine how many frames there are between that and the point. Hits the ball, wait for ages for the point. Oh, lost it. That's why it's interesting because that's the reinforcement learning thing. Suddenly, oh, you got a point and the move that you made to get that point happened ages ago or it's a whole sequence of moves you have to make in order to get that point. That's what is really interesting. Also, we didn't tell it anything about the game. Just by looking at that view, it had to figure out what is the best way to process that with the convolution al layers and everything, training all of those filters in the neural network to extract the salient information from that image. Just out of interest, let's just look at a less well-trained model , one which is here. Let's have a look. What have we got?This is after 145 episodes, so basically, not many games. This is it running with much less training. You can see here basically the strategy that it's learned so faris just to move over to the right. It's not acting randomly. It's not just random, is it?It's just basically figured out, sometimes, when I move over to the right, I manage to hit the ball, so I'm going to do that just about every time. You can see, it's a bit reacting a little bit to the ball. That's what it looks like at an earlier stage of training. Some reflections on that. First of all, I will give you that modelso you can run that on your own machine and have a look at it. I will also give you the Atari model which you'll see is a vastly superior in performance and perhaps more impressive. What can we say?It's not amazing, but it's clearly learning some techniques, as I saw earlier. Why is it not as good as the Atari one? It's a completely different game engine. We actually built it all in Pi Game, and the way it generates pixels and the drawingis also a different resolution. Even though it gets scaled down, it starts out at a much higher resolution, so who knows what's going on there. Also, confession time, it was quite a hacky adaptation I didto get the game that was written in Pi Game and convert it into a Gym, and then, to get it to wrap up in the right wayso that it looked like an Atari game, so I can just have it interchangeable with the Atari Gym. Maybe, I've made some little flaws in the conversion there compared to what it should be. Clearly, it works, but just not quite as well as the Breakout does. In summary, we've just been looking at a real trained network and seeing how it plays the game. I started out by giving you some information about the practicalities of training it, how I tried out various different environments. I was running it on my machine that I've got over here. I was training it online in Google Cola band running it on an old computer for days. I tried all kinds of different things. That was just my process of figuring out the best way to train it. I showed you some plots of how well it trained, and we saw clearly the Atari Breakout version of the environment. It trained better on that than it did on our adaptation of that but, still, when we ran the simulation, we were able to see that it actually played BreakWallreasonably well as well. Definitely getting a few points. That's certainly a lot better than random and following the ball and everything. We, at the end, just reflected on maybe what the difference is a couple of things. We just didn't run it for as long. There was an issue where it technically fell off a cliff, the performance, and I haven't fully debugged that yet. Also, various other things that were different, the way I converted my environment into the appropriate one for the Gym. In this video, we've just been looking at a real AI playing a videogameand seeing that it can learn how to do it. The performance is not bad, but just also acknowledging the practical difficulties of training, and also having to adapt it to a new game environment."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Weekly outro,https://learn.london.ac.uk/mod/page/view.php?id=96489&forceview=1,"[music]-Excellent, you made it to the end of week 4. You should hopefully now have a working agent running on your own machine and you should be able to see it attempting to play these video games. What have we been doing this week?Well, first of all, we should now be able to explain how the DQN agent architecture and learning systemis expressed as a working program. You've seen all the code, and you've seen the connection between the code and all of those equations and things in the paper. You should be able to give some insight into how to evaluate the performance of a DQN agent. You should be able to deploy a pre-trained model to play video games. We provided you with various pre-trained models. Unfortunately, we couldn't show the Atari game on the video, but hopefully you've downloaded the mod eland you've been able to see how well it can play breakout after it's been trained. Well done, you've reached the end of week 4. One more week to gowhere we're going to be looking at what happened next. What was the next best system after DQN in 2015?What have people been up to for the last few years since then?"
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Weekly introduction,https://learn.london.ac.uk/mod/page/view.php?id=96492&forceview=1,"[music]Welcome to week five. This is the final set of content on the game-playing AI, where are we going to be looking a little bit at the state of the art and also considering the ethics of game-playing AIs. Here are the learning objectives for this week. First of all, I want you to be able to present examples of human competitive AI game players. I want you to be able to reason about the impact of human competitiveAI game players on human beings. Finally, I want you to be able to justify and explain the trend towards general AI and discuss its potential impact on society We've kind of already considered this trend of general AI earlier in this content, but we're going to sort of revisit itand think about what that means for fairness, especially in games, and also what the ethical impacts of that all. To give you a quick heads up, this is an example of some of the references you're going to see. These are all the systems that people built after DQN in 2015, leading up to the state-of-the-art system as of 2020, which is agent 57, which is deep minds of all conquering Atari game-playing bot. We're also going to be looking at various other video games as well. That's it, welcome to week five. I hope you enjoy finding out using your sort of new knowledge of how these reinforcement learning systems operate and applying that to looking at some new systems and seeing how you can better understand them now that you've seen have this deep dive into your own sort of reinforcement learning system. Good luck viewing all the content and doing the activities."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: State of the art game players,https://learn.london.ac.uk/mod/page/view.php?id=96494&forceview=1,"[music]In this video, I'm going to be reviewing some of the things that happened around and after the DQN agent paper that was published in 2015. In summary, we're going to talk about progress that happened with Atari video game playing after DQN, and then we're going to loo kat some other games. We'll look at Star Craft, which has been focused on by Deep Mind, and then Dota 2, which is OpenAI, a similar-looking company. They worked on Dota 2. Then we'll revisit the Mario AI Competition and see what's happened there. This is a list of papers, and I'm going to give you these reference sin a reading, but these are things which happened after DQN. Number one, they invented a thing called Double DQN, and that led to the rainbow system. What I want you to do with these reference sis take what you've learned about DQN and think about the key elements of the system. There's the general runtime of the agent. What does the agent do?That's one thing. Then thinking about, how does it gather experiences?How does it build that state transition matrix?How does it gain an understanding of how the environment operates?In DQN, obviously, it's the replay buffer, but then these systems, they come up with new more and interesting ways of doing it. Beyond Epsilon-Greedy, what's the next exploratory behavior that's required to gain better performance?The other thing to look at is, obviously, the neural network architecture. What kind of neural network architectures do these different systems use and how did they advance that?Also, what was the loss function, very importantly?How do they compute the ground truth when you don't have ground truth?We saw that DQN did it in a very interesting way by having an older version of the network and combining that with the immediate reward plus some prediction of what the reward will be in the future and then using that to train the networkas ground truth. Those are the things to look at when you go through these and following on from double DQN, we had Dreamer version two, and just to say that that research paper has a really nice set of references in, so that's a good reason for going and digging that one out. A lot of this stuff is open access, on archive, and things like that. Then you got R2D2, which is another Atari game-playing bot. Finally, number four, 2020 DeepMindpretty much put an end to all of this by building an agent architecture and publishing, which just is better than most professional game players are, all of these, the benchmark set of 57 Atari Games. That's why it's called Agent57. Even the games where it takes an extremely long time to gain a reward. There's zero reward for thousands of turns. You have to explore for ages before you get anything. It's able to play even games that are really complicated games with lots of complicated visuals and things like that. That's almost the end of the work on Atari Games, as I would say, because it's clearly out competing human players, and it's a general system that plays all of the games with the same setup. Probably you have to retrain it for each game like the other ones but nevertheless, that is where it's at now. That's pretty much you can consider playing Atari video games better than human beings as they solve problem as of 2020 on Agent57. What other games are there? Well, Star Craft is another game. I'm going to drop down to the second quote there. This is the second one. The original game and its sequel, Star Craft 2, have several properties that make it considerably more challenging than even Go. That's really interesting. Remember that Alpha Go was the Go-playing system. Now Alpha Star further worked towards general AI system. The idea is that Alpha Star is a very general-purpose system that can learn to play all kinds of different games. In 2019, it was able to play Starcraftto the standard of professional players and beyond. It was able to beat professional players at what is considered to bean extremely hard video game. The bottom quote, this one's really interesting. Alpha Star draws on many areas of AI research, including deep learning, reinforcement learning, game theory, and evolutionary computation. Just go and read the paper, read about it, evolutionary computation was in there as well to show that that is absolutelya state-of-the-art technique that's being used to evolve neural networks and training systems and so on. That's Star Craft, what about Dota?StarCraft, that's Deep Mind versus OpenAI if you like. Dota is another video game which is considered very difficult and involves multiple humans playing in teams against each other. In 2019, it won back-to-back games versus professional world champion players OG as a team. I guess that means original gangsters or something. It's able to be a really excellent professional team and was the first AI to do that. Then further, beyond that, in 2019, they put the system online so people could log on, and with certain constraints, they could play against the AI in real-time in competitive games. It won 7, 215 games and only lost 42. That's pretty impressive and that is addressing some of the problems. Remember, the research paper we're looking at that was called When Are We Finished Playing Games, and in that paper, they discussed the unfairness of many of these competitions between AIs and human game players. Clearly putting it online is addressing one of those things, you're letting anyone play. It's very uncontrolled environment. Although they still didn't address all of the problems. For example, it was quite a constrained version of the game, you can only play with certain team member sand things like that. Also, of course, the AI didn't have to operate a robot hand to press the buttons on the controller. There's all that complexity, which humans are doing as well, which the AI didn't have to do but still very impressive. What about the Mario AI Competition?I'm putting this in here because Mario AI Competition predates this. There's a lot of this Atari stuff. Remember, the original arcade learning environment was 2013 or around that. Before that, there was the Mario AI competition, that's when these game-playing bots became popular maybe. Basically, what happened is, I think they got some heat from Nintendo for publishing pictures of Mario Games all the time. I think they mentioned that in the paper, but also, they found their other more general-purpose competitions, like the platform AI competition. They also note, they mentioned the Ms. Pac-Man Competition. There's a few other competitions grew out of it and were inspired by it. For me, this is a community of people playing a wider range of games, not just focusing on the super Atari beating game, but they are playing lots of different games. I think it's an interesting community and I've given you lots of reference sin various points in the course to go and explore that a bit more. In summary, we've just been looking at some of the thing sand important milestones I suppose that have happened since DQNand around that time. We've looked at the Atari game playing trajectory leading upto Agent57 in 2020, which is maybe the end of that work. I'm not sure because it's so good and it beats all the professional players. That's pretty good, and then we looked at Star Craft and Alpha Star, the system that can beat Star Craft players, and Star Craft is considered as hard or harder than Go and Dota 2, where they even put the system online and let anyone play itand it was able to beat anyone pretty much consistently. Finally, we looked a little bit at the Mario AI Competition and what happened next with that in the final slide there. In this video, we've just been reviewing some of the things that have happened since the DQN paper was originally published."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Ethics of game playing AI,https://learn.london.ac.uk/mod/page/view.php?id=96496&forceview=1,"[music]In this video, we're going to consider a couple of aspects of the ethics of game playing AI. In summary, we're going to first think about fairness. How fair are the competitions that we have created where humans play AI's at games?Secondly, we're going to think about the impact on players of having these superhuman AI's that play games. What about fairness?Here is an extreme view of fairness. We're going to look at the middle quote there. Just that one there, ""A completely fair competition can only be achieved against an artificial system that is essentially equivalent to a flesh and blood human. ""What they're saying is that if you want a completely fair competition, you need to have a flesh and blood human playinga flesh and blood human, otherwise, it's not going to be fair. That's not that helpful, though. Frankly, it doesn't really tell us how we can make the competition more fair. Luckily, they came up with six dimensions of fairness in this paper, where we can consider how fair is our competition that we've set up. Let's go through each of these in turn. I'm just going to list them off first. We've got perceptual, motoric, historic, knowledge, compute, and common sense. What do they all mean? First of all, perceptual. Do the two competitors use the same input space?What is the input space?If you think about the DQN agent, the input spaceis really the state of the game that we can see. You can see clearly by allowing the AI to only see screenshots of what's going on in the game, as opposed to having detailed information about where all the pieces are, it just has the raw pixel screenshots. That's trying to address that dimension. Is trying to make it as perceptually fair as it can. Obviously, the extreme view would be well, actually, a human being, the light has to come into their eye, and then it has to be processed, and all this kind of stuff happens. It's a little bit unfair, whereas the AI just gets the raw data from the pixels directly. That's what they mean probably by saying there has to bea flesh and blood human. Nevertheless, these are continuums. You can move along from really unfair to more fair. I would say that, say for Atari video game players, for example, only being able to let the AI see the raw pixel data that is moving towards fairness. What about motoric?Do the two competitors use the same output space?The output space is literally the action space in our open AI gyms that we've been using in the Atari games. The action space is one, two, three, or four in the case of Breakout. You can go left, right, or do nothing or move up, I think is the other one that doesn't do anything. You got four options, that's your output space. The question is, is the AI been given any extra things that it can do that the human can't do?You got to wonder how quickly a human is able to actually generate those outputs. How quickly can a human being go left or right?There's an accelerated decision-making process going on there, and then the ability to enact those decisions is instant. The AI can go left. As soon as it decides to go left, it goes left in the next frame, doesn't it?Whereas the human might be thinking about what they had for dinner, or whatever before they go left. That's one element, but certainly, only allowing the AI to generate the control inputs for the game, and not having any deeper access to the game is again, moving along towards fairness. What about historic?Did the two systems spend the same amount of time on training?What we're talking about here is it, for example, the Atari game, it trained played for thousands and thousands and thousands of games. When I'm trying to play against it, if I played, if I'd been given the chance to play for thousands and thousands of games, or, and then even worse with things like Dota 2 and the OpenAI Five, I don't know how long that played for, but it probably--If, for example, it's playing for thousands or hundreds of years, because it can run the simulation really fast on a whole rack of computing hardware, it might be playing for hundreds of years even. I don't know what the difference is. If the AI is able to play for years and years and years of actual real-time playing equivalent, whereas the human being can only play since the game became available to them. They've only had a limited time window and only they got to do other things like maybe going to school or whatever they do. That's clearly unfair. The historic aspect I would say they're not doing that well on that yet. To make it fair, you'd have to say, ""Well, we're going to create a new game, and the human gets to play it for 10 hours, and the AI gets to play it for 10 hours. It has to train on that limited time. ""Then that's how you would make that fair. What about knowledge?Do the two systems have the same access to declarative knowledge about the game?What we're talking about here, as I understand it, is whether the AI has some special knowledge about what's going on in the game. If you think about the Breakout example, are you telling the AI exactly the XY positions of all the bricks, so as you've got this special data coming through, or you're just giving itthe raw pixel data?I would say this relates to perceptual a little bit. Are you just giving it a shared perceptual spaceand it has to figure out what's going on by just looking at the game?Or are you giving it special data about where other people are?Another thing to say about the perceptual knowledge space that's mentioned in terms of these Star Craft and Dota, is, as I understand it when you're playing those games as a human, you can only see a limited window on the game. On the game map, you only see a little bit of the game. The game map might be like that. You can only ever see that much, and you have to move around to see more of it. Whereas the AI in some of those games, I think they give it the whole map. It can instantaneously access any bit of the map, whereas a human would have to move to the bit they want to. That's interesting, isn't it? What about compute?Do the two systems have the same computational power?The question is, do I have the same computation power as this?I don't know. Absolutely no idea how you'd measure that. I'm sure there are papers you can find where people do attempt to do that, but that's clearly a challenge. I would say that probably having a massive room full of graphics cards, and then versus one human is maybe not fair, but I don't know. Then what about common sense?Do the agents have the same knowledge about other things?This is coming back to the more of a paradox thing. It's the idea that AI's might be able to get incredibly goo dat playing Star Craft, but they can't make a cup of tea, or they can't take the dog for a walk or whatever. That's the thing. When we mean by common sense, have you addressed the challenges of general AI in your agent?I would say, again, it's a continuum from not at all to quite well, and things like Agent57, the most recent Deep Mind Atari game-playing agent, it's a general AI and even the DQN is a general AIin terms of is general across Atari game. It doesn't just play one. It is able to learn to play multiple games with the same architecture or the same hyper parameters. That's the idea is are the people working towards general AIor are they just building a highly specific system just to play this one game?Here's a more moderate view of fair. Now we've considered all of that, here's a more moderate view of fairness. Remember, our extreme view is that basically, it's only fair if you've got two human beings playing each other. The moderate view is ""Through the discussion of two areas, the perceptual and motoric abilities, and the game's extrinsic and intrinsic factors, we have to show that, so far, no fair competition between AIs and humans has occurred. ""It's slightly more moderate. This is in the when are we done with games paper. They conclude that they were not done with games, which is good because games are fun. I now want to move on to another area of ethics relating to video games, which is the impact of superhuman AI players on game players. First of all, let's look at this first quote here. ""The lessons that Alpha Go is teaching us are going to influence how go is played for the next 1000 years. ""That is a commentator from Alpha Go the movie. I recommend you watch Alpha Go the movie, it's on YouTube. It's free. That's clearly a positive message to say, Alpha Go, the way it played Go was unique, and it came up with interesting, creative new ways to play it that nobody's used before even though the game's really old. Therefore, we're learning from it. It's teaching us new ways to do things, new creative ways to play Go, which is--That's cool. That's good. That's positive. The other quote is from the BBC News, ""The South Korean said he had decided to retire after realizing: ""I'm not at the top even if I become the number one. ""Lee Se dol decided to retire after this game with AlphaGobecause he couldn't be number one anymore because he realized that AlphaGowas just going to get better and better. There's no way he's going to be able to beat it. For him, for that amazing, beautiful player that everybody across the world in the world of Go just saw as this, the peak of Go, historically amazing playergenerationally amazing and he stopped playing so that is sad. Clearly, that's a negative impact, a very negative impact on someone. He's got to do something else now with his life, so that's interesting. That is like a microcosm of what superhuman AI isand AI's that can do humans jobs cheaper are going to be doing over the next few decades. To conclude, we've just been talking about two aspects of the ethics of AI game players. We talked about fairness and gave you a load of different dimension sin which you might measure how fair a given AI human competition is. Then we talked about the impact of superhuman AI game players on human sand human players, and so as a positive view of that and also a more negative view. In this video, we just been considering ethical aspects of game-playing AI's."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Weekly outro,https://learn.london.ac.uk/mod/page/view.php?id=96498&forceview=1,"[music]Well done. You've made it to the end of week five where we've been thinking about the state of the artin AI game players, and also the ethical aspects of it. What are the learning objectives we've achieved here?First of all, we should be able to present examples of human capacitive AI game players. We've seen a few more of those now. We should be able to reason about the imp actor the ethical elements of human competitive AI gameplayersand how they affect human beings. Finally, we should be able to justify the trend towards general AI and discuss its potential impact on society. Well done for reaching the end of week five, which is the final week in this case study."
DSM100-2022-OCT,Topics 9&10: Game Player,Lecture: Game player case study outro,https://learn.london.ac.uk/mod/page/view.php?id=96501&forceview=1,"[music]Congratulationsyou've reached the end of the AI game player case study. I really hope you've learned a lo tall about how you can build retro video game playing AI, as well as other types of game playing AIs. Okay, so let's just review what we've been looking at. We've had five modules of content. We've had the history of the field, and the nature of the field. We looked at history, all the different games that have been played, and when big milestones were reached. In Week 2 we were looking at formalizing game playing. We looked at how we can express the process of learning how to play a game into equations, so that we can then later turn into code. In Week 3 we looked at the tooling. We are looking at the OpenAI Gym, and we were looking at how convolution al neural network scan be used to process images. Even we had a look at what the inside of neural network, and what it can see. Then in Week 4we looked at the implementation of the DQN agent from Deep Mind. We actually saw how those expression sin the research paper can be turned into code, and mechanically operated to build a real working agent. Then in the final section we were looking at the state of the art, and also considering ethical aspects of game play AIs. Congratulations on reaching the end of this case study. I hope you've enjoyed working through the material. I certainly enjoyed preparing it, and I'm really looking forward to seeing how you react to this material, and to find out what you do with it next. Yes. Well done."
