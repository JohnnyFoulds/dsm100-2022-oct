{
  "course": "DSM100-2022-OCT",
  "topic": "Topics 5&6: Creatures",
  "title": "Lecture: Why do genetic algorithms work?",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96233&forceview=1",
  "transcript": "[MUSIC]In this video, I'm going to be discussing why genetic algorithms workby highlighting some features of a search spaceand explaining how the genetic algorithm deals with those.Let's look at the summary.We're going to be introducing the concept of local and global maxima,and talking about hyperplane sampling,which helps deal with the problem of local and global maxima.Then I'm going to introduce the schema theoremwhich is John Holland's explanation of why the genetic algorithmallows a hyperplane sampling.Then we're going to talk about implicit parallelism,which is another key concept in genetic algorithms and why they work.Finally, we'll talk about computational parallelismas a balanced view because those are two types of parallelism.This is a nice quote from Whitley's genetic algorithm tutorial.At this point, when people are learning about genetic algorithms,\"The question that most people who are new to the field of genetic algorithmsask at this point iswhy such a process should do anything useful?\"Because you've been probably looking at this algorithm thinking,\"Yes, that's kind of interesting, but it seems really complicated.It's got all these weird bits in it like breeding and crossover and mutation,why should any of that work?\"In Holland's book,that's one of the things he explains.I'm going to introduce a couple of conceptswhich help to understand that.First of all, the concept of maxima, global and local maximum.What do we mean?Imagine here, we have a space of 1D space basically,and we are moving through that space.As we search, we're moving through the space.Let's imagine a simple hill-climbing algorithm.A hill-climbing algorithm simply takes a solution.You're using the same encoding, a bit string, a solution,and then it just mutates it, makes multiple mutations of it,and it searches around,then tests them all,and whichever solution gets it to go up slightly on the hill,it will choose that one,so every generation is always going to go up.Not a genetic algorithm, but a simple hill climber.What will happen is, let's say you are hereand you're rising up,you'll find that you only ever are going to go up to that one,you'll never going to find that one over there.You're never going to find the global maximumbecause you're only ever going to climb up your local hill.That is something that genetic algorithms aim to solve.Imagine that your search space is far more complicated.This is a 1D search space,we've just got one thing that we're varying.Imagine if the search space is 100 dimensional,then this space, you can't even conceive of what 100-dimensional space looks like.It's a really complicated space,there's going to be all kinds of hills and troughs,and it's going to be really complicated.If you think about a natural evolution again,I talked about searching spaces in natural evolution.There's a great book about this, Origins of Order by Stuart Kauffmanwhich inspired me to understand this better.Think about the natural worldis like this really complicated abstract search spacethat's changing all the time.You need a really hardcore algorithmto be able to search a really complicated high dimensional spaceto find local or global, at least high peaks.How does a genetic algorithm do this?Let's think about two things that we want to consider here.Hyperplane sampling,which is being able to break a solution into componentsand to test these componentsin multiple combination with other components.Let me jump over to this drawing.Remember this, where we're talking about crossover?This is what we mean, we mean the idea that we can basically treat--First of all, our encoding allows us to break it into components.We actually explicitly say,\"The first few bits are going to be the shape of the wingand the next few bits are going to be the length of the wing.\"Those are distinct components within that.Crossover allows us to, in a fairly, maybe more loose way,have sections of the genome which can get frozen,and then we can test them out with other sections.In this crossover example, we're testing the top bit of the parent,that red bit,with loads of different versions of that from other individuals, for example.It's sort of like saying, \"Okay, well this bit looks pretty goodbut we want to see how it interacts if we change a little bit of it.\"That's what we mean by hyperplane sampling.Then we get to the schema theorem.I'm going to skip implicit parallelism, come back to that,and continue with this feel of the schema theorem.The scheme of theorem, pushing on,is the idea that you can essentially describe a schemawhich is like a regular expression.It's kind of like a regular expression.It's saying, \"Okay, if you've got a bit stringand here, my schema is 1**0.\"Imagine a 4-bit string and your schema is 1**0,that means that any strings that start with a 1,end with a 0 would match that schema.Similarly,we could say that the schema here iswhatever we have in that red section, 10001,whatever it is, that's our numbers.Then we have stars for the green bit.We're saying, we're freezing the red bitand we're trying out different variations of the green bit.That's the way of describing a genotype in a schema type of way.The schema theorem says that if you do thisand you start sampling by sampling through things that match the schema,then you are doing hyperplane samplingbecause you're freezing yourself in a particular plane,so the plane that you're freezing is the bit of the genotypethat's got the numbers in, not the stars, so you're freezing.Here's a plane that goes through state space.Then you're going to search by the bits that have stars,you try variations on the stars.You're cutting a plane through state spacewith your frozen part of your genotype.Then you're searching with the other bit.That's what we mean by hyperplane samplingcombined with schema theorem.That was, I guess, John Holland's contribution to all of this.That is expressed quite neatly and mathematicallyin the various references I've given you on this.Just to clarify that, maybe with a visual.Obviously, I've got colored strips going across here,and each colored strip represents a plane, if you like,that we're freezing,and then we can swap out the other colored strips for different ones.That's the idea is that you can jump around in state spaceby freezing one bit and searching the rest.I want to go back now to implicit parallelism.Let's think about implicit parallelism.This one here, let me just [?].Implicit parallelism is looking at a search at the next level up,we're looking at the population.We're using a population model to maintain,optimize, and even recombine multiple solutions at the same time.The difference between this and the hyperplane sampling is,hyperplane sampling, you're saying, within the genotype,we're freezing bits and allowing some--We can say this bit of the genotype is good,but we're going to try it out with different other bitsto see how it combines.That's hyperplane sampling.Implicit parallelism is going up a level to the populationand saying, we're going to allow multiple bloodlineswithin the populationbecause if you think about how the roulette wheel works,what it does is it makes it possible for multiple individualsto contribute to the next generation,which means if you've got a large population,you can have multiple families going through the generationsbecause they can potentially breed.That's what we mean by implicit parallelism,it's maintaining several parallel solutionsand evaluating them,and even allowing them to recombine with each other at the same time.That's a key feature of their genetic algorithm.Let's just see.That's Holland's idea of parallelism.It's different from the next idea of parallelismwhich is computational parallelism.Computational parallelism is because GAs lend themselvesto being computed in parallel.In other words, you can--We'll see this when we build our own GA.What we mean is on a computer,you don't just have to evaluate each individual in series,you can evaluate them in a multithreaded setup.It's more to do with how you program it.The fact that having a population allows you to evaluate multiple itemson different cores on your CPU or whatever.That's the idea of computational parallelism.That's different from implicit parallelismwhich is more of a subtle thing relating to the idea of different familiesflowing through your genetic algorithms population.That's enough.There's a lot of concepts there but I wanted to try and explainwhy the genetic algorithm has these featuresand why they contribute to its success as a search algorithm.We've introduced a few concepts, so we've had local and global maxima.The idea that the search space has these different positions in itwhich have varying fitness.If you don't have an elaborate enough algorithm,you might end up stuck on a local maximumand will never find that high peak.Hyperplane sampling, we talked about the ideaand how that relates to the schema theoremthat you can freeze one bit of your genotype through crossover,and then through mutation and crossover,then sample through that frozen plane for various other solutions.Then the idea of implicit parallelism,where the population model allows us to maintain multiple bloodlinesor families within the populationso that we don't just optimize one solution,we optimize multiple possible solutions at the same time.Finally, we talked about computation parallelism,which is that when you're programming these things,you can actually use parallel computing techniques,threading, and so on to make them faster.In this video, I've been explaining why the genetic algorithm works."
}