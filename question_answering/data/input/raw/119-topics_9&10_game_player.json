{
  "course": "DSM100-2022-OCT",
  "topic": "Topics 9&10: Game Player",
  "title": "Lecture: Formalising the problem",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96445&forceview=1",
  "transcript": "[music]In this video, we're going to formalize the problem of playing games with an AIinto the construct of states, actions, and rewards.First of all, we're going to find out about states and actions.We're going to find out about rewards.Then we're going to find out what a Markov decision process is.Let's start out with a little reminder.How can we create an AI that can play games?Most games involve an iterative process of observing and acting.Most games involve some positive or negative result.Think of an arcade game where you've got observations,you've got actions, and you've got a result.Remember if we've got the idea of an arcade game,we've got our observation, which is, in the case of breakoutwhere the bricks are, where the ball is,where the bat is, basically, the pixels on the screen.We take actions, which are things like moving left, moving right, or doing nothing.Sometimes we get rewards, which might be a brick disappearing.We then get some points, so +1 point, or maybe the ball goes off the screenand we die, which would be maybe a negative reward, something like that.That's just a quick heads up on what we're talking about.The state is the current view of the game, in breakout, and the actionsare move left, move right, or stay still, as I just said.The reward is one point each time you hit a brick, or game over if you miss the ball.Actually, there's a pretty heavy negative rewardif you miss the ball and it goes off the screen.What we can say is that we can formulate the game in terms of states and actions.The player observes state S, and they take action A,and then they observe the next state; primals, next state.What we say is, the state, so basically that's the observation, that's state.Then the action is the action we take, so that's the action.Then it leads to a subsequent state which we, basically the next screenshot,which is maybe the ball is further down or whatever, so that's the next state.That's what we mean, so state, action, and next state.Sometimes it's a probabilistic thing.We can say, there's a probabilistic relationship between one state and the next.You can take the same action in the same state but it would lead to two different neck states.What do we mean by that?How does that work?Well, let's say we've got a view of the screen.Let's just simplify it now, the ball is there.It's basically going that way, which we don't know by just looking at the screen.We don't know which direction it's going in just by looking at the screen.The next state on that one would be that the ball is further that way.Let's say the ball happened to be going that way and in that case,then the next state would be the ball being further that way.That's what we mean.We could say based on our observation there'sa 50/50 chance because it's equally likely to be going either way.We can say probability is 0.5 there and 0.5 there, that and that is prime, that is prime.That's what we mean by it being probabilistic.We express that formally by saying probability of state prime given state and action.That's what we call effectively a state transition matrix.Okay, what about rewards?What we can say is, when we've got our state, action,and next state construct, there's a reward associated with that,which we give the letter R, and state which says takingaction A in state S leads to the following state and a reward of R.Sometimes we formalize that as a reward function, so we say there's areward function that operates over the state transition matrix, if you like.What we mean there is, let's say just for the sake of argument,I'm just going to say the bat is there.If I've missed the ball, basically, the ball's heading off that way,then I might get a reward of -1 because I've basically lost the game.If I happen to hit a brick, then maybe I get a reward of +1.Then if I don't do anything, which is what happens in most-most frames nothing's going to happen, most times the reward is going to be 0.If you don't achieve any points in a given frame, maybe you have anegative reward for that as well, so that could punish inaction, if you like.If the thing isn't gaining points all the time, periodically,then it's going to gradually lose its reward, something like that.There's different ways to model the reward, but essentially it's either areward function or it's just an R that are part of the observation, if you like.That means what we have, with those parts, with the probabilistic state transition matrix,and this reward function, that's a Markov decision process, which is a well-known process in AI really.What we say is that a Markov decision process is a way offormalizing a stochastic sequential decision problem.Let's go through those words and make sure they're really clear.Formalizing is basically expressing something in a clear mathematical way,if you like, that can then be used to build algorithms and so on.A stochastic sequential decision problem; stochastic means that it has aprobabilistic element, and that's where theprobabilistic state transition matrix comes in.It's stochastic because there's different-- based on what thecurrent state is, it might go to other states probabilistically.Then it's sequential, and that's really important.It's sequential because we're going to have to make a whole series of decisions.Because we see a sequence of frames of what'sgoing on in the game and we have to makea decision, each frame, as to whether we're going to move left, right, or stay still.It's sequential because we have to make a sequence of decisions.Then sequential decision problem, obviously, is a problem of making decisions.That's what we mean by stochastic sequential decision problem.A Markov decision process is another name for aformalization of a stochastic sequential decision problem.Just in case this helps you visualize that, the idea of a state transitionmatrix where we're moving around and we're gaining reward.You can see, sometimes a little reward pops up,and sometimes we just transition between states.That's what we mean by Markov decision process.The decision and this is what we're going to cover in thenext video, the decision is, which action are we going to take?That's the decision we have to make, is what are we going to do.We've just been going through the basic elements of the Markov decision process,which is the states, actions, and the rewards.We found out that it's essentially a stochastic sequentialdecision-making problem and that's what we're dealing with.Now, in the next video, we're going to be looking at the action policy and Q-learning.In this video, we just introduced the concept of the Markov decision process."
}