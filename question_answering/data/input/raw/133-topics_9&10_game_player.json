{
  "course": "DSM100-2022-OCT",
  "topic": "Topics 9&10: Game Player",
  "title": "Lecture: DQN loss function and training",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96482&forceview=1",
  "transcript": "[music]In this video, we're going to be talking about the DQN loss functionand digging into the code to see exactlyhow that is used to train the neural network.Summary.First, we'll review the loss function briefly justto remind ourselves what to expect in the codeand then we're going to see how we select items for training,how we compute the loss, and then how we compute the gradientand apply that back into the network in order to train it.Quite a lot to cover but let's dig in.There's the citation just to note, the codewe're going to look at is from Chapman and Lechner and the loss function.I'm going to jump straight over to my drawingprogram here just to show you the loss.Do you remember this from a couple of weeks ago, the loss function?This is derived somewhat from the Bellman equationsand the idea is that what we're saying is,what is wrong with this network is the differencebetween what the network predicts we should do,and what our best guess predicts we should do.Normally, when you're training neural networks, you have a ground truth.For example, if you're training it to recognize images,you have a load of labeled images of cats,and then you can tell whether it's got it correct or not.The problem is, we don't have that data.In this example, we don't know exactly how much each action is worth.What we have to do is you have to make our best guessabout what we think the action is probably worth,and then try and get the network to approximate that.How do we get the best guess?Well, let's break down this equation.Just remind ourselves.We've already done this, but I'll do it again.First of all, when I said the best guess, that bit is our best guess.This is the training data.That is what we do.That's a sample of items from the replay buffer.We got some states actions, rewards and next states from the training buffer.That is a uniform sample of random examples from the training buffer,and the point is, in that sample, we know what the reward is going to be.Part of our best guess, this is our best guess,what we might call the ground truth,or as close to a ground truth as we're going to get.Part of that is actually the reward, the known reward.That is ground truth.We know what the reward is but the thing is,this thing needs to know what's going to happen in the future.It's not just the immediate reward that we'll get from doing this action,it needs to be thinking a few moves ahead,to gather what is the best action to lead to a future reward.That's the trick.We can't just look at the reward,we have to also look at some reward going into the future.How do we know what the reward going into the future is?Well, we use the Q-Network, which remember is this old version of the network,which we're updating every 10.000 frames, or whatever it was.That is the previous version of the network.What we do is we got this gamma thing,which is a discount factor applied to that.We're saying, what is the known reward?That's the real reward we'll definitely get in the next step,or at least in the past observations,and that is the discounted guess at what the reward will be iteratinga few frames into the future to what we call the horizon.That's our best guess of what is the right output for the network.The network should output something like that.What we do is we say, well, that's our best guess because that's Qand that's using Q-, remember, which is the older version of the network,and then this one is using, sorry, it's theta dash.Then this is the current version of the network.There's no dash thereand it's saying compare what our best guess is to the whateverthe current version of the network is calculating.Then it's a mean square squared error deal.Although we're not actually going to use that in this implementation,we use the Huber error.The way that we calculate the error between those two,that's the error function if you like.That is the loss function.The loss function is basically telling us of the latestversion in the network comes out with this valueand what is our best guess of what the valueshould have been the ground truth,we compare the two and that gives us the loss.That's what I wanted to say about that.We've kind of already covered that.I just wanted to remind you because it was a little while ago.What does the code look like for that loss function?Let's go through.First of all, we want to know, let's look at the code to get this bit.That's the first bit we're going to find in the code.How do we sample from the replay buffer?Well, let's go in and find the sampling bit.There's the code, here's the training bit.That is it there.What do we do?We basically take, this is essentially saying,if the whole replay buffer were in fact,remember the replay buffer is multiple lists, but we can just choose one of them.We take the whole list, and we take a random set of samples,the size is batch size, which is I think, 32.We basically choose 32 items from the replay buffer,and then we basically sample.That's the state bit.That's the S bit, that's the S dash bit.That's the next state.That is the R bit, and then that is the action.That's our random sample.Those are the bits, which we see here.That's all of that, randomly sampled from the available replay buffer.Good.We've got that bit and then the next step is how to... that's selecting the 32.The next bit is then basically calculatingwhat the network thinks the reward is.That's calculating this bit.We've got to calculate that bitand we've got to calculate... well, we know what that bit is,because that comes with the dataset, because that's the R there.We're going to calculate that and we've got to calculate that.Let's find that in the code.Let's see what we got.First of all, we have the... yes, there we go.That bit see if you can guess what that is, future rewards.Model target is your Q-Network, right?That is your Q-Network.What's that?The future reward is exactly this.That's calculating that bit.We literally just pass in the next stateand we use the network to predict.Model target is the neural network which has the old version of the weightsand we could predict on that, which essentially feedsin the sample of next states and it gives us back the rewards,which remember are going to be for each of those states that we're passing in,it's going to give us the predictive rewardthat that network thinks we're going to get.That is that bitand then we need to discount it.There we go.We got the gamma's in there.That's the gamma.It's all there.There's the gamma, gamma, gamma, gamma.That's there and then what else have we got?That's all that and then we basically do one hard encoding.We choose the... I think we're basically selecting the actionand then we've got the bit herewhere we actually calculate the Q-values using the model.That line there is doing it on the model.That is... sorry to keep jumping around,but I want you to be really clearon which bit of the equation we're looking at.This is the bit where we calculatewhat our very latest network is generating.That's what we think we're training on.That is there.You can see we're passing in models.We get the Q-values, and basically filter them a bit,process them, and eventually, we call loss function.Oh, did we add the reward?Where do we add the reward?Just going to see where we added the reward?Oh, yes, there it is, it is there.To calculate the ground truth, we did add the rewardand we then do the gamma scaled version of the original network,and then we recalculate the difference.That is where we do the whole thing.That line there is doing this.That's where we do this subtraction and it's calling.It's passing then to loss functions.What is loss function?Where is loss function?See if we can find that.I'm going to search for it, loss function.There we go.Loss function is just one of the standard losses available.In fact, they're using the Huber loss,which is a way of calculating the distance between two sets.Oops, I delete it.There is and that's it, and then that gives us basically the loss.Then because we're using a custom loss function,this custom way of calculating the ground truth versus the predicted truth,we then use this tape gradient technique to essentiallyworkout what the gradients are in that loss,which are basically the distance between what the network predictedand what we were hoping it would predict if you like,the direction of those distances.That's your calculus bit.It's like calculating the gradientsbetween where we are and where we want to be.Then that line there,essentially just backpropagates those gradients back into the network.That's where the network weights are actually updated.That's more or less it.I'm going to stop there.I just wanted to show you exactly the mapping between the codeand the loss function.You can see there's a direct mapping.Every bit of that equation is implemented in the code,there's a line of code that does it.I missed out a little bit of detail there but you can goand read further exactly what all those Tensor.Processing functions are doing but that's essentially it.In summary, we've just been looking at the loss function.We found out how it selects items for trainingand then we've made a mapping between the original lossfunction equation from the research paper into the codeand seen how we compute the loss by having the known ground truth,which we extract from that Q-Network versus the current network.Then we compute the gradient and roll that,backpropagate that back into the network to updatethose 1.6 million weights in the network,accordingly so that it'll get a better prediction next time.In this video,we've just been deep-diving into the loss function code in the DQN implementationto find out exactly how every bit of that equation is implemented in code."
}