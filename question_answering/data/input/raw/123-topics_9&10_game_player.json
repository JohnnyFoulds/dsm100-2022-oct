{
  "course": "DSM100-2022-OCT",
  "topic": "Topics 9&10: Game Player",
  "title": "Lecture: Visual processing and states in DQN",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96455&forceview=1",
  "transcript": "-In this video, we're going to be talking about  some of the little changes that are made,well, quite big changes actually,  to the data that comes outof the Atari game emulator and into the DQN agent.We will think about some of the details  of what you have to do when you're building a real AI that can really do something  in the real world as it were, or at least the real world of video game playing.First of all, we're going to talk about  some secret sauce they addto the state to make the state  a little bit more informative to the network.Then we're going to be talking  about how they deal with a certain issueor technical limitation with the Atari platform,  which meant that the programmers use little hacks, which cause flickering,  so we're going to talk about that.Then we'll talk about the final processes,  which is really just data reduction, and we'll see that that's clearly  a sufficient amount of datato know what's going on, in this game at least.First of all, my observation.There's no sense of time.If we look at these two screenshots,  you can see on the left one the ball is there and the right one obviously  you can tell which direction the ball is moving in because you've got that sense of time.If the neural network is only being passed  a single frame as its state,if it only ever knows, \"This is the state now,\" and it has to make a decision  as to what the value of that state is,value of the different actions are for that state, then how does it do that if it can't even tell  which direction anything's moving in?Especially with these arcade games,  it's all about things moving.Somehow the network needs to know  which direction things are moving in.Otherwise, how's it going to know  which direction you should move in?That's really one of the problems  that came into my mind as I was reading the original version of the paper  before I got right into the details.Now, typically, if you're modeling sequential data  over time, for example,audio or video, or text even,  because text is sequential data, typically people use some sort  of recurrent system like an LSTM network, you feed it the sequence  and it has some memory so it knowswhat the previous sequence was,  it remembers that over time so it can build upthis knowledge over time.The network that gets used in DQN  is not recurrent, it doesn't haveany of that sense of retaining the pattern  over time that's been fed in.How do they do it?Well, the answer is,  the state that gets fed in is, in fact,three frames so it looks at the last three frames.It's not just a single frame,  so by \"state\" in DQN we mean actually threeframes of what's going on in the game.You can see an example here.We've got three frames  where you can see the ball moving from the left over towards the bat there.That means that it has a sense of time  because the state that you're giving it actually includes information  about three frames.I won't call it cheating because it's not cheating.It works.Nothing's cheating here.It's just that there is a way that they feed  some sense of time into the neural network.You might think of it as being a state  which is second order or something because it retains  two previous states in a certain sense.That's the first thing of the secret  sauce of the three frames of state.The frames thing gets more complicated still.Have a look at this quote from the paper.\"We take the maximum value  for each pixel color valueover the frame being encoded  and the previous frame.\" It's looking at two frames.Forget the three frames thing.Let's imagine we're just trying to get  one of those three frames.How are we going to get it?Well, actually, one of those three frames  that gets fed in as the stateis compounded out of two frames.They take the maximum value.Why was this necessary?Because they needed to remove  flickering that is present in games where some objects appear only in even frames  while other objects appearonly in odd frames and artifact caused  by limited number of sprites Atari 2600can display at once.Let me just draw a picture to explain that.What we're saying is, we've got two frames.The problem with the Atari 2600,  the original hardware, is it could only display  a limited number of sprites.Sprites are the graphics on the screen.A brick would be a sprite probably  or the player's bat would be a sprite or the ball would be a spriteIf you want to draw loads of sprites  on the screen, like in Space Invaders,there's loads of sprites on the screen,  what you'd have to do is go draw whatever you could on one frame  and then the second frame you'd draw the other set of sprites  and then on the subsequent frame you'd have to draw back your original set  of sprites again, that may have moved.What that means is it's flickering.The sprites are flickering on and off  in between frames but as a human player,you don't really notice this.Actually, Ataris used to be a bit flickery.You notice it but it works for you  because your persistence of visionthat you have in your visual apparatus  basically means you make upfor it with your brain so you can deal with it.If an AI is looking at this it just says,\"Oh, it flicked it off,\"What they do is they essentially  combine these two frames into one by taking the maximum value of a pixel.We're assuming that it has a black background  and if it has a black background when the sprite disappears it goes black.That means that you always choose the data  that is when the sprite's appearingso if you stick those two together  you'd end up with that.You'd see all of the sprites.Of those three frames  that we're feeding in as the state each of those is actually made out of two frames  where we've taken the max valueto deal with this flickering problem.That's the next fiddly thing that they do.This is a bit more straightforward.Basically, this is where we get  into the idea of data reduction.The next step is instead of having RGB values,they convert those into a luminance value  which is a bit like grayscale really.The luminance value gives you  a single number per pixelinstead of having three numbers per pixel.That just means we've got less data  to feed into the neural networkbut you can see from that screenshot, clearly  you can still see exactly what's going on in the game.The next data reduction step,  the final one, is to take the number of pixels down from 216 wide down to 84.It basically rescales the image  and there's the code that does that in the [?] example we'll be looking at.We'll see that again later,  don't worry about understanding that.Essentially, you can see from this screenshot up  here that it's still really clear what's going on.You can see where the ball is,  you can see where the bat is, it does look a bit blurry or whatever,  a bit pixelated, but that's fine,because as long as you can see  what's going on it doesn't really matter whether it's pixelated or notAgain, you're throwing away loads  of unnecessary data, again there.Grayscaling and resizing throws away  a load of data but, of course,the other steps are adding more data.We're having effectively six frames  fed in as the input although we're combining two of those into three.That's it.In summary, we've just been looking at some  of the interesting processing stepsthat they had to do in order to, firstly,  retain some sense of what happenedin the past and secondly,  to solve the problem of flickeringwhich is a of an artifact of the necessarily  hacky programming they had to do to switch the sprites on and off because of the limitations  of the hardware in the original system.Then we talked  about some of the data reduction steps,the grayscale, and the resize step.In this video we've just been getting  a heads up on how the raw datafrom the Atari emulator is processed  to be ready for training the neural network."
}