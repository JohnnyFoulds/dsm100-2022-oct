{
  "course": "DSM100-2022-OCT",
  "topic": "Topic 1: Introduction",
  "title": "Lecture 6: Goal-based, model-based, utility-based, learning agents",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96092&forceview=1",
  "transcript": "[MUSIC]In the previous mini-lecture,we talked about simple reflex agents and their limitations.In this mini-lecture, we will consider other types of agents,model-based,goal-based,utility-based,and learning agents.Model-based reflex agents store knowledge about their environmentcalled the knowledge model or a domain modelor just a model.The more an agent knows about the world it is in,the more intelligent behavior it can exhibit.For example,if the vacuum cleaner has a model of the floorwhere obstacles areand also where dirt usually is,then it can make better decisions.A model-based vacuum cleaner can operate by different rules.If it does know where dirt is,then it can find a path to get there and to avoid obstacles.This slide shows a simplified program in the pseudocodefor a model-based agent.The model may include a path between each pair of locations.There are algorithms for finding such a path.Having a model of the worldmay not be enough to decide what to do.For example,for a self-driving car agent,it is not enough to have a map of roadsand rules of movements.It still needs to decide if to turn left or right,and it is hard to decide without knowing the destination.In other words, the car agent needs to know a goal.Agents that make decisions based on a given goalare called goal-based agents.Knowing goals and modelsmakes agents even more intelligent.Sometimes, goals can be achieved by a couple of actions,but sometimes, it may require complicatedand not obvious sequences of actions.Such agents need to search through the space of possible actionsand also to plan their actions.Search and planning are subfields of AIdevoted to finding solutions to achieve agents' goals.Utility-based agents.Knowing a goal is good,but it still may not be enough to generate high-quality behavior.In complex environments,the same goal can be achieved by different ways.For example, a self-driving car agent can reach a destinationby following the best route, fastest route,or the most ecology-friendly one.Previously, we discussed the performance measuresfor rational agents.Such a measure can be implemented in the agent programas a utility function.A rational utility-based agentchooses actions that maximize the expected utilityof the action outcomes.Learning agents.Modeling the world and writing productionor situation action rulesis a time-consuming task.Can an agent learn it?The answer is yes.There are different architectures for learning.We will consider an architecture that includes an Oracle or a critic,someone or something outside the agentthat can provide a feedback on agent's performance.A learning agent has to explorepossible goals, states, actions, and their outcomes.It is usually done using a problem generator.Let us continue with the vacuum cleaner agent example.Imagine a cleaner that knows nothing about the complex world around.The world is a floor that consists of many squaresand some have dirt and also some large objects, obstacles.The agent has learning capabilities,having an Oracle, a problem generator, and performance measures.It also knows what it can do,move left, right, straight, back, and suck in dirt.It has sensors to detect in what location it is inand if there is dirt in that location.For the exploration stage,the problem generatorwill randomly generate 100 three-action sequencesor some other sequences.The goal will be to execute them and evaluate the agent's performance.The performance measure remains the same,clean the floor,but for the learning stage, it can be fragmented.The Oracle can assign three points for each clean square,one point for a move to adjust to dirt square,and it can penalize the agentif it was to move back after hitting an obstacle.The cleaner may start from selecting a three-action sequence randomly,executing it,and learning from the experience.It will learn sequences leading to a reward.For example,move from location A1 straight,then again straight, then suck in.Also, it will learn sequences leading to a penalty.For example,move from location A1 right,then straight,but it can't,so it has to go back.After executing 100 three-action sequences,the cleaner agent is likely to learn the whole map of the floor,where the dirt is and where the obstacles are.It will construct the model of the worldor maybe it will construct the model only partiallyand then more and maybe longer sequences of actions will be generated.The agent will learn more and morewith executing new sequences of actions.It will learn about the world and also what actions lead to a reward.Its behavior will become more and more rationaland intelligent."
}