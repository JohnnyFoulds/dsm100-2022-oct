{
  "course": "DSM100-2022-OCT",
  "topic": "Topics 9&10: Game Player",
  "title": "Lecture: State of the art game players",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96494&forceview=1",
  "transcript": "[music]In this video, I'm going to be reviewing some of the things that happenedaround and after the DQN agent paper that was published in 2015.In summary, we're going to talk about progress that happenedwith Atari video game playing after DQN, and then we're going to lookat some other games.We'll look at StarCraft, which has been focused on by DeepMind,and then Dota 2, which is OpenAI, a similar-looking company.They worked on Dota 2.Then we'll revisit the Mario AI Competitionand see what's happened there.This is a list of papers, and I'm going to give you these referencesin a reading, but these are things which happened after DQN.Number one, they invented a thing called Double DQN,and that led to the rainbow system.What I want you to do with these referencesis take what you've learned about DQN and think about the key elementsof the system. There's the general runtime of the agent.What does the agent do?That's one thing.Then thinking about, how does it gather experiences?How does it build that state transition matrix?How does it gain an understanding of how the environment operates?In DQN, obviously, it's the replay buffer, but then these systems,they come up with new more and interesting ways of doing it.Beyond Epsilon-Greedy, what's the next exploratory behaviorthat's required to gain better performance?The other thing to look at is, obviously, the neural network architecture.What kind of neural network architectures do these different systems useand how did they advance that?Also, what was the loss function, very importantly?How do they compute the ground truth when you don't have ground truth?We saw that DQN did it in a very interesting wayby having an older version of the network and combining thatwith the immediate reward plus some prediction of what the rewardwill be in the future and then using that to train the networkas ground truth.Those are the things to look at when you go through theseand following on from double DQN, we had Dreamer version two,and just to say that that research paper has a really nice set of references in,so that's a good reason for going and digging that one out.A lot of this stuff is open access, on archive, and things like that.Then you got R2D2, which is another Atari game-playing bot.Finally, number four, 2020 DeepMindpretty much put an end to all of thisby building an agent architecture and publishing, which just is betterthan most professional game players are, all of these,the benchmark set of 57 Atari Games.That's why it's called Agent57.Even the games where it takes an extremely long time to gain a reward.There's zero reward for thousands of turns.You have to explore for ages before you get anything.It's able to play even games that are really complicated gameswith lots of complicated visuals and things like that.That's almost the end of the work on Atari Games, as I would say,because it's clearly out competing human players,and it's a general system that plays all of the gameswith the same setup.Probably you have to retrain it for each game like the other onesbut nevertheless, that is where it's at now.That's pretty much you can consider playing Atari video games betterthan human beings as they solve problem as of 2020 on Agent57.What other games are there? Well, StarCraft is another game.I'm going to drop down to the second quote there.This is the second one.The original game and its sequel, StarCraft 2, have several propertiesthat make it considerably more challenging than even Go.That's really interesting.Remember that AlphaGo was the Go-playing system.Now AlphaStar further worked towards general AI system.The idea is that AlphaStar is a very general-purpose systemthat can learn to play all kinds of different games.In 2019, it was able to play Starcraftto the standard of professional players and beyond.It was able to beat professional players at what is considered to bean extremely hard video game.The bottom quote, this one's really interesting.AlphaStar draws on many areas of AI research,including deep learning, reinforcement learning, game theory,and evolutionary computation.Just go and read the paper, read about it, evolutionary computationwas in there as well to show that that is absolutelya state-of-the-art technique that's being usedto evolve neural networks and training systems and so on.That's StarCraft, what about Dota?StarCraft, that's DeepMind versus OpenAI if you like.Dota is another video game which is considered very difficultand involves multiple humans playing in teams against each other.In 2019, it won back-to-back games versus professionalworld champion players OG as a team.I guess that means original gangsters or something.It's able to be a really excellent professional teamand was the first AI to do that.Then further, beyond that, in 2019, they put the system onlineso people could log on, and with certain constraints,they could play against the AI in real-time in competitive games.It won 7,215 games and only lost 42.That's pretty impressive and that is addressingsome of the problems.Remember, the research paper we're looking at that was calledWhen Are We Finished Playing Games, and in that paper,they discussed the unfairness of many of these competitionsbetween AIs and human game players.Clearly putting it online is addressing one of those things,you're letting anyone play. It's very uncontrolled environment.Although they still didn't address all of the problems.For example, it was quite a constrained versionof the game, you can only play with certain team membersand things like that.Also, of course, the AI didn't have to operate a robot handto press the buttons on the controller.There's all that complexity, which humans are doing as well,which the AI didn't have to do but still very impressive.What about the Mario AI Competition?I'm putting this in here because Mario AI Competitionpredates this. There's a lot of this Atari stuff.Remember, the original arcade learning environmentwas 2013 or around that.Before that, there was the Mario AI competition,that's when these game-playing bots became popular maybe.Basically, what happened is,I think they got some heat from Nintendofor publishing pictures of Mario Games all the time.I think they mentioned that in the paper, but also, they found their othermore general-purpose competitions, like the platform AI competition.They also note, they mentioned the Ms. Pac-Man Competition.There's a few other competitions grew out of it and were inspired by it.For me, this is a community of people playing a wider range of games,not just focusing on the super Atari beating game,but they are playing lots of different games.I think it's an interesting community and I've given you lots of referencesin various points in the course to go and explore that a bit more.In summary, we've just been looking at some of the thingsand important milestones I suppose that have happened since DQNand around that time.We've looked at the Atari game playing trajectory leading upto Agent57 in 2020, which is maybe the end of that work.I'm not sure because it's so good and it beats all the professional players.That's pretty good, and then we looked at StarCraft and AlphaStar,the system that can beat StarCraft players,and StarCraft is considered as hard or harder than Go and Dota 2,where they even put the system online and let anyone play itand it was able to beat anyone pretty much consistently.Finally, we looked a little bit at the Mario AI Competitionand what happened next with that in the final slide there.In this video, we've just been reviewing some of the things that have happenedsince the DQN paper was originally published."
}