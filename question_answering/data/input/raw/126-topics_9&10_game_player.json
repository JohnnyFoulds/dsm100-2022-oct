{
  "course": "DSM100-2022-OCT",
  "topic": "Topics 9&10: Game Player",
  "title": "Lecture:  What is openai gym?",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96463&forceview=1",
  "transcript": "[music][music]-In this video, we're going to be talking about OpenAI Gymand having a quick demo of how we can use it.In summary, we're going to first of all ask the question, \"What is a gym?\"Then, we're going to be looking at OpenAI Gym in particular,finding out about all the different environmentsthat are available or some of them anyway.Then, learning how we can create an environment.How we can act in the environment and Then, access all of those thingswhich we know about from looking at the Markov decision process wayof modeling reinforcement learning.First of all, what is a gym?I've tried to come up with three points here.Number one is, it's a way of providing standardized environmentsfor reinforcement learning algorithms to operate in.It's a standardized APIthat allows you to plug your reinforcement learning algorithm inand it will have a standard way of doing thingswhich we'll see in a minute.Number two, it allows the developer to focus on their agentinstead of the simulation.We saw that brought up in an earlier discussionwhere we're talking about I think the Mario AI competition,and the idea that as a developer who wants to work on AI algorithms,you might not want to spend all your time building the best simulation of physicsor whatever it is.This gives you a bunch of ready-made simulations or environments.You can just plug your algorithm inand work on that rather than working on the simulation.The third point is that it includes versioned standard set of environmentsenabling easy comparison.The fine point isthat because everybody is using the same simulation, if you like,it means that it's very easy to compare algorithms.If you have a simulation of a video gameand your algorithm gets 27 points,someone else's algorithm gets 30 points consistently,you can see that it looks like your performance is going is comparable,first of all, and you can prove that it's better.There are various gym-like systems out there.The OpenAI Gym is not the first or the last, probably.I've just picked out a few here.You've got the Arcade Learning Environment from 2013,which predates the Deepmind DQN paperand I think that's what they used.Then, you've got Vizdoom, another example from 2016.I've got all the citations up here, but don't worry about those.I'm going to give you those as direct links in the reading.Then, Vizdoom allows you to basically design agentsto play the Doom first person shooter game.Then, you've got OpenAI Gym,which is the one we're going to be looking at.So, what is OpenAI Gym?Well, this is what it aims to do, anyway.It aims to combine the best elements of these previous benchmark collectionsin a software package that is maximally convenient and accessible.The design goals areto have as many of these other benchmarks in there as possible,but to make it easily accessible.Indeed, you'll see from the Python interfacewe're going to be looking at that it is pretty easy to access.These gyms, and specifically OpenAI Gymcomes with a whole bunch of different environments.We've got three hereand I'm just going to show you the webpage,because it's probably easier to see them there.This is the OpenAI webpage for gym.You can see over that sidethat we've got the various different categories of environment.There's the algorithms one, the Atari one,which I can't show you because of copyright concerns.Box2D which is a 2D physics environment.Let's look at a few of those.You can see that the first couple are these Walker ones.You can see there's this two-legged ED-209 if you know Robocop,it's like an ED-209 type creature.No machine guns, though.That's walking across an environmentand there's two different versions of that.The one on the right has a more challenging environmentas I understand it.Then, there's a driving game.You can see the green one there in the top left is car racing,so you're driving this car.Then, we've got lunar lander up there as well.Those are some environments that come with OpenAI Gym.Then, what else have we got?These are classic reinforcement earning problems.So, the CartPole is the absolute classic.I can actually remember my tutor lecturerwhen I did my master's at Susex,Adrian Thompson, who is one of the inventorsof evolvable hardware actually.He was demonstrating CartPoleby running it backwards and forwards with the pointer for his blackboard,a big tall stick.He ran backwards and forwards across the lecture hall to illustratehow difficult it was to balance the CartPole.Of course, reinforcement learning algorithms are very goodat learning how to do that.Anyway, that's that one.What else we got? Then, we can jump into these ones here.MuJoCo is a 3D physics engine, a really precise physics engine.You can see they've got various ready made robotswhich you Then, have to plug your algorithm intoto try and teach them to walk.Then, there's various different robots.There's a humanoid one there and various other bits and bobs.There's a snakey one there.So, that's that. Then, interestingly,at the time of filmingit's just happened that Deepmind have actually just bought MuJoCo.Mujoco was previously a commercial product,a commercial physics enginewhich has just been bought by Deepmind in 2021.Then, they're going to open source it.That means that these fancy physics enginesare going to become available.I don't know how it compares to Bullet,which I've used in one of my other case studies,but apparently it's pretty good.Then, we've got a whole set of robotics 3D environments as well.There's all kinds of really cool environments available hereto train algorithms in.That just gives you a little insight.Back to my slide here, you can seein the middle I've got this break wall one.We had to create our own environmentso that we could show you something learning how to play a gamewithout having to show you the Atari games.What we did is we were able to fairly easily create our environmentby creating a game using the Pygame library,which is a very common Python gaming environment,it lets you build a game engine.So, it's a game engine that lets you build games very easily.We built this game and then, we were able to wrap it upin the OpenAI Gym systemsuch that it could just be a new gym.So, you can create your own environments pretty easily,because I did it [chuckles]. I can tell you it's not that difficult.Moving on. How do we create an environment?Well, first of all we need to know which environments are available.What I've got here is my Python environment.I can import gymand then, these commands here allow me to just show youhow many different environments are currently installed in my system.I think I've installed the Atari ones on here as well.So, we do gym.envs.registry.all.I've now stored all of the names of all the environments into x.Let's just do length of x.You can see I've got 859 available environments.Now, they're not all totally different.They might be different versions of the same environment.Let's just print them out.For i in x print i,let's see what happens.It's now printed out all those games,so i know that certainly Zaxxon, for example, is a Atari games.Obviously, got lots of Atari games on there.Wizards, pinball, tennis, Star Gunner, Space Invaders,all the favorites there.No Robotron. Robotank, though.You've got all these ready-made environments on there.Also, as I say, you can easily write your own.That's how you get access to all the different environments.Let's say I want to create an environment.Well, here's the workflow for doing that.Here, what I'm doing is I'm creating env2. gym.makeis the way of creating the environment,and I need to give it a name.Let's follow that along.I'll do env.Let's call it Lunar Lander env to be clear equals--Actually, let me just do this.I'm going to cheat and go--Here's one I made earlier.I'll call it Lunar Lander environment.That's the command.Run that.Then, the first thing you'd normally do with a new environment is reset it.I do ll_env.reset.It passes me back some stuff therewhich we're going to talk about in a minute.Then, the next thing you probably want to do is actually render it,because that lets you see the current environment, what it's doing.I'm going to jump into that, because that's quite small.That's pretty much it.You can see at the top therethat's there's some Apollo Lunar Lander capsule coming down.You can see the legsand that's going to be trying to get down to that place there.So, this is the task here, in this particular environment.Then, what I would do next is I'd want to step.I'm going to do ll_env.step.We wouldn't normally call render when we're trainingbecause we don't need to see it.More about that in a minute.Then, I'm going to pass it just some valuesand I'll explain what they are.I just passed in a zero zero,and nowand if I keep rendering it again.Sorry, ll_env.render.Then, I step.That's basically it. You can step through.It's not really moving much because the frame rate's quite low.Let's say I do a quick quick iteration on this.For i in range 100,and I'm going to do-- sorry, ll_env.step,and I'm going to tell it what to do and ll_env.render.Now, look carefully at the thing up there.Let's just pull that over there.Okay, hopefully, you saw that.It basically plopped down.Actually, I managed to land it, which is pretty remarkable.Yes, so that's an exampleof the environment iterating and rendering.I don't know if you saw that plopping down.You can always rewind if you didn't.That's the basics of operating an environment.The question is, what was that zero zero I was passing in?The answer is that as the action.Okay, so if we go, we get back into memory,the Markov decision process,where we basically tell the agent to--we take actionsand as a result, the state of the environment we receive,the next state,okay, so we know the current state.We act, and we get back the new state.We also get back other thingsand we'll talk about the other things in a minute.Let's talk about acting first.The available actions are called the action space.If I do ll_env.action_space, okay.Then, that tells me it's box two, which is a bit mysterious.What I can do is I can call sample(),and that will basically generate a random action of all the available ones.You can see that it's a 32-bit float with two values,it's basically a two-dimensional array there.Each time I sample I'm getting a different random action.I'm not taking this action yet, I'm just sampling the action.Now, if I create another environment.So, if I do this,and I'm going to call it breakwall environment.This time, I'm going to create something called breakwall.This is the one we've created.Again, if I do bw.env.action_space--sorry, bw._env.action_space.sample()and I should get examples.This one has got a much simpler action, it's just an integer.I could just see what it is.It says Discrete 4, that's basically four possible values.You can see that different environments have different action spaces. Okay.Let's say we want to build our simplest possible thing to playto operate in this environment.Then, what I can do is I can sample a random actionand then, pass that into step.Okay, so, let's try that.I'm going to do for i in range(1000).I'm going to do it with the breakwall environment here.I'll doa = bw_env.action_space.sample().Okay, so that's my random action,Then, I'm going to take my random action.I'll do bw_env.step(a) that I've just generated at random.I'm going to do bw_env.render(), just so we can see what's going on.I'll just make sure it's on the screenbecause otherwise, we might not be able to see,you haven't rendered it yet.Okay, right. Are you ready?Let's get out of the way.You can see the bat twitching around at random,because it does not do anything sensible,well, it's a bug in the game that allowed itto somehow magically not die.Okay, so that was the 1000 random steps taken in breakwall.What about lunar lander?Well, I'm going to reset it first,because I noticed that the lunar lander has actually landed.If I do the same thing, but do ll, here.I'll take a random action, the lunar lander 1000 times.Where's my lunar lander environment? There it is.Okay.I think it's crashed already.It's obviously not--It went a bit fast, didn't it?Let's quit that one, let's try again.If I just reset it first.Okay, Then, let's try again.Okay, it just randomly crashed.Okay. That gives you the idea of basically what you can do,how you can select random actions.We've done that.Now, the next thing to note is that when we step,it returns some stuff and this is not surprising.You can see here, it's really the MDP, right?They call it the observation, but we can call it the state.The O, so it's o, r, d, i equals ll_env.step.Okay. It's giving us--we can actually find out what this is.If we do ?ll_env.step like that, that'll give us a description.This is it, it says,\"Run one timestep of the environment's dynamics.\"Then, it will tell us what it returns.We get back.The observation is the first thing.Okay, the observation, the reward, done, and info.Okay, so the observation is really the state of the environmentas far as our agent is aware.That is exactly the stuff that we need for doing Q learning or whatever.We need to be able to act in the worldand we know what the result is of our action,which is the next state of the world.Plus the reward we got,whether the game's over, which is the done thing,and then, some extra info as well, in case that's useful.Let's say I doll_env.step(ll_env.action_space.sample ()).Okay, so I'm just going to take a random step.Let's just see, what is that?That's the state that's coming back.The first thing is the state.That first block of stuff up there is the state.Then, the second bit is whether is the reward,which is this -100, which doesn't look like a good reward.The next thing is, whether it's finished, the game is over or not.Remember, we saw it crashing down, so it must be over.Then, something about timelimit truncated.Okay, so what about if I reset it, and then, do it?Then, do another random action.Okay, so you can see now the game is not over.I've got some different values back,which I'm not totally sure what those arebut they're obviously some information about the state of the game,which we can use to train on.Now, what about the breakwall environment?Let's try that, bw_env.reset, let's reset it first.Then, do a step and see what we get back from this one.You can see, first of all, when I called reset,you can see I got back,basically loads of numbers, lots of them are zerosbecause they're basically black pixels.It's given me all of the RGB values for the pixels.Then, you can see, similarly, I'm getting the--That's the observation, that's the state, that big thing.That's the reward, which is zero in this case.That's whether the game is finished or not and there's no info, any extra info.Okay, that is, we're looking at there, the different environmentsgive us back different types of observations,and they take different actions.Yes, different environments give us different thingsand that's really it.That's the end of my introduction to OpenAI Gym.There's going to be a worksheet where you get to play around with thisand build your own random agent and exploreand look at the different environments.That's something you can look at.In summary, the OpenAI Gym provides the workflow for reinforcement learning,of acting, observing, and getting rewards.It's basically giving us everything we needto build one of these Markov Decision Processes,and then, do some learning on that.In summary,we've just been finding out all about gyms,find out what the basic idea of a gym is at first.Then, we looked at the OpenAI Gym in particular.We looked at some available environments.We looked at some different types of environments that it has.We looked at how you can create an environment,how you can take different actionsand that different environments have different types of actions,how you can make observations of the state of the environment,and how you can look at the rewards.Yes, in summary, we've just been looking at OpenAI Gym."
}