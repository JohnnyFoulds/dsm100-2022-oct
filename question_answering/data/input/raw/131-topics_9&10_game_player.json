{
  "course": "DSM100-2022-OCT",
  "topic": "Topics 9&10: Game Player",
  "title": "Lecture: Overview of the DQN runtime",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96478&forceview=1",
  "transcript": "-In this video, we're going to take our first look at the DQN implementation,which we're going to be working with this weekand see how it operates at a runtime levelto see what the steps are giving a general flowchart of the algorithm.Okay, in summary, I'm first of all going to give you a citationbecause the code that we're using is based on an example code,which I found.I'm going to explain why that is in a minute.We're going to have a quick demo of the code runningjust to see exactly what it's doing and see some printouts there.Then, we're going to review the agent architecture,check out the flowchart of the agent runtime,and think about episodes and frames along the way,and also the replay buffer.Okay, here is the citation to start off with.This code is really the cleanest implementationof DQN I could find,and also the one that works on current as of the time of filming in 2021 versionsof TensorFlow and Keras.I'll just say that in order to prepare this course,I investigated all kinds of implementations of DQN.There's a lot of them out there, and this is the one that I found,which I felt was the most elegant and clean,and most easy to understand, and that's why I chose itbecause I figured it was the best one to teach with.Well done to Jacob Chapman and Mathias Lechner,I want to give a shout out to them to just say,\"Really very nice, clean code.Super clean implementation.\"It has an Apache license,so it means we're allowed to essentially distribute itand make edits to it, and so on.That's the reference there and I'll give you a link to this.Let's dive in, start off with a quick demo.I've got in my editor over here.What I want to do is just run my slightly edited version of it.I've edited it.It prints out a few things to tell us what's going on.The first thing that will happen is that it's going to pump outall of this usual stuff.If you've used TensorFlow before,you're familiar with pumping out all that stuff.What's happening now is, the agent is playing the game.In the background is loaded up that open AI gym,loaded up breakwall.It's observing, it's taking actions,and then what it's doing is it's collectinga history of all the observations, and actions, and so on,as described previously.It's putting all those in, it's periodicallyusing that to train the network.You can see, every time it's printing out the network training thing there,you can see it's basically taking a random sample of what it's seenand feeding that into the network.Then, you've got these less frequent outputs,which is the end of an episode, which we'll talk a bit about in a minute.You can see, and that's telling you how many frames it's played.For example, it says it's played 1,572 frames.That means, really the game has,we've called step on the game, 1,572 times.If it's running at 30 frames a second or whatever,that gives you an idea of how many seconds of real-time gameplay it's been doing.It's running at much faster than real-time.It's playing the game faster than real-timeand it's collecting experiences, it's training the network.I've also got this other output,because it builds up the memory buffer over timeand that's just printing out how much of that memory buffer is built up to.I'll just say that,I've been able to get reasonable performanceusing this implementation training on our version of breakwall.The original papers, obviously, playing the direct Atari games,and so we had to reimplement our own versionof the Atari game for this.I've got it to play pretty well.It can keep going for a few turns and it knocks out quite a few bricks.Got an average score of about 20 over time.It's pretty good.I think just because our version of brick wall is differentand I had to train it on my laptop,which has got a special graphics card in for the 24 hoursor 36 hours to get decent performance.I'm going to give you that pre-train networkso that you can play with it and see what it does.Because it takes a lot of power to train it,it takes a long time to do it,but the network can run in faster than real-time.We can actually play the game faster in real-time with the network.Typically with machine learning,the inference where you're running numbers through the networkto get it to actually use it is really fast,but the training is the slow bit.Okay, that's just showing you that.I'm going to stop that because TensorFlowjust uses every bit of power it can find in your machine.I'm going to stop that now.That's a quick demo just showing you that it runs and that it does stuff right.What about the agent architecture, which I said we're going to review?Well, we've just been watching this whole thing running in real-time.We've been watching the agent over here, basically, generating actions somehow,and using that to play the game,and then making observations and storing that into the memory,which it was gradually filling up.We saw that percentage going up and up.It's filling up the memory and periodically training the network.Actually, quite often it trains the network every few frames.What we didn't mention here is the epsilon greedy thing,which I'll talk a little bit more about in a minutewhen we look at the workflow.Okay, that's the agent architecture review.Now, what about the runtime?Okay, over there, you can see the runtime.There's a lot of steps in here and we're going to go and look at thosein the code in a minute, but I've got a zoomed-in version of this, hopefully.Up here, let me just find that. Whoops.Yes, there we go.Sorry, about the scrolly screen.I've got a zoomed-in version here.Let me get out of the way.What we're doing here is, we start off by,basically, creating the gym environment.Okay, that's the first thing we're going to do,and then we create the neural network.We define the key or the layers are in the network.We literally define what the network structure isand create that and then we get into the business,which is, while the running reward is less than 40.Remember, I said I was able to achieve like 20 as a running reward.The idea is, it remembers, of the last few games that it's played,quite a few of the past few games.It keeps a running score of what the average scoreis to make sure that it's consistently learning.It doesn't stop until it's consistently ableto play to a good level.In this case, in the original script, for the Atari breakout,the consistently, good enough score is 40.That's, basically,if it can knock out 40 bricks consistently,the whole thing will exit and that's the end.That isn't really going to happen very quickly.The next thing it does, it carries out an episode of 10.000 actions, okay.Each episode is a lot of actions, and actions, remember,are like one step in the game.Basically, choosing what to do each frame in the game.Then, what we do, is we do the epsilon greedy thing,which I think I mentioned before,which is the idea that most of the time, it takes random actions.At the beginning, it takes random actions.In fact, if we look at the code, let's go back into the codeto see what those properties are.At the beginning, you can see that this is the actual code here.Epsilon starts off at one.Beginning, the chance of it doing a random action is 100%.Okay, and basically, over time, that reduces.Over a certain number of frames, epsilon reduces from its maximumof 1 down to a minimum of 0.1.We still retain some random action, but it only goes down to 10%.Over time, it switches from taking random actionsinto using the network.As it's running,the neural network's getting better and better.It starts off rubbish, it gets better and better,and it gradually shifts on to making decisionsusing the neural network.That's the epsilon greedy process we talked about before.That's the thing it's doing there.That's what we mean by reducing the chance of random.That's basically epsilon being reduced.Then, the next thing it does is, yes, either selectswith random or the Q function.Basically, either uses random or it uses the Q function,which is the current neural network.Based on that, that reducing chance that it takes is action,acts in the game, and then it stores the results.We saw earlier in that last weekend in the open AI gym,that we get the states, and so on, back.We store all that into the replay buffer, and basically,if we've done a certain number of actions, then we're going to update the network,we're going to retrain the network.We saw those printouts coming out.We saw their training network prints out coming out.That's what's happening there and then after a certain number of frames,what it does is it updates the Q network, the Q dash network to the Q network.I'm not sure if you remember back,but when we were going to look at the loss function in detailin the next video,but it's actually maintaining two neural networks.One of them is the current one that it keeps trainingand the other one is used for the loss function,and that one is has got an older version of the weights.Okay, what it does is, every often, it updates the Q dash oneto be to have the latest weights, but not very often.That gives it this stabilitybecause if the network that it was using for the loss functionwas the same as the currently trained network,they found that it didn't train very well.What they do is they keep a network static for a while,and then occasionally update it to the latest weights.Okay, that's how that works and then at some point,we basically start rewriting the memory.Once it fills up the memory to a certain size,it starts writing back at the beginning again,so it wipes over.It's like a looping buffer thing.Okay, that's a general idea.That's what it's going to do.How does that work in the code?Well,let's look at this code hereand find out a bit more about that.I want to go through.Here's the Create Model Codeand we're going to look a bit more detail at that in a later video.That's the basic idea.That's the function that creates the model,then there's a bunch of configuration parametersto do the memory size, how long epsilon takes to reduce down to 0.1.How often you're going to retrain the network,how often you're going to updatethe secondary network to the latest version.Then it defines a type of loss function,which is used to calculate the loss between the input and the output.Then, we got the main loop here, which is an infinite loop,which will only exit if we get that continuous score of 40.Basically, you can see it starts off by resetting the environmentand then we then iterate over max steps per episode,which we can find up heresomewhere max steps per episode it's 10.000.As I said earlier, it runs those 10.000 steps.In this case, we're calling that an episode,but typically an episode is playing the game until you lose.Okay, if at some point we lose the game, then we need to reset the environment.Okay, but otherwise, it will try and keep playing the game10.000 frames if it can.Then, we're obviously counting the number of frames.In this case, this was actually rendering the environment every time.We should be able to see the window, the environment there.Then, what it's doing is then--that's the epsilon greedy stuff.Basically, if we want to choose a random actionor if we want to use the network to generate an action,and then we update epsilon, as we said.That's changed.Over time, it gradually reduces the epsilon.The chances of using the network for an action increases over time,then that's where we actually do the steps things.You've got step, that's where we actually act in the environmentand we're calculating how much reward have we gathered in this episode.We store the reward, the total reward before we die.Okay, that's really how many points have we gained.Then that's managing the Action Replay buffer.The action replay buffer is actually in this implementation multiple listsfor simplicity,you could make it a class that takes in the state action rewardin next state.In this case, we're just maintaining some simple arrays thereand just sticking the latest things on the end.We, basically, got for each of the elements of the replay buffer,we've got a separate list.A certain point, we basically do our--What's this?This is the training.This is training the network.To train the network,which I'm going to look a bit more at this in one when I coverthe neural network, but basically, we select a bunch of random examplesfrom the replay buffer and we use those.We run those through the networkand we then calculate the output of the networkand eventually, that gives us the loss over here.That's probably the most gnarly bit of code really to just calculate the lossand then we feed that loss back into the network there.That's basically saying,\"Okay, you're updating the networkand if we've done a certain number of frames,then we update our Q dash networkwith the latest network.\"As I was saying, anyway, maintain two networks here.Finally, that's the bit of code that wraps around the replay buffer.That's really it.If you trim this one down, it goes down to like 189 lines, basically.It's like not a lot of code to read throughand figure out exactly what's going on.Yes, as I said, the replay buffer is actually a set of lists containingall of those thingsand that's clear and we saw that in the code.We'll cover more about the training in the next later videos,but for now, that's giving you the general ideaof what the runtime of the agent is.We've run the agent with a quick demo there.We've seen who's written the code.We've been through the architecture, been through the runtime.Finally, we just thought a little bit about episodes or episodes,is this thing of running,playing the game until you die, basically, up to 10.000 frames,and then the frames of one step in the game.The replay buffer is this set of lists,which remember what's happened in the gamein terms of states, actions, and rewards, allows us to buildthat kind of state transition matrix stuff for the MDP.Okay.Right, in this video, we've just been taking our firstkind of deep dive into the code of the DQN agentand trying to match a sort of flowchart of the runtimeinto the actual code.I've highlighted the blocks of the code where each of the key thingsthat it's doing are happening.Hopefully, you've got some idea of this cycle that it's doing now of,basically, playing the game for a bit,training the network, occasionally swapping,updating the long term network, maintaining this buffer of experiences,and eventually exitingif the performance gets high enough over an average time."
}