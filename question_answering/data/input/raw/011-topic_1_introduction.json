{
  "course": "DSM100-2022-OCT",
  "topic": "Topic 1: Introduction",
  "title": "Lecture 8: Ethical and legal issues",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96103&forceview=1",
  "transcript": "[music]AI is a dual-use technology.It can be used for peaceful applications such as flight control, visual trackingand navigation, multi-agent planning, but it also can be easily appliedto military purposes.The benefits of AI are great, but so are the risks.AI and robotics have the potential to free us from repetitive work,and dramatically increase production of goods and services.As Demis Hassabis, CEO of Google DeepMind has suggested,\"First solve AI, then use AI to solve everything else.\"Unfortunately, AI and robotics can be usedas lethal autonomous weapons for civilians,for making unfair decisions,and taking our jobs.There are numerous ethical issues associated with AI technologies.For example, how do we protect ourselves against unintended consequences?How does AI affect our behavior and interaction?How to fairly distribute the wealth created by AI?Can AI make mistakes?Can we stay in control of a complex AI system?How should we treat AI, and many other concerns.You, as a specialist in data science, have to be aware of the potential risksof AI technologies and associated ethical issues,and do whatever possible to mitigate those risks and resolve issues.In this mini-lecture, we will discuss some of issuesassociated with AI technologies.Autonomous weapons have been calledthe third revolution warfare after gunpower and nuclear weapons.Their military potential is obvious.Few experts doubt that autonomous fighter aircraftwould defeat any human pilot.Autonomous aircraft, tanks, submarines, can be cheaper,faster, more maneuverable, and have longer range.Examples include Israel's HAROP Missile with a 10-foot wingspanand a 50-pound warhead.It searches for up to six hours in a given geographical regionfor any targets that meets given criteria and destroys it.Since 2014, the United Nations under the auspices of the conventionon Certain Conventional Weapons are discussing if to banlethal autonomous weapons.The ethical side of this discussions is that many find it morally unacceptableto delegate the decision to kill humans to a machine.Antonio Guterres, the Head of United Nations,stated in 2019,\"Machines with the power and discretion to take liveswithout human involvement are politically unacceptable,morally repugnant, and should be prohibitedby international law.\"Personally, I think it is a blueprint for solving ethical issues.Even if AI technologies have the capability for causing harm,the legislation must prevent such possibilities.Unfortunately, when it concerns AI, the legislation often lags behind.It is obvious that more efforts in this area are required.As of 2018, there was 350 million surveillance cameras in Chinaand 70 million in US.China and other countries are exporting the surveillance equipmentto other countries.Some has reputation for mistreating their citizen.More and more data about our daily activities are being collectedby governments and corporations.They want to use such data to prevent crime, to stop terrorists,to have better services, and we want to cure diseases,but we don't want to compromise our privacy, our human rights.We don't want to live in a police state.This quote from Virginia Eubanks illustrates how much data is collectedfrom us in our everyday lives.\"Digital security guards collect information about us,make inferences about our behavior and control access to resources.Some are obvious and visible: closed-circuit cameras bristleon our street corners, our cell phones' global positioning devices recordour movements, police drones fly over political protests,but many of devices that collect our information and monitor our actionsare inscrutable, invisible pieces of code.They are embedded in social media interactions,flow through applications for government services,envelop every product we buy or try.They are so deeply woven into the fabric of social lives that,most of the time, we don't even notice we are being watched and analyzed.We all inhabit this new regime of digital data.\"The legislation must enforce the right way of collectingand using data.Data collectors have a moral and legal responsibilityto be good stewards of the data they hold.For example, in Europe, GDPR, General Data Protection Regulation,mandates that companies design their systems with protectionof data in mind,and requires that they obtain consent for any collection or processing of data.One of the primary concern arising from the advanceof AI is that human labor can become obsolete.Oxford Economics predicted that 20 million manufacturing jobscould be lost due to automation by 2030.In 2017, Frey and Osborne surveyed 702 different occupationsand estimated that 47% of them are at risk of being automated.For example, 3% of all the workforce in US are vehicle drivers.The task of driving is likely to be eliminated by driverless cars,taxis, and trucks.McKinsey estimates that only 5% of occupations are fully automatable,but that 60% of occupations can have about 30% of their tasks automated.The fact is, AI mainly taking over tasks, not jobs.Historically, there were several periods of technological unemployment.For example, in the beginning of the 19th century, weaverswere replaced by automated looms.However, the employment levels eventually recovered.There is a compensation effect that: Greater productivity leadsto increase in overall wealth.It leads to greater demand for goods and services.It leads to increase in employment.An example, ATMs replaced humans in the job of counting out cashfor withdrawals, that made it cheaper to operate the bank branch.The number of branches increased, more bank employees overall.The effect of AI may be similar.For example, one of the common jobs of the future can be labeling datasets.The most realistic scenario is that humans labor will not be obsolete,but the tasks humans are working on will change.In 2019, IBM predicted that 120 million workers would need retrainingdue to automation by 2022.Another important issue is that AI systems are often unable to explaintheir decisions and recommendations.Explainable and interpretable AI is a hot topic.Explainability and interpretability matters because it is givingusers confidence in the system.People seek explanations for a variety of purposes.To support learning, to manage social interactions,to persuade and to assign responsibility.Safeguarding against bias: In order to check or confirmsthat an AI system is not using data in a way that results in biasor discriminatory outcomes.Assessing risks, robustness, and vulnerability.This can be particularly important if a system is deployedin a new environment, where the user cannot be sureof its effectiveness.Interpretability can help developers understandhow a system might be vulnerable to so-calledadversarial attacks.The next issue we will consider is AI safety.It would be unethical to build and/ or distribute an unsafe AI agent.AI system has to be safe, should be able to avoid accidents,be resistant to adversarial attacks and malicious abuse, and in general,to cause benefits, not harm.Unfortunately, at present, there is no means to guarantee that.Almost things technology has the potential to cause harmin the wrong hands.The concern about AI and robotics in the handsmay be operating on their own.Many are raising serious concerns that AI systems can posea threat to humanity because of their potentialto self-improve at a rapid rate.It is up to us to develop AI systems that benefit humanity,not harm it.There is a long history of safety engineeringin traditional engineering fields, and we should employtheir methods for making AI safe.For example, in the failure models and effects analysis,analysts can see that each component of the systemand imagine every possible way the component could go wrong.The field of software engineering is mainly focusingon the correctness of the programs, not their safety.This is shifted now towards safety.For example, what if a tire of a self-driving car is punctuatedat the highest speed?A safe system would be tested for this, and would havesoftware to correct for the resulting loss of control.Engineering and Physical Sciences Research Committee UKand many other government organizations set principlesof AI and robotics.The most common principles are insurance safety,ensure fairness, respect privacy, promote collaboration,establish accountability, uphold human rightsand values, respect diversity and inclusion,avoid concentration of power.In this mini-lecture, we discuss only someof ethical issues associated with AI technologies.AI has a great potential to improve the quality of our lives,but there are also serious risks.Unfortunately, there is not enough recognitionof the pressing need to address ethical issues.According to a McKinsey survey, despite growing callsto address ethical concerns associated with using AI,efforts to address these concerns in the industry are limited.For example, issues such as equity and fairness in AI continueto receive comparatively little attention from companies.Moreover, fewer companies in 2020 view personalor individual privacy risks as relevant compared with in 2019.That puts even more emphasis on having in place regulations,guidance, and laws to govern the development,in a moral and ethical manner.exploitation of AI technologies"
}