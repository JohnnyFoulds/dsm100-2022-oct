{
  "course": "DSM100-2022-OCT",
  "topic": "Topics 9&10: Game Player",
  "title": "Lecture: DQN agent architecture",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96451&forceview=1",
  "transcript": "[music]In this video, we're going to get started looking at the DQN agent architecture.In summary, we're going to think about the overview of the architecture for us of all.Then we're going to think in detail about the replay bufferand how it's filled up using Epsilon greedy exploration.First of all what is an agent?I'll start referring to this thing as an agent and it'san entity that can observe an act autonomously.Just trying to come up with a really simpledescription of an agent, it just means some entitywhich can act in the world and observe and have somesort of autonomy so it's making its own decisions.You could roll into that, well it has some goals, it's tryingto maximize its reward going into the future as well.That's another feature of it.What's the agent architecture for?Essentially we need the agent architecture to solve these problems.We don't have a state transition matrix and we don't have an action policy.We need some way of gathering the data that we need in orderto develop those by basically learning this Q function.You can see that it's got a whole bunch ofmechanical bits in the agent which allow us to do this.I'm going to explain how they all fit together and work.This is an overview of the agent architecture and I'm going to splitover here to my drawing program so that I can show you that.What have we got?Let's just go for the high-level stuff first of all.First, on the left here, that block that I've highlighted there, that is the agent.Then this is effectively the game simulation or the game environment.The first thing to think about is the data that's flowinginto the game environment and out of it, back out to the agent.What we've got is the agent makes observationsof the game and what's happening in the game.Also, it knows about the rewards.We've got states and we've got rewards.Then the agent acts in the world by taking actions.That's really the flow between the two.Then, within the agent, we have a replay buffer over here.If we zoom into that you can see that it consists of exactly what we'd expect.It's basically gathering observations of statesand actions taken in those states, what the nextstate was, what the reward, was and whether the game was done at the end of that or not.The replay buffer consists of many hundred thousands of observationsof what's happened as the agent is taking these actions in the world.Then, the other key element of the agent is the Q function.This is the thing we're going to be training.This is basically the deep neural network which is being told, \"Oh, here's the state.What is the action I need to take?\"It's essentially this value function which tells us in the given state,what are the value of all the available actionsso that I can choose the highest value action.What happens is the behavior of the agent is, it's going to becollecting observations and I'll talk about how that works in a minute.Then, periodically, it's going to stop collectingobservations and it's going to go off and it'sgoing to train the Q function up on its currentset of observations or some batch of that.That's essentially what the agent's going to be doing.The replay buffer; let's think about the replay buffer.The replay buffer this is really how the agent gathers,learns or gathers information about the world.The question is how does it do that?How does it know, how does it take actions in the world?Essentially, just to clarify what's in there just in case we've forgotten.State S is the state now, A is the action taken in that state.S' is the next state.R is the reward and done or D is true or false is the game finished.That's what we mean by the replay buffers, it's basically a massive list of those.The data might look like, for example, \"I was in state 1.\"You see over there.\"I was in State 1, I took action 1 and I sawstate 2 and I gained reward 1 and done with 0.\"Just to make that really clear what's going on there.What's Epsilon greedy exploration?This is the method that the agent uses to fill up the replay buffer.This is how it gathers observations.Epsilon greedy exploration sounds really complicated but it's not.Let me explain how it works.If I go back over here.What's happening is I've just explained, we've got this cycle where the agentis essentially taking actions in the world and making observations.The question is what are the actions?At the beginning, we don't have a Q function.There's no Q function.We don't have this magic function that tells us what to do.All it does is it actually has another block here which we can just call random.Instead of choosing to do something according to the Q function,most of the time, at the beginning, the agent just says,\"Well, I'll just choose a random action.\"Random actions.It just doesn't really even worry about the state.It just takes a random action every time.It's the equivalent of just you're watching the game,just moving the joystick at random.Every frame you're going,  \"Oh left, right, nothing. Left, right,nothing,\" whatever.It's just randomly acting in the world.That exploration yields a load of observations in the replay buffer.Periodically it's going to take those observations and use them to train the Q function.Now, the thing that makes it Epsilon greedy, the greedy bit is, I guess, the random.Greedy means using the Q function to choose your action because that's more expensive.That's my understanding of the word greedy.Greedy means you're using the Q function and Epsilon greedymeans Epsilon is just a value that's changing over time.It dictates how greedy you're going to be.At the beginning you start off with zero greed.Let's just draw a little graph here.Time and Epsilon with is a wiggly E like that.That's our greed.The greed level starts off at 0 or very low and eventuallyit goes up to 100% or somewhere thereabouts.What do we mean by Epsilon greed?Epsilon is increasing over time.That's how greedy we are.Remember greedy means you're using the Q function to choose your move.You can see here at the beginning we're not greedy at all.That means we're just using the random function to generate our moves.As Epsilon increases- and this may be many thousandsor millions of iterations of playing the game- as Epsilonincreases we more and more start relying on the burgeoninggrowing learning Q function to select our moves.What happens?Typical pattern would be at the beginning we are going,\"Hit the random, keep hitting the random. Keep hitting the random.\"Then we do some training.Then Epsilon goes up a bit.Then we're hitting the random.Occasionally, we're going to use the actual Q function to make a decision.Then we're going to hit random again.This is the pattern.As Epsilon, the greed increases, we lean more and more over to that Q function.We start using our Q function which is improving over time, of course.The beauty of that is it starts off givingthe agent a random exploratory behavior but asthe agent learns over time, so as the Q functionimproves over time, it uses it more and more.That allows it to basically learn how to playthe game by randomly exploring and then startingto use an actual functional policy which is actually going to give it reward.Because, of course, behaving randomly doesn't yield a reward generally but it'llfind roughly patterns of sequences, maybe at random that allow it to yield a reward.Then eventually the Q function will allow it to put those together into bigger sequences.This is the idea.It's a really elegant solution, the idea of changing the behavior agent overtime to be more and more reliant on what it's learnt about the environment.That's pretty cool.That's one of the main features of the deep Q learning.That's all I wanted to say for this video.I wanted to give you that idea of these Epsilon greedy explorationand give you an idea of what the overall agent architecture is.In this video, we've just been finding out about the basic features ofthe agent architecture and how it uses Epsilon greedy method of exploration."
}