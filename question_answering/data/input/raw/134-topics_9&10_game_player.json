{
  "course": "DSM100-2022-OCT",
  "topic": "Topics 9&10: Game Player",
  "title": "Lecture: DQN code review",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96484&forceview=1",
  "transcript": "[music]In this video, we're finally going to see the DQN agent playing a video game.Let's look at the summary and see what we're going to go through.First of all, I want to give you some informationabout how I went about actually running the training sessionsbecause it wasn't as easy as you might think.It's not just a case of running the script.I'm going to show you some plotsof how well the training went in a couple of examples sessions.We're going to talk through the waythat I can then use the agentto play the game in real-time and view that,so we're going to see how that script works.We're going to have a demo of it actually playing the game,and we're going to reflect on the experience.First of all,when I was developing this course, before we get into that,my initial phase, it was just really trying to figure outwhat's the best way to show you the DQN agent.I looked at loads of different implementationsof the DQN agent in different languages and things like thatand I settled on the one that I chose because it's a very clean code.My first step was to verify, \"Does this actually work?There's no point in me going through it if it's not going to work in the end.The first thing is I verifiedthat the DQN agent could train to play the Atari Breakout game.Indeed, it does train very well.We'll see a graph that proves that shortly,but unfortunately, because I'm not able to show you the Atari Breakout game itselfbecause of copyright reasons,we then created a clone of Breakout called BreakWall,which is kind of similar but it's different in certain ways.I attempted to train the same agent to play that game instead.It wasn't quite as successfulbut it still clearly learned some technique on how to play.We're going to be seeing the demo of that agent playing it shortly.How do I go about training it?The thing with the Keras script is that,it has this one problem which I encountered,was the size of the replay buffer.In the original nature paper, the replay buffer is a million units.They have a million states and after-statesand everything else stored in the replay buffer.That takes a lot of RAM.In the Keras script, they've reduced that to 100.000.I found that on a 16-gig machine,I still couldn't actually maintain that size of memory buffer.It would just crash out the script.My plan of, basically, training on my learning machine heredidn't work because it only has 16 gig.I, first of all, tried it on Colab and on my regular Mac.I've got a 10-year-old Mac that has 24 gigs of RAM on it.I was able to run the script on both Colab and the regular Mac.If you haven't seen Colab before,it's Google's Jupyter Notebook system,which is really goodbecause you can access it for free and run training sessions,and it has a GPU backend.You can actually get access to an accelerated training environmentfor TensorFlow. That's pretty cool.I was able to train it on Colab,and also on my unaccelerated Mac, but, obviously, that was quite slow.Finally, I've got some RAM for this machine.This machine now has 64 gigs of RAM in it, and this is my learning machine.It's got a GeForce 2070 chip in it, and that allows me to train much faster.I did a lot of my training on that machine.One of the models we're going to see laterthe results of only just finished training it.Let's look at some training here.This is the agent being taught how to play the original Atari Breakout.The graph is going up.Graphs going up is generally good,in machine learning, means it's getting better.What does the graph mean?The graph is the running average scorethat it's achieving across multiple episodes?Remember, an episode is basicallythe agent playing for either 10.000 frames,or until it loses the game, so the ball goes off the screen.This is the running average score. You can see over many 1,000s of games.It is eventually getting some pretty decent scores there.It's clocking in a regular 10-point score,which means it's knocking out 10 bricks very consistently.Probably, most of the time, it's getting more or less,but averaging out 10 or 11.That's the first one.That's my proof that it is able to learn how to play Atari Breakout.I'm going to give you this model.I can't show you this model playing the game,but I'm going to give you this pre-trained modeland a script that lets you run it on your machine.You can see actually an AI playing Atari Breakout on your own machine,because whilst the training does require loads of RAM,and, ideally, a graphics card that accelerates the training,actually running the model's very fast.It doesn't require as much RAM, hardly any,and it is very efficient.Thisis the slightly more disappointing trainingof BreakWall.You can see it is going up.There are some points when it got pretty high,but it's not quite as consistent.The thing is, I might need to train it for a lot longer to really get very good,but what happened is, while the training was going on,it actually fell off the cliff.You see it just dropped down.The laptop didn't literally fall off a cliffbut, certainly, the running training performance did.This one, I had to really abandon.I will try a few more runs, see if I can get a better model for you.I've got a reasonable model.We're going to see it later, which I did in another run,but you can see that clearly BreakWall is not training as well.We're going to talk a little bit about why that might be later.Now, I want to just actually run the script and show you,but before we do that, I'll just remind you of the agent architecture.Actually, we're not going to use that bit, so we cross that out, cross that bit.We're only interested in this bit for nowbecause this is the bit we're going to be running.Essentially, we're going to be running the game.The game is going to be running.We're going to take those screenshots, the four screenshots.We're going to be sending them through to the Q function,which is essentially the trained neural network.The Q function, remember, is our trained neural network.It's going to get the status.It's going to tell us what the value of each of the actions is,and then we're going to take the highest value actionand do that, and then keep going.That route goes round and round, and it'll play the game.I've got a script that does exactly that.Remember, I've removed those two bitsbecause we're not remembering what happenedand we're not training in this version.We're just running the model.I've got a script that does that.Essentially, what it does is, it creates the model.Just like we did before, it creates the Keras model.I can just quickly show you the code for that.It's really the same code, the create model code.It creates all the layers and returns the model,and then, it loads the weights.The next step is that it loads the weights.I'm just finding where that is. There we go. It creates the model there.Loading the weights is actually super easy.You just call model, load weights like that,and then you pass it the weights file.Where did I get the weights file from?The training script I have, what it does is,every time it hits a historic high in the running,training performance,it will just save the weights down to a file.I've got a folder over here with all of my pre-trained models here,and some of which I'm going to give to you.That's the Atari Breakout one.Those are the best ones that it achieved, and then, that's the one.There's a few different snapshots of it playing the BreakWall,and there's another training session I did on my Mac.Basically, I've got loads of different pre-trained models.I basically load one of those in and call load weights,and then, it loads those 1.6 million parameters into the model.Once you've loaded the weights, you create the environment.I've got a little function here that creates the environment.The core of that is this bit.It isn't just as simple as creating a regular type of Gym environmentbecause this Gym environment doesn't just give you the raw state.It actually pre-processes the state a little bit.If you remember back to quite a few videos ago,the image processing,so remember, where we turned it into a grayscale image,and then, we store the last four framesand we also resized the imagebefore it even goes into the neural network.That's creating a specialized version of the environmentby wrapping it in these special functionswhich come from the open AI baselines package.That basically gives usthe same environment as they're used in the paper.That's the idea, more or less.Once I've created the environment,I've got my model, I've loaded my weights, I've got my environment.Now, I just need to do the runtime. That is pretty straightforward.Let's just go through that.This is the function.We start out by resetting the environmentand storing the state that we got,and then, we call render on the environment.That pops up a window to show us what's going on.We convert the state that we receive from the reset functioninto an appropriate format,the tensor format that we can pass into the model.We basically pass that state tensorinto the modelwhich will give us back a set of probabilitiesof really the values of all of the actions.We select the highest value action with this tf.argmax function there.That gives us the highest value action,and we then take that action in the world by calling env.step,passing it the action we've chosen,and then, it receives the results of taking the action, the reward,and the done state.If the thing's done, if basically it's lost the game,it just resets the environment again,and then, it all starts again.That's essentially it and that is the script.What we can do now is run it.I'm going to go over here and run it.This is the one that I trained that we just saw the graph for.You can see it's missing sometimes, but it's clearly sometimes.It's not too bad. It's actually getting to it.It knows where the ball is.It's figuring out where the ball is and getting the bat over to that.You see it got it there and it's going to go.It got all the way up to the side.It managed to predict where the ball was going to go.Clearly, it's learned.It's not exactly the same.When you run the Breakout model on your own machine,you'll see that it's much better.It gets a much higher score, but this is not bad.Clearly, it's not just random, is it?It's definitely following the ball and getting the bat in the right place.Clearly, this has learned how to play the game.The reason that's interesting--You might say, \"Well, it's quite a simple game.It's not that interesting.\"The reason it's interestingis because we didn't tell anything about how to play the game.We only told it when it gets a point,and getting a point can happen quite far in the future, can't it?It might actually only get the point after several runs.I'm going to get that out of the way.You might have to wait for age. There's the point.Imagine how many frames there are between that and the point.Hits the ball, wait for ages for the point.Oh, lost it.That's why it's interestingbecause that's the reinforcement learning thing.Suddenly, oh, you got a pointand the move that you made to get that point happened ages agoor it's a whole sequence of moves you have to makein order to get that point.That's what is really interesting.Also, we didn't tell it anything about the game.Just by looking at that view,it had to figure out what is the best way to process thatwith the convolutional layers and everything,training all of those filters in the neural networkto extract the salient information from that image.Just out of interest,let's just look at a less well-trained model ,one which is here.Let's have a look. What have we got?This is after 145 episodes, so basically, not many games.This is it running with much less training.You can see herebasically the strategy that it's learned so faris just to move over to the right.It's not acting randomly. It's not just random, is it?It's just basically figured out,sometimes, when I move over to the right, I manage to hit the ball,so I'm going to do that just about every time.You can see, it's a bit reacting a little bit to the ball.That's what it looks like at an earlier stage of training.Some reflections on that.First of all, I will give you that modelso you can run that on your own machine and have a look at it.I will also give you the Atari modelwhich you'll see is a vastly superior in performanceand perhaps more impressive.What can we say?It's not amazing, but it's clearly learning some techniques,as I saw earlier.Why is it not as good as the Atari one? It's a completely different game engine.We actually built it all in Pi Game,and the way it generates pixels and the drawingis also a different resolution.Even though it gets scaled down, it starts out at a much higher resolution,so who knows what's going on there.Also, confession time, it was quite a hacky adaptation I didto get the game that was written in Pi Gameand convert it into a Gym,and then, to get it to wrap up in the right wayso that it looked like an Atari game,so I can just have it interchangeable with the Atari Gym.Maybe, I've made some little flaws in the conversion therecompared to what it should be.Clearly, it works,but just not quite as well as the Breakout does.In summary, we've just been looking at a real trained networkand seeing how it plays the game.I started out by giving you some informationabout the practicalities of training it,how I tried out various different environments.I was running it on my machine that I've got over here.I was training it online in Google Colaband running it on an old computer for days.I tried all kinds of different things.That was just my process of figuring out the best way to train it.I showed you some plots of how well it trained,and we saw clearlythe Atari Breakout version of the environment.It trained better on that than it did on our adaptation of thatbut, still, when we ran the simulation,we were able to see that it actually played BreakWallreasonably well as well.Definitely getting a few points.That's certainly a lot better than random and following the ball and everything.We, at the end, just reflected onmaybe what the difference is a couple of things.We just didn't run it for as long.There was an issue where it technically fell off a cliff, the performance,and I haven't fully debugged that yet.Also, various other things that were different,the way I converted my environment into the appropriate one for the Gym.In this video, we've just been looking at a real AI playing a videogameand seeing that it can learn how to do it.The performance is not bad,but just also acknowledging the practical difficulties of training,and also having to adapt it to a new game environment."
}