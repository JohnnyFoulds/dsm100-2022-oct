{
  "course": "DSM100-2022-OCT",
  "topic": "Topics 9&10: Game Player",
  "title": "Lecture: The DQN neural network architecture",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96480&forceview=1",
  "transcript": "In this video, we're going to be lookingat the actual neural network architecture that is used for the DQN agent.We're going to start by just having an overview of the architecture,then we're going to go through each of the layers,and just look at inputs, convolutional layers,looking at the different features of the convolutional layers.Then we're going to look at the dense layers at the end.Finally, the output layers, and we'll look at a summaryand see how many parameters there are in the wholenetwork because it's quite a sophisticated piece of kit.There is the overview of the network.You can see at the top, we've got the inputs coming in,and then we've got three convolutional layers.You remember from my videos previouslythat a convolutional layer can actually apply many filters.In fact, we'll see how many filters each one of them applies.For example, you'd have 32 filters being applied in a single convolutional layer,so they're pretty sophisticated layers.We've got three of those, and then we've got a flattening layer,which I'll talk about more in a minute.It basically takes the really complicated output of a convolutionand turns it into a single layer of nodes.Then we've got two densely connected layerswhich are layers where everything's connected to everything.Then finally, the output.Just to remind ourselves, this is the Q-function.In Q-learning, the problem with Q-learning is getting that Q-function,which tells us the value of all of the availableactions so we can choose what to do given the input.The input is the state and the output is the value of all of the actions,and that's exactly what a Q-function's supposed to do.That's the purpose of this network.There's that citation again.Chapman and Lechner,just noting that they wrote the code which we might look at in a minute.Let's go through the layers.First of all, we've got the input layers.You'll notice that we've got 84, 84, n, there.In the DQN paper,what they do is they take the screenshot of the game,which in our case in Break Wall I think is 800x600or something, and they resize it to 84x84.There's actually some code in the Atari Gym.Environment which resizes it down to that size,so before it even gets into the neural network, the image has been resized.Not only that, but you see you've got this n thing there,n is the number of frames.In this example, we've got 4.If I look at the properties of that layer.Here, I've got my network summary.With Keras, you can call model. Summary,and it will tell you all about the model you've just defined.You can see that we've got... That's the size of the input.It's 84x84x4.This is quite a common thing for an image input.You have your XY frames of the image,and then the XY define the frame of the image,plus you've got multiple channels.It's not a black and white image.It's typically got red, green, blue, and then an alpha channel.We're cheating a bit.Instead of having all the different colors,we've got just a black and white image,but then we've got the last four frames that we've seen of the game.It's sneakily feeding in a bit of history to the networkso it can maybe see where the ball has already beenor what's happened in the last few frames.That's the first thing.That's the input, and that's the simplest layer.You can conceptualize the input layer as just reallya bunch of nodes 84x84x3 which means it's a tensor.It's 84x84, so we've got 84 that way,84 that way, and then it goes that way 4 as well.It's a kind of cuboid structure,which is what we mean by tensor.That's what's coming in.Incredibly complicated input layer there.That allows it to receive the image.Yes, said that.Then convolution.We know about convolution.I talked about that in a previous video, so I'm not going to repeat myself.It's basically this idea that you can filter images,and we know all about that.The first convolutional layer has this specification.If we look in the code, we'll see exactly that.It's a Conv2D, which makes sense because it's a 2D image coming in.You might use a Conv1D if you're, for example,processing a time series like audio.You might use a Conv3D if you're processing a time seriesof video frames or something like that.What is it?It's Conv2D.You've got 32 filters.That's the first thing.Remember the visualization that we sawbefore where I showed you the different things.We've got 32 filters being applied to that image.The size of the filters is 8,so it's an 8x8 filter, and the strides is 4.It's skipping four steps each one,so it's not actually processing every pixel individually.It's jumping through the image missing out some of the pixels.If we look at the size of that over here,you can see that it comes out actually 20x20x32.32 is the number of filters and 20x20 is the size of the output of each filter.Actually, it's going down.The convolution is scaling it from 84x84,so that image is being scaled down to 20x20once it's gone through each of those filters.Then we've got 32 instances of that image, one for each filter,so it's got 32 images coming outof the other end but they're scaled-down.That's it.Then the next thing is... Oh, where's the... Sorry, I'll just goand look at the code for the other convolutional layers.Yes, there we go.Here are the other convolutional layers.The next one is 64x4.That's a 4x4 filter being applied.644x4 filters being used to process the image.That's got a stride of 2,so that's moving more carefully through the image if you like.Instead of skipping four pixels each time,that's going 2, but then given that they're only 20x20,you want to be stepping through those fairly carefully.If we look at the size of that one,it's actually then giving me a 9x9 image coming out of that filter but 64 of them.You then got 649x9 images filtered, so double processed.Then we've got another layer againwhich is another 64 filters with a 3x3 filter,and that scales it down to 7x7.That now, coming out of there, you've got really tiny images coming out.7x7 but 64 of them.Remember, the whole idea of these convolutionalfilters is about reducing the amount of dataand just pulling out the salient features.That's what they're doing.They're just trying to just compress that image down.It's a form of compression.We're taking out just the most important featuresof the image that allow the network to train properly.What's next?We've got a flatten layers.A flattening layer.Because this convolutional layer is this complicatedthing where it's actually... What was the shape again?It was 7x7x64.We've got this 7x7,so 7, 7 by 64.We've got this quite complicated structure coming out of that.What we do with the flattening layer is we basically wire all of that.We flatten that back out into just a flat layer of nodes like that.You can think of that as being7x7x64 numbers.Each of those circles is a number,and so we're unpacking them all into a single list, basically.That's what the flattening layer is doing.Hopefully, it makes sense that 7x7x64... If I just do a quick calculation.Python, 7x7x64gives us 3,136.Does that make sense?Yes, because you can see that the flattened layer has gotexactly the same number of units in it if you like,but it's a single flat layer, which makes it easier to connect up.Then we connect that to a densely connected layer of 512.What we do then is we go down from that 3,000,so that's 3,136 of those.It goes down to a 512 densely connected layer which is smaller, basically.What do we mean by densely connected layer?Well, that means that each node in the 3,136 layeris connected to each node in the 512 layer.That's what we mean by densely connected.That means that each node gets to contributeto the activation on the next layer of every node,but obviously, all of the weights, all of those lines have a trainable weight.That's what we mean by the number of parameters,which you can see if I just disappear.You can see that the number is telling us how many parameters each level has.That first convolutional layer has this many trainable parameters,and then even more, even more, and eventually,the whole thing has 1.6 million trainable parameters.We've got two dense layers, the second one.Finally, we've got a densely connected one to give us the outputs,which is 4 because there are four possible moves,which I don't know what exactly... I thought there were three left,right, and do nothing, but there must be a fourth move on the Breakout game.That's why you've got four moves at the end there.The idea is that finally, at the bottom of this network,we get four simple outputs which are for each of the moves,and we just get a probability.That is what the key function outputs.That's the value of each of the possible moves that we can make.That is what we want in the bottom of the network there.That's it.Before we go to the summary, let's just view that overview again.Just to review what we've done there.We've got the inputs coming in,we've got three convolutional layers, we've got a flattening layer,and we've got two dense layers finally leadingto the four output points at the end there.It gives us these 1.6 million trainable parameters.All those parameters rule those filters, all the weights connecting everything up,all the weights in those densely connected layers, all of that stuff.You can see why there's so many parameters.Let's summarize now.In summary,we've just been going through the network architecture for the DQN agentand sort of referencing the code and just looking at how many parameters there areand everything else and just reviewing convolution a little bitand talking about densely connected layers and flat layers and so on.Finally, that essential output layer that gives us the four valuesfor the different possible moves that we can make,which is what we want to make our decision about how to play the game.In this video, we've just been goingthrough the neural network architecture that's used in DQN."
}