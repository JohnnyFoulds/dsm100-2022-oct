{
  "course": "DSM100-2022-OCT",
  "topic": "Topics 9&10: Game Player",
  "title": "Lecture: Convolutional layers",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96470&forceview=1",
  "transcript": "[music]In this video, I'm going to be talking about convolutionand convolutional neural network layers.In summary, first of all, we're going to think aboutwhat is convolution, and what convolution does to images,and then we're going to be thinking about how we can embed this processof convolution inside a neural network and use it to enable the neural networkto learn how to process images to extract the salient information.We're going to be talking about how we parameterizethese convolutional layers and the neural network,and then at the end, we're going to be seeing a visualizationof how a real neural network sees a video game while it's being played.I wanted to start out by just reminding youwhat the agent architecture is because this kind of motivateswhat we're doing.What we've got here is the agent architecture,which we've seen in a previous video.This is the agent and this is the video game,and the basic idea is the agent makes observationsabout what's going on in the game, which are essentially the screen grabsof the pixels on the screen, and then it makes a decisionabout what to do in the game. It makes moves.It might move the bat right or left, or it might do nothing.What's interesting for our convolutional layersis these inputs, which the video game screenshotsare fed into this neural network which then outputswhat the appropriate action is based on what it can see.Inside here, we're going to be putting convolutional layersbecause they're commonly used to process images.That's a just quick review, where this fitsinto the agent architecture.As I say, the motivation is that the DQN network architectureuses convolutional layers, and it does make sensethat it uses convolutional layers, because they're commonly usedfor image processing tasks.Lots of the image classification models, pre-trained neural network modelsthat you'll see are based around convolutional layersbecause they're extracting information from image data.First of all, what is convolution?It's a filtering technique often used for images, but it's also usedfor other signals like sound.If you're on the BSc, and you've taken the graphics programming course,then you will have seen convolution already there,and if you're on the BSc, and you've takenthe signal processing course, then I talkedabout audio signal processing in that course.The idea is that you've got your image or your signal.Let's talk about images.The task of convolution is to calculate a new version of that image by applyingsome sort of filter to the images.Essentially, you go in and you pass a filter over the image,and that generates a new image.Let's look at some examples of that.I don't know how well you can see this, but the idea is that the left-hand imageis fairly sharp and the right-hand image is blurred.Hopefully, you can see that there's it's slightly blurrierdepending on what kind of thing you're looking at this video on.That's one example, and the way that one works,and we'll look at the details in a minute is essentially,what we do is the filter is placed over each section of the imageand it mixes together the various values of the pixels into the new pixel value.It's just the equivalent of essentially an artist havinga bunch of paint on a canvas that hasn't dried yet,and they're smearing that paint around.The paint gets mixed together, so you get this kind of averaging effect.That's one example.Here's another example that's perhaps more striking,and this is called edge detection, which is very similar.It's exactly the same process.It's just we have different ways of processing the image.In this case, the filter instead of averaging outover the pixels to make a blurred version, it almost super sharpens it.It blocks out any pixels that don't change compared to their neighbors,but pixels that do change relative to their neighbors get emphasized.What you see in this image is that the bits of the imagewhere there's rapid change between neighboring pixels get emphasizedso they go brighter, and the bits of the pics,the image where there's not much change gets de-emphasized.That's what we mean by edge detection.How does it work?I've got an animation for you here, which I've prepared in P5.The gray thing is the image, and then the green thing moving acrossis the filter.We don't filter the whole image at once.We basically apply the filter for various positions in the image,and we calculate the new pixel, the center pixel of that moving filteris calculated, and the reason I've got a square over top of other squaresis to show that the new pixel value is based around some sort of calculationsthat's done on multiple pixels surrounding it,and that green thing moving across there is called the filter kernel,and it can be different sizes.If I just edit the code a bit, we could havea 9 by 9 filter kernel as well.If I reload it and play that, that was obviously huge.Each new pixel is calculated based on all of those other pixelsthat's almost taken account of the whole image.Obviously, with a real image, it's going to be 1000s of pixelsby 1000s of pixels, but we could do a 5 by 5,which might be a bit more subtle.In this case, it's taking account of quite a few of the pixels around it,but as I say this is just a demo.Okay, so how does it work?The image kernel, which is that box that we saw moving across,has coefficients, and the idea is that you multiplyeach of the pixel values in the kernel by these coefficients.The left-hand one is the edge detect.What we're doing, you can see that we're basically negatingthe surrounding pixels and emphasizing the center pixel.It's almost certain that that gives you that edge detect effectwhen you apply that in that way over the whole image,and then the blur effect is done by essentially calculating an average.If you think about what you see in that blur filter, each thing is a 0.5.It's taking a little bit of each of the pixels or half of the valueof each of the pixels around it, and mixing them togetherinto the new value.It multiplies all the pixels by those values, add them together,and that gives you your new pixel value.Those are two classic convolution filter examples.What is a convolutional layer in the neural network?As you might imagine, it is a neural network layerwhich applies trainable filters to images or other data.Instead of hard coding the filter parameters,like we did in the previous slide, we've got question marks.We don't tell the neural network layer what the filter is going to be.We don't start off with these filters.It has to learn a filter, and the way it learns the filteris by training.We pass it a bunch of examples, and then we train it,and it feeds back the error back into the configuration of the filters.There are a few different properties which definehow a convolutional layer operates.Firstly, how many filters does it apply?In the examples we've been seeing before, like the blur and the edge detect,we're only applying a single filter, but a convolutional layercan apply multiple filters to the same image.Then secondly, it's the size.That's what we saw when I was adjusting the number,how big the box was.That's the size, how big is the filter kernel basically.Then finally, the stride.In our example earlier, it's moving one pixel at a time.If we just look at that one very quickly again,you can see it moved across one pixel at a time.The stride allows you to jump more than one pixel at a time.You can basically make a smaller image.If you only, let's say, you've got a 10 by 10 image,and you're stepping through it with a bigger stride,you might end up with a 4 by 4 image at the end,because you've made it smaller by leaping through,and only calculating a certain number of pixels coming out of it.It can actually shrink the size of the image.It filters and shrinks it typically, and does that multiple times.Now, don't worry if that's a bit abstract, we're going to seea very concrete example shortly.The reason we can see a concrete example is that we can actually plug into the convolutional layer.Remember, basically, we just remind ourselveswhat it looks like, so it's that neural network again.Imagine that this is the convolutional layer,and it's applying a bunch of filters.We can actually hook into it.We can hook in, and we can visualize it.You can see what's coming out of it in terms of an image.You can see what that convolutional layer is actually doing to the image.This is really cool.I based this code on-- I've just cited it here,because my code has kind of inspired by this code that I useto generate the video and the images you're about to see.Yes, this is it.What we fed in is the image you see at the top left there,which is the breakwall game.You can see we've got the bat at the bottom and the ball,and then the bricks and then what we found was coming outof a 32 filter convolutional layer was this.You can see what it's done.Each of these kind of rectangles is a different filter.I've got 32 different filters.This is 16 by 2, and each of those filters is showing what that filter outputwhen we fed in the image, and this is a trained network.This is what it learned by training to see that image.As I say, it allows the neural network to learnwhat are the most important things it needs to knowin order to reduce its error,because that's what it's basically trying to do.You can see if you look at those images, that in some of the images,it's managed to hide the bricks entirely.These are all of the same screenshot coming in.In fact, it's being fed multiple screenshots,and a block of screens.You can see, in some cases, it's removed the bricks.It manages to invent its own filter that can remove the bricks.In other cases, it's come up with a filter,which only shows certain bricks or certain layers of the bricks.What it's being able to do is that it emphasize different bitsof the screen that the agent might be interested in.What I'm going to show you now is actually an animationof this happening in real-time.I've rescattered it, but this is basically a 4 by 8 gridof filter outputs from the first layer in the DQN agent.I trained the DQN agent to be able to play this game,and this is what it could see after it'd been trained.Let's play the video.Look out for the bat and the ball moving around.You can see that the bat's moving around and the ball's bouncing.The reason there are multiple balls is because it's actually being fedthe last four frames of the game.It knows the last few positions of the ball, but it could also be usingsome convolutional techniques to shift, to spread the ball out a bit as well.That's really cool.That gives you an idea of what the neural network actually seeswhen it's got a convolutional layer.It's learned these 32 different filter kernelsto process the image with.That's it.What we've been talking about is convolution.I gave you a very quick introduction to convolution and the ideathat you can essentially filter an image using a filter kernel that slidesacross the image and that the output can pull outdifferent features of the image or process the image in different ways,blurring, edge detection, there's all kinds of other things.Then we talked about how you can have a convolutional layerin a neural network, which actually allows it to learna bunch of different filters, which it applies to the image,which allow it to extract more useful information from the imagethat is available just in the raw image data.That's the whole point, it's to extract useful informationwhich help it to train better.It's got a feature extraction technique.You've got image feature extraction embedded inside your neural network.This is exactly what DQN is doing.In the end, we just saw a visualization of what a trained neural network is seeingwhen it's playing a game.In this video, I've just given you an introduction to convolutionand convolutional neural network layers."
}