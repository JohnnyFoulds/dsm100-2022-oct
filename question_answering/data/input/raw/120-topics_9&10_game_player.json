{
  "course": "DSM100-2022-OCT",
  "topic": "Topics 9&10: Game Player",
  "title": "Lecture: Value functions and DQN",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96447&forceview=1",
  "transcript": "[music]In this video, we're going to continue with our discussionof how to formally model this decision-making process.In summary, we're going to be thinking about action policies nowand what does an optimal action policy look like?Then we're going to think about the concept of future rewardand how we can express that in terms of a Bellman equation.We're going to see a big gnarly equation at one point in this video.Then we're going to think about what Q-learning doesand what Deep Q learning does.Lots to cover, so let's get started.First of all,remember, we were talking about this sequential decision-making process.Well, the action policy tells us what to do in a given state,and it's normally denoted with the symbol pi.We say, \"Pi acts upon a given state to yield an action.\"In other words, so if the state is S, then we need to take action A.The policy is simply a mapping between states and actions.What does an optimal policy look like?Well, because it's a sequential decision problem, not just a one shotreward might come in the near or far future.That's a really important thing to understand.Normally, when we're doing machine learning,we consider the reward to be instantaneous.If you're training an image classifier network,so you've got a bunch of images and you've got a bunch of tags,like dog, cat, et cetera, you essentially train it to know,\"Okay, if you see this image, say dog.If you see this image, say cat.\"It's a one-shot thing.It doesn't have to then say,\"Well, you're going to see this and then you can see this.\"It's sequential problem.When we consider what the best action to take is,we have to consider, are we unlocking a load of possible future,other actions by doing this action?For example, think about breakout.If I engineer it so I can get the ball on the top of the bricks,it will bounce along and I'll unlock loads of future reward.In order to do that, I need to knock out the bricks on the side firstand I need to hit the ball on a certain part of the bat.I'm not getting much reward until way, way in the future.That's the problem with a sequential decision problem.When we're formalizing it, we have to take accountof the reward we're going to get in the future.The Bellman equation allows us to formalize that.Let me bring this up on my editorbecause that is a pretty gnarly-looking thing.I'm actually going to explain why this doesn't really help us that much.Let's go to it.I think I've got it over here. Yes, there we go.Here is the Bellman equation.There are lots of different versions  of the Bellman equation.This is the one that is presented in the DQN research papers.I've recreated it with my own LaTex code.Let's go through it and say what each of the bits means.Well, essentially, what that equation means is,it's saying that the value of a given action in a given state is based onthe reward we might achieve in the future.That's what it's saying is if we assumingwe're taking actions based on a certain decision policy.If we have a certain action policy,and we take the actions in that action policy going into the future,then this action we're taking now will yield this value.Value is reward going into the future.How is it expressing that?Let's go through each of the bits.This is the first bit.This is what we're looking for.This is the value function, it's saying the value of action A in state S is?Then what does all this stuff mean?Let's break it down.First of all, we've got this rt.That is the immediate reward we're going to achieve by taking this action.Remember, we've got this state, action, next state, and reward.That's what we're talking about there.It's taking account of the immediate reward, first of all.That's the first thing.That's what that is there.The next thing is, let me just get rid of that highlight,the next thing is this, these things.This is, again, processing some reward.We've got an r in there.What does it mean?That's a gamma. That's letter gamma.What does it mean Î³rt+1?What do you think rt+1 means?Bearing in mind, we're looking into the futuret+1, t, time, plus 1 into the future, right?What we're saying is, we're going to say,\"in order to work out the value of this particular action,we're going to look at the reward in the next time step,but we're scaling it by gamma.\"Gamma, this sign here, gamma is less than one.Imagine gamma is 0.5, what we're going to say is,\"Well, that one, t+1 is times naught 0.5.\"That is half of the actual reward.The further into the future it is, the smaller the scaling on the reward.If gamma is 0.5, then gamma squared is 0.25.We're only taking a quarter of that reward.We take half of that reward, quarter of that reward.What we're doing is further into the future, we're saying,\"Well, we're not that confident of what's going to happen in the future.\"We're only going to take a little bit of the reward that comes in the future.That's my interpretation of it.You can see that we're taking account of t+2 there.That's t+1, t+2.We're looking two steps into the future.Now, we've got the dot, dot, dot, so that tells you it's going to continueinto the future to a certain amount.Then, this bit here is saying, essentially, you can translate thatas assuming that we're using the action policy pi to select actions.It's saying, \"Well, here's the reward you're going to getif you operate using the action policy to select your next actionto lead to the next state.This means that, essentially, we're making a decision,which is we choose the most valuable thing and the action policy every time.Pi is the action policy, max is just saying the highest.Basically, we're making the best decision.It's saying, \"If you make the best decision you canaccording to the action policy, going into the future,this is the value that you'll receive.\"The problem is it still doesn't really tell us what the action policy is.[chuckles] How do we get this value function?That is the big problem.We're actually missing a couple of things here.We're missing, so, the state transition matrix,so we don't actually knowwhat the state transition matrix.Think about our breakout game, we don't knowwhat the state transitions are,because we don't have a massive data set which tells uswhat the state transitions are.That's the first thing we're missing.The second thing we're missing is the action policy,which tells us what decision to make.We're missing most of these things.If we don't have those, then the value function doesn't really help us.This is the problem.Luckily, there's a thing called Q-learning.In essence, before we look at the details on that slide,what Q-learning is doing is it's saying,\"Well, I'm not going to have a perfect value function.I don't worry about that, but I'm going to try and get my best approximationof what the value function is.I'm going to somehow, through observation,learn what the value function is.\"The Q-function in Q-learning,the Q-function is an approximation of the value function.What we do is we just take the original equationand we just swap out the V for a Q,and then that isour Q-learning expression.What we're saying is the best Q-functionis the one that maximizes value over time using the action policy.Instead of having to know all this stuff,what we do is we somehow learn the mapping between states and action,the value of a given action in a given state.All we need to know is the state and the actionand then it will tell us what the value is.That's the problem.That all sounds great.Q-learning is a type of reinforcement learningand there are various methods to do it to approximate what that function is.By the way, most of them don't work for real problems,especially they don't work for Atari video games.This is the crux, this is the problem with Q-learning.It sounds like a brilliant way of doing it.Basically, somehow you can figure out what the value function iswithout the need to know all that stuff.You just learn it somehow.They don't work.That's where the DQN system comes in.DQN is a way of approximating the Q-function using a Deep Q Network.We learn the Q-function using a deep network,of course, because that's what we do all the time in machine learning and AI.We throw a deep network at it.Yes, luckily, in Atari video game context,it appears that with the right type of neural network,you can actually approximate that value function.That's why it's called Deep Q Network.That's all I want to say for now on that one.What we've been doing is we've been learning about a few new things.We've been learning about what an action policy is,or an optimal action policy is which is where we maximize rewardgoing into the future and how we can express that using the Bellman equation.We then learned what Q-learning is,which is essentially a way of approximatingwhat the value of a given action isgoing into the future by learning it somehow.We found out, finally that Deep Q Network is a type of neural networkwhich is used to learn the value function,in our case, in retro video games.We've been learning a few new things here,action policy, Q-learning, and Bellman equations.Now, we're ready to continue learning about the Deep Q Network agent."
}