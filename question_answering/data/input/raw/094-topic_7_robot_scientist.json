{
  "course": "DSM100-2022-OCT",
  "topic": "Topic 7: Robot Scientist",
  "title": "Lecture 4: Legal and ethical issues in robotics and lab automation",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96371&forceview=1",
  "transcript": "[music]In this mini lecture, we will consider important ethical and legal issues.We will not cover the whole area of AI, instead, we will be focusingon ethical and legal issues that are most relevantto the considered case study, the Robot Scientist, and a little bitwhy the laboratory automation.We developers of search system have to observe such issues.We have to remember about it and we are, ultimately, responsiblefor developing ethical AI systems, and we're also responsiblefor how such systems are used.Everyone working in this areahave to be aware of all surrounding issuesand problems, and try to mitigate whenever possible.There is a whole area of research, now is called Roboethics.It covers ethical issues related to robot design,operation, and use.The focus on answering such interesting and important questions like,\"Can we design robots to act ethically?How people should treat robots and how should robots treat people?\"\"Should robots have rights?Who or what is responsible if a robot or other automated system causes harm?Are there any risk to create emotional bonds with robots?Which type of ethical codes is correct for robots and automated systems,and so on?\"There are many questions and issues and we will not cover them.We will not answer all questions since that will we'll consideronly few issues that are most relevant to the Robot Scientist Project,our case study.The Robot Scientist Project has been frequently criticizedfor replacing human scientists,and more generally, yes,it's a serious problemsthat more and more robots performing jobsthat are typically performed by humans.The whole job market is distorted and the tendencies firmly set,so it's unlikely to change.On the slide, I provided several links to YouTube videos,where you can watch how robots are performing jobsthat we typically associate with humans.Amazon robots move products for packaging and sending to customers.Robots are making drinks in a bar, and soon, you will be able to buy a robotas a personal chef for your kitchen.In laboratory automation specifically,an issue is that lab automated systemare replacing lab technicians and threatening jobs of scientists.I'm working in this area many years, and I can confidently tell thatin this particular area, it's not a big issue.I don't claim that it's not issue for the whole AI.I do believe that it has negative impact.It also has positive impact, new jobs are being created.Particularly laboratory automation, the true goalis to automate repetitive tasks or tasks that must be performed in environmentsthat are dangerous to humans.I would like to give only one example.During the pandemic, many scientific labs were shut down because it was dangerousfor humans to travel and to be there.In many labs, it's not possible to observe social distancing.The best solution is to build a remote autonomous laboratoriesthat can be directed by humans remotely by phone or emails.Automated robotic parts of this lab can carry out experiments autonomouslywith minimum or maybe even no directions from humans.Moreover, it is importantto understand limitations and advantagesof both AI systems and also humans.AI systems are better in processing large volumes of data,reasoning, and in parallel tasking.AI system can walk nonstop, 24 hours, seven days a week.They don't need holidays, don't have headache.Nevertheless, humans are better in higher-level planing,making decisions in uncertain, nondeterministic,or partially observable environments.I firmly believe that the target should be human-robot collaborationand co-creation.It is known that the AI systemcan beat the best human chess player.Interestingly, a team of AI systemand humans can play better chessthan a team of AI systems or a team of humans.I believe that the same would apply to scientific researchand many other areas.Another issue I would like to highlightis when AI systems exhibit some biasand also makes mistakes.It's important to remember that AI systems can be biased.They have less bias, but it can be inbuilt by design by humans.For example, it is well known that medical data sets are biasedsimply because not enough data collected from female patientsor from some minority groups.As a result, AI systems that uses this biased data setscan make biased decisions based on these datasets.The best mitigation is, first, to be aware of that.Try to compensate for bias and data sets, for example, bias stratification,or at least, fairly accurately recording everything what is done.Decisions can be checked, what AI decided and how,and then decision can be made if to trust this decision or not.If it's biased decision or not.AI system can make mistakes.There are technical systems, it can go wrong.For example, the now project and the Robot Scientist Project,all experiments are repeated many timesto ensure that there is consistencyin the results because if one something goes wrong,then other experiments will show different results.Only when results are consistent, only then they have been usedfor further steps.Legal issues.Existing laws have not been developed with AI in mind.Again, I will give only one examples that highlights this problem.The Copyright Law didn't permit us to include Robot Scientists, Adam,as a cause to the paper reporting the discovery made by Adam.We published this in one of the most prestigious journalsin science and the editor explainedthat we cannot include Adam as a causebecause Adam cannot sign the copyright agreement.It doesn't sound right, but this is how it is.A bigger question is, \"Should robots have rights?\"There is no definite answer, not yet to theseand to many other questions, but this area attracteda lot of attention.There is a lot of research in this area and, hopefully, substantial progresswill be made fairly soon.I would like to talk about one more issue, it's issue of safety.AI systems must be designed in a way that they do not poseany threat to humans.Again, I will focus on our case study, the Robot Scientist Project.The Robot Scientist Adam was placed inside in enclosure.It was even not possible to approach it.The Robot Scientist Eve didn't have an enclosure,but it had sensors.If someone would approach, any dangerous piece of equipment,the system would automatically shut down.In our case, it's very unlikely that this system can causereal physical harm.Of course, there is a possibility that they may make predictionsor like for application of cancers, they can make wrong recommendationsfor drug to use, but that is, again, accountabilityand checking how a decision was made.Otherwise, I believe that the areaof lab automationis less proneto these ethical and legal problems.However, I do recognize that why that in AI researchthere are many ethical and legal issues that need our attention.There is a good steady progress is madein answering these important questions."
}