{
  "course": "DSM100-2022-OCT",
  "topic": "Topics 5&6: Creatures",
  "title": "Lecture: State of the art for GAs",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96347&forceview=1",
  "transcript": "[music]In this video,I'm going to look at a couple ofstate-of-the-art trendsand applications of genetic algorithmsin other areas of morphologythan basic robot design.What am I talking about?First of all, I'm going to talk about evolving neural networks.Instead of evolving robot designs,we're going to be evolving neural networks.Then we'll talk very briefly about evolving programsand some of the trends and benchmarksthat are happening.This is a research paperwhere they reporton the being able to evolveneural networkswhich are competitivewith human design deep networks.I'll give you a link to this paperso you don't need to take it down now,but you'll have it in the reading after the video.Now, here's the motivationfor doing this stuff.Why evolve a neural networkwhen we've got really good ones at work already?The fact is that we are reaching the limits of human designof networks.It's very difficult to designand optimize the designof a deep neural network.It is a highly complicated structure,which you don't know how well it's going to perform until you train it.Then you have to figure outif it doesn't work, why it doesn't work?There's this whole hyperparameterelement to itwhere you're trying out different settings and everything else.There's a lot ofwhat I would call black magic involvedin neural network designbecause you're basically trying lots of things out.That's why you need labs full of people doing it to get it to work.Now,I think that one other really interesting thingis about neural network evolutionis that it can potentiallynot just come up with designswhich are as good as the human ones,but they can be more efficientbecause you can optimize themto get rid of the stuff it doesn't need,which will be very difficult for a human to do tosay, \"Oh, we're not really using that layer much,we can get rid of that.\"Potentially, it could be much more computationally efficientand that's really important.Deep neural networks use a lot of electricity.If you can do the same jobbut with a smaller network,that's great.On the opposite side of the coin to that,they're very computationally expensive.In order to evolve a neural networkwhat you have to dois generate lots of different neural networksand then train each onewith the same dataset, presumably,and then evaluate how well it performs,and potentially even try it with different parameters and so on.You can imagine thatrather than just designing one networkand training that,it can be very expensiveto have to generatethousands of networksand evaluate those as we have done withthe Karl Sims' creature.It could be very computationally expensiveto get the thing in the first place.Once you have it,once you've got your evolved neural network,it can potentially be much more efficient.It's the evolved neural networkthat's going to be running all the time,doing all of the inference.In a way, it's better to have that being really efficientbut having spent a bit of electricity at the beginningbut to be able to then havean artifact which is more efficient.How does it work?There's two schemeswhich I'm not going to go into great detail,but let me just summarize.You've got NEAT and DeepNeat.NEAT is essentially similarto the system we've been looking at.You have a chromosome if you like.You've got what they call chromosomes,but they really mean genes.They separate into chromosomal unit,so that's your genome.It's got a bunch of chromosomesas they call it,but I would call them geneswith my biological sciences backgroundbecause actually, a chromosome is a collection of genes,so it doesn't quite work as a word.Anyway, they call it a chromosome,so let's go for that.Each chromosome encodesfor a node in the network,and how it connectsto other nodes presumably.Then depending on what the chromosomes areand what their characteristics are, you get different types of connections.You can have a huge variety of different networks,very similar to what we were doing with the creatures.That's NEAT. Now, DeepNEAT,and it's got certain features of the genetic algorithmwhich make it interesting as wellrelating to how it managesthe population into species and things like that.Some interesting state-of-the-artGA stuff going on as well.Now, DeepNEAT is different in thatinstead of evolving these smaller networks,it's actually looking to evolvea great big networkmade out of lots of layerswhich is what we do with deep networks.We have lots of layerswith lots of nodes in them.Then they then get wired upin different ways and in different patterns.As you might be guessing,what I'm going to do here is that instead of encodingindividual neurons,DeepNEAT actually \"has a chromosone\"for each layer.You'd have a chromosoneencoding each layer and its properties.It operates at a higher levelof organization than NEAT does.That's NEAT and DeepNeat.That's how they do it.No huge surprises there.Now, the question is,are they competitive with human designs,state-of-the-art networks?According to one of the authors, in 2019,the results show that the approach discovers designsthat are comparable to the state of the artand does it automaticallywithout much development effort.It makes very good designsand it's not that hard to use itbecause it's designing them for you.That's pretty impressive.That's in 2019 as well,so this is in the midst of deep learningconquering all genetic algorithms,still a very valid toolin the arsenal of the artificial intelligence scientist.Here's another onethat I actually wanted to briefly mention.There's a real history of peopletrying to evolve softwareusing evolutionary techniques,and here is a 2018 paper showing thatpeople are still doing it.In this case,this is a big surveythat will show you loads of examples of it.If you're interested in the idea ofautomatically designing software, which againis a topic that's coming up in deep learning now as wellwith the recent release of various toolsfrom Microsoft based onthe OpenAI networks like GPT-3.So yes, Autocode completion, that kind of thing.People have been doing Autocode stufffor a long time with genetic algorithms,and here is a survey of that.The final thing I wanted to mention is that another trendin genetic algorithms and evolutionary computingor bioinspired computingis the idea of benchmarks.As any computer science field,especially one that's related to optimizationand efficiently solving problems,that kind of thing.As it evolves and matures,often it'll end up having a benchmarkso that when people come up with new algorithms,they're running them against the same dataset,or there's a fixed challengewhich people attack,and then that gives thema valid way of comparing algorithms.That's absolutely happeningin bioinspired computing as well.Here's a bit of a review ofsome of the stuff covered in this article.In this video, I've just been giving youa bit of a run-through of some of the other areasthat evolutionary bioinspired computingis being applied to.We looked in little bit of detailhow we can evolve neural networksusing genetic algorithm techniques.Also, I mentioned brieflythat people are also usingthese techniques to evolve software programs.Also, I mentioned some of the trendssuch as benchmarkingthat are going on in the field.In this video, we just had a brief reviewof some other application areasthat are current and trendingin the area of evolutionaryor bioinspired computing."
}