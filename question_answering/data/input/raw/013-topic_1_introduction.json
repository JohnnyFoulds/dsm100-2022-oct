{
  "course": "DSM100-2022-OCT",
  "topic": "Topic 1: Introduction",
  "title": "Lecture 10: Explainable AI",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96107&forceview=1",
  "transcript": "[music]-In this mini lecture, we will focus on the issue of interpretabilityand expandability of AI systems.AI programs are used to make important decisionsaffecting our everyday life.If to give you a bank loan or not, if to subscribe you to a socialcare program or not, if you're likely to commit a crime or not.Virginia Eubanks in her book Automating Inequality describes herpersonal painful experience when her house insurance has been suspended.It took her one year significant financial resources tocorrect what she believed was an automated decision.\"We spent our savings.Then we stopped paying our mortgage.Finally, we took out a new credit card and racked upan additional 5,000 in debt.Not everyone fares so well when targeted by digitaldecision-making systems.Some families do not have the material resources andcommunity support we enjoyed.\"There are many ethical issues associated with automated decisions.To complicate the metaphors, it can be hard to understandunderlying models and explain how decisions are made.The main reason is that machine learning algorithms were not designedto be interpretable, unexplainable.They were designed and optimized for producing as accurateas possible predictions, generalizations, and classifications.The workings of some algorithms are easier to understand than the others.Decision trees are considered to be easily interpretable.However, if a tree is very complex and notes are notmeaningful, then it may be hard to understand how decisions are made.Random forests is less interpretable and neural networks may beextremely hard to interpret.The whole concept of interpretability and explainabilityis still not well defined.Miller defines explainability as the level to which a system canprovide the cause of its decision.Tomsett et al defines interpretability as the degreeto which a human can understand the cause of a decision.The questions are, for whom explanation should be provided.There can be different groups of explainees.For example, data scientists need to understand how recommendations abouttreatments of particular patients are made in order to evaluate therobustness of underlying algorithms.Doctors and patients also need to understand how suchrecommendations are made and if there are alternatives.General public may wish to see if there are any ethicalimplications in the way decisions are made, and so on.In what form explanations should be produced?In the form of natural language?Different groups of explainees would require different explanations with adifferent focus and level of detail, perhaps using different vocabularies.Moreover, software and robotic agents may requireexplanations for their workings.How much of explanations is enough and for what purpose?These all are open questions, and there is not enoughdone to answer them.Perhaps the most debated area is interpretability and explainabilityof deep learning models.For example, Su et al showed that it is often enough to change onlyone pixel to fool deep neural network classification models.The researches showed that change in one pixel in about 74% of theanalyzed test images made the neural nets wrongly label what they say.This doesn't boost public trust and recommendationsbased on machine learning.It is not good to label a stillborn as a dog.It can be life-threatening to mislabel an image as nocancer instead of cancer.There are legal rights to explanation.In US, under the Equal Credit Opportunity Act, creditorsare required to notify applicants of action takenand provide specific reasons.In European Union, according to GDPR or General Data ProtectionRegulation, the data subject should have the right not to be subjectto a decision, which may include a measure, evaluating personalaspects related to him or her which is based solely on automatedprocessing and which produces legal effects concerning him or her orsimilarly significantly affects him or her, such as automaticrefusal of an online credit application or e-recruiting practiceswithout any human intervention.Unfortunately, even if the legislation requires explanationsof automated decisions, it may be not technologically possible.From the report on XAI by the Royal Society, \"Some of today's AI toolsare able to produce highly-accurate results, but are also highly complex.These so-called 'black box' models can be too complicated for evenexpert users to fully understand.\"I would like to give an example of where an automated decisioncould have led to World War III.On September 26 in 1983, a missile officer, Stanislav Petrov sawon his computer screen an alert about an upcoming missile attack.According to the protocol, Petrov should have initiated anuclear counterattack, but he suspected the alert was a bug.He was right and World War III was narrowly averted.This example shows the importance of having a human in theloop for making decisions.Nowadays, XAI [eXplainable AI] is a hot area of research.In 2019, Hall et al proposed a conceptual model of XAI anda systematic method for the development of XAI systems.The model consists of the following elements; an AI system,groups of explainees,  and explanator module.The outcome of an AI system is a model or some decisionoutput, which is evaluated in terms of AI system performance.An explanator forms part of the AI system which generatesexplanation artifacts, which in turn are evaluated in terms ofexplainability effectiveness.An explainee agent, human or machine, will consume explanation artifactsand their ability to understand.This is evaluated in terms of explanation interpretability.The method for the development of XAI system includes the following steps.Determine the relevant explainee roles.Determine explanation characteristics.Capture explanation requirements.Assess the ability of appropriate explainable methodsto meet these requirements.Map existing explainable techniques to XAI system requirements.Explainability can be assessed by the following measures.Transparency, the level to which a system provides information about itsinternal workings or structure, and the data it has been trained with.Complexity, the size of an explanation.For example, rule length or decision tree depth and relationshipsbetween features presented in an explanation, for example,correlation or conjunction.Explanatory power, the scope of questions thatthe explanation can answer.Generalizability, the range of model to which explanationmethod can be applied.Interpretability can be assessed by the following measures.Timeliness, the time required to usefully produce andinterpret an explanation.The presentation of explanation, the form in which theexplanation is presented.Explainee's satisfaction in explanations.Interactivity, the degree to which an explanation may be interacted with.Personalization, can explanations be given to category of individualsor a specific individual?In this mini lecture, we discussed the issue of interpretabilityand explainability of AI systems.In some situations, explanations have low priorities thanin other situations.For example, if we urgently need a new drug, we would accept theautomatic prediction of what chemicals are likely to be the drugas long as it works, even if there is no explanation of why it works.However, in most of situation, explanations are essentialand interpretability is a vital feature of AI or future"
}