{
  "course": "DSM100-2022-OCT",
  "topic": "Topic 1: Introduction",
  "title": "Lecture 9: Biases in data",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96105&forceview=1",
  "transcript": "[music]Data scientists are working with dataThe results of their work  largely depends on the quality of dataThe well-known principle,  garbage in garbage out, tells that the quality of output  depends on the quality of inputThere are many issues  associated with data qualityOne of them is bias in datasets but it's not only a quality problem,  it is also an ethical issueYou as a data scientist need to be aware of it and remedy  the situation whenever it's possibleMedical data may be biasedFemales are often greatly  underrepresented in many clinical studiesLarrazabal et al,note in their article that little attention  is paid to the way databases are collected and how this might influence  the performance of AI systemsThey also carried out a large-scale study using three  deep neural network architectures and two well-known  publicly available datasetsThe results show a consistent decrease in  performance for underrepresented gendersWhen female patients are excluded, or significantly underrepresented  in the training data used to develop a machine learning model, then the model performs worse  in diagnosing themFor example, it has been found in some datasets only 6%  of the total sample were womenFeldman et al,analyzed women's participation  in clinical research over 25 years, from 1993 to 2018They show substantial female  underrepresentation in 7 out of 11 disease categories such as HIV, kidney diseases,  and cardiovascular diseasesHealth care data may be biased  by socio-economic status of patientsAccording to Gianfrancesco et al,biases and deficiencies in the data  used for the machine learning algorithm may contribute to socioeconomic  disparities in health careThey give reasons of biases  in electronic health records which include patients  with low health literacy may not be able to access  online patient portals and document patients-reported outcomesCertain subgroups of patients  may not exist in sufficient numbers for a predictive analytic algorithmPatients of low socioeconomic status may be more likely  to be seen in teaching clinics where data input or clinical reasoning  may be less accurate or systematically different than that from patients  who have higher socioeconomic statusImplicit bias by healthcare practitioners  leads to disparities in careSocial care data may be biasedVirginia Eubanks studied the subject  for over a decadeHer conclusion is, \"Most people are targeted  for digital scrutiny as members of social groups  not as individualsPeople of color, migrants, and popular  religious groups, sexual minorities, the poor, and other oppressed  and exploited populations bear a much higher burden of monitoring  and tracking than advantaged groupsMarginalized groups  face higher level of data collection when they access public benefits,  walks through highly police neighborhoods, and there's a health care system  across national bordersThat data  acts to reinforce their marginality when it is used to target them  for suspicion and extra scrutinyThese groups are seen  as undeserving and singled out for punitive public policy  and more intensive scrutiny and the cycle begins again\"Data science techniques, if not used  with due care, may introduce biasFor example, the US health care system uses commercial  algorithms to guide health decisionsObermeyer et alfound evidence of racial bias  in widely used algorithm which means that Black patients  assigned the same level of risk by the algorithm  are sicker than white patientsIt's also estimated that this ratio bias reduces the number of Black patients  identified for extra care by more than halfWhat you as a data scientist can do to reduce or eliminate biases  in datasets and algorithmsFirstly, you need to understand your dataFor example, a postcode can be  a proxy for socio-economical statusSecondly, you can stratify your dataFor example, you can include  more data on females in your training sets to better represent the real ratio between males and females  or you can impute missing valuesYou also can ask someone external  to evaluate your datasets and algorithms for biases including  for your unconscious bias"
}