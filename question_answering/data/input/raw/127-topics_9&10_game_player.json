{
  "course": "DSM100-2022-OCT",
  "topic": "Topics 9&10: Game Player",
  "title": "Lecture: Introduction to keras and neural networks",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96468&forceview=1",
  "transcript": "-In this video, I'm going to give you a real crash course in neural networks,just to give you an insight to some of the key termsthat we're going to be looking at as we discussed the DQN neural network.First of all, I'm going to answer the question, what is Keras?Then we're going to be talking aboutall the different elements of neural networks.I'm going to try and give you a kind of quick intuitive graspof what's going on with neural networks because I'm not going to assumethat you have done an extensive neural networks course in this course.What's my motivation?As I say, we need some basic neural network termsto be able to understand the DQN network.I'm telling you all of this because in order to get a good graspof how the Deep Q learning works,we need to understand a bit about how neural networks workbecause when we look at the code, we'll see what's going on.What is in your network?It's a set of processing units connected together into a neural network.Let me show you a picture.Here's a bunch of processing units,and they're connected together into a neural network.When we connect them all together like this,there's many different ways you could connect them togetherand different connections that could even be connected to thereif you want but there's all kinds of different--That is a neural network, a bunch of processing units.What do you do with it?Well, you feed in some numbers at one end,and other numbers come out of the other end.Okay, so back to the drawing.We put some numbers in at the top, so some numbers come in here.Well, maybe 0.1, 0.2, 0.1 or something,so we're feeding numbers in at the top and then at the other end,those numbers they flow through the network.That one's going to go down to there, down to there,and then that one's going to go all over the place.Basically, the numbers go through the processing network,and eventually, other numbers come out the other end.It might be a 0.0001, you might get there.Then maybe you get another number there.Numbers have come in the topand other numbers came out at the bottom, that's the basic idea.Then, the other thing to know is that training in neural networkinvolves incrementally adjusting the settings of the processing unitsuntil they cause the correct outputs to come out in response to a set of inputs.The whole point of training is that when we feed in those numbers,we have a bunch of examples of inputs and outputs typically,and we say, when you see this input,we want you to produce a specific output.We feed the numbers in, we have a look at what comes out,and we've got our idea. Well, actually, we didn't want that.This is what we wanted. We wanted some different numbers.We wanted to say 0.1 to come out and 0.2 there and maybe 0 there.You know what came out, you know what you wanted to come out,and then training involves going, \"Okay, well, it was wrong,\"so training involves feeding that error.We look at the loss between what we got and what we wanted and we feed that backthrough the network, feed it back through and the error,and we adjust all the settings on the network.I'm going to talk a little bit about what those settings are in a minutebut suffice to say that training involves looking at that networkand reconfiguring it slightly based on what the errorwas so that it gets slightly closer to the correct answer next time.That is training and basics of neural networks.What is Keras? Well, Keras is a library,which allows us in a high-level way, it's a high-level API librarythat lets us program TensorFlow neural networks basically.It gives a nice simple Python interface on the TensorFlow librarywhere we can build networks.The DQN example code uses Keras.I forgot to say on the previous slide,that DQN uses a neural network to learn the Q functionwhere the input is game state and output is value of all actions.If this is DQN network, the thing coming in at the top there,the thing coming in here, remember, is the state of the game,which is basically all the pixel values for the last four frames,I think it is. We're basically passing in loads of-the number of inputs is like one for each of the pixels on the screen,so it's got a huge input layer coming in there.Then the output, what it's giving us,is the value of each of the possible actions.Those numbers coming out the bottom in the DQN networkare like how much is going left?If I'm playing break out, I've got to move my bat left or right,it's telling me how much is it worth to go left now,how much is it worth to go right, and how much is it worth to do nothing?It's telling me what is the best action to takegiven what we've told it about the current screen.It looks at the screen, it tells what--That's what we're doing and the idea is to learn what the best actionis for every possible screen ultimately in DQN.Keras is the neural network library. What are layers?Well, neural networks are made out of layers,and deep neural networks typically have quite a few layersand that's what we mean by deep.There's different types of layers with different nodes in themwhich process the signals in different ways.Back to the drawing. That's the input layer, for example,and that's what we might call a hidden layer because it's insideand that's the output layer but we have loads of layers in thereand the more layers you have, the more processing is done to the numbersas they flow through the network.As those numbers are going through the network each layer,each node is doing some sort of processing on the numbers.A deep network has loads of layers,and therefore it's quite difficult to trainbecause you have to give it loads of examples for it to learnhow to configure the network correctly.There's lots of different types of layers,and this is the Keras documentation for the different types of layers.What I say, when I first saw this webpage,it really blew my mind because I felt like I've just gone into a sweet shopfor neural networks, the equivalent of the neural network sweet shopbecause look at all these things it's got.You got all these different types of core layers.You've got input layers, dense layers, activation layers,embedding layers, masking layers, lambda layers,that's just the core layers.Then we've got all these different types of convolutional layers.We're going to talk a lot more about convolution in the next video,so don't worry about what they are for now but there's loads of different ones.We've got pooling layers, we've got recurrent layers, long,short term memory networks, GRUs.We've even got attention layers, which are the trendy new thingfor doing transformers and that kind of thing.It's all there.When I was doing neural networks first in the late '90s when I was at Sussexdoing my masters, we didn't have all this stuff.We had a really basic neural network library in MATLAB,or whatever, and that was a lot.A lot of people were hand-coding these things in Cto get them to work fast enough and so on.This is exciting times to be working on neural networksthat's what I'll say about that.Where am I? That's the layers.There's lots of different types of layers.Keras makes it very easy to build, lots of layers together,and those are the core layers. I just mentioned those.Those are some of the quotes from the documentation.A dense layer is just your regular densely-connected layer.For example, a dense layer is a type of layerwhere everything is connected into it.You can see this, I guess, this is actually a densely-connected layerbecause everything from the previous layer--Actually, it's missing one there. It actually go there.Each of the sort of inputs to that layer, if you like,is connected to each of the outputs.That's what we mean by densely-connected.There's different ways.Depending on the layer, things are connected differently,and also, there's different mathematical functions embedded in that layerto process the numbers coming through.As I say, there' more of these convolutional layers,and DQN uses convolutional layers a lot so we're going to talkin a later video much more about what convolution does.Then you've got this idea of an activation function.What's an activation function?Well, back over here, what happens is the neural network,each of these nodes is going to do some sort of processing.It's going to say-- The numbers are going to come in to that node.In this case, we're going to have three numbers coming in,and the activation function it's really based on a biological metaphorso it's bio-inspired computing, remember.It's bio-inspired computing.It's the idea that in your brain or the way natural neurons work,you've got lots of neurons wad into another neuron and signals come in,and if the sum of all the signals coming in is high enough,then the neuron will fire. We have a similar concept in here.Let's say we were to draw a simple graph of it.We would have the input coming in there, and that's the output.Typical biological one would be as the input goes up, and up and up,eventually gets high enough and it fires and an output comes out.That's the, if you like, the most basic activation function.Then you got kind of linear ones,as the input increases, the output increases.Then you got kind of sinusoidal ones,all kinds of different shapes, [chuckles] whatever you want.You can have whatever activation function you like.There's loads of different ones.There's lots of built-in ones,or you can probably write your own ones as well.Okay, that's activation functions,and the purpose of an activation function is to controlwhen the neuron outputs and how much outputso it can combine lots of inputs and respond to those.The next concept is weights.The weights interact with the activation function.Each of these lines, basically, is a connection.Each of these margin has got a number next to it 0.1, 0.2, 0.3.You scale the numbers as they flow through.They get scaled. It's another powerful sum.You add up all of the inputs, multiply them by each by their weights,and then that dictates how much is flowing through the network.Then those weights, then feed into the activation functionand depending on the type of activation function you have,you'll get a different output from that node,depending on the weights and the activation function,and the activation coming in as a result of the inputs.All works together to basically set up this whole designfor how signal flows through the network.Right. Then you have this loss function.The loss function is when you feed in,as I referred to it a bit earlier,but you feed in a given set of inputs and you know what you want,you know what the output should be--Okay, well, I want that output,but maybe you only got this output, and that's the output you got.The loss function is, I know what I wanted, but that's what I got.The loss function is the difference.It's like the difference.There's lots of different ways of calculatingthe difference between the two.Just different distance functions, if you look at that,it's like a 3D vector, isn't it?There's lots of different ways to calculate the distancesbetween two places in 3D spaces it were,and that's what the loss function is doing.The idea is that you feed in the inputs,and you calculate how wrong it is.The loss function is how wrong is the neural network now.It gives you a value.Once you've got your loss function,I'll come back to that, you then do training.The idea of training is that you feed a bunch of examples,you calculate the loss, which is how wrong it is,and you feed the loss back into the network weights.That's the key thing.Once you know how wrong the neural network is, using basic calculus,you can feed that loss back through the network, back up into that.You got your loss, and you feed that,that dribbles back up to the network,and you can adjust the weights. That's what we mean by training.We basically change those numbers.We change the weights slightly so that it gives a slightly better result.What we can do is we keep feeding it examples, we check the loss,and then we feed the loss back through the weights,and the weights improve over timetill eventually the network actually outputs the thing that we want,or at least as close as possible to the thing that we wantover a wide range of inputs.It comes out with the correct outputs for each input.That's what training is.It's really just feeding things in, seeing how wrong the network is,and feeding that wrongness back into the network,and adjusting the weights so that it gives a slightly better answer next time.We keep doing that iteratively until it's really good.Okay, now, I'm going to come back to this.Custom loss functions.You don't have to use the built-in ones.In fact, in DQN we use a custom loss function.We've already seen the equation for this,we're going to revisit that and see how that gets implemented in code.Remember the loss function in DQN is fancybecause it basically uses a slightly less trained version of the networkto partially calculate the loss.It's a bit fiddly, but we'll talk a bit more about that later.Okay. In summary, the complete picture is that we build a modelby sticking a bunch of layers together.We set the weights normally to random valuesor there's various ways you can initialize the weights,we feed in a bunch of data, we look at what came out,we calculate the loss, and then we back propagate the loss into the network,and then we go back to three, so rinse and repeat.We just keep feeding in batches of inputs, looking at the output,feeding the loss through, feed another set of inputs inand we keep training again and again until the loss is low enoughover our data set that we're happy that it's being trained.Okay. In summary, we've just been answering the question, what is Keras?Then we've dived in to a bit of a crash course in neural network.We've talked about layers.We've talked about transfer functions.We've talked about weights, loss functions, custom loss functions,and finally what training is.Hopefully, at this point, you should have some idea,the basic idea of what a neural network is.It's this data processing thing where we can learn how to process the databy feeding it examples and feeding the errorback into the network to improve it over time.In this video, I've just given you a crash course into our second major toolthat we're going to be using to build DQN which is Keras,the neural network library."
}