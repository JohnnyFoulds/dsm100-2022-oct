{
  "course": "DSM100-2022-OCT",
  "topic": "Topics 9&10: Game Player",
  "title": "Lecture: The loss function for DQN",
  "url": "https://learn.london.ac.uk/mod/page/view.php?id=96453&forceview=1",
  "transcript": "[music]In this video, we're going to be learning how the DQN network can be trained.In summary, we're going to, first of all,think about where Q fits in the architecture,where's the Q function in the agent architecture.Then we're going to be thinking about how we train neural networks,and then specifically, how we're going to train our new networkin this deep Q learnings scenario, okay.Here's a reminder of the architecture.We've been looking at the replay buffer,but now we're going to be looking at the Q functionand how the Q function could be trained over time.The first thing to know is that the inputto that training is the replay buffer, that's our data set.The second thing to know is, looking at that Q network over there,you can see that what comes in is the statesand what comes out is the action.That's when we're using it, essentially,it tells us the value of all the actionsthat we could possibly take in a given state.The state comes in,which is basically the screenshot of what's on the screen,and then the action is that the value of all the actions, sort of say,\"The left has a value of one, right has a value of 0.9,and doing nothing has a value of two,\" whatever.It basically tells us what the value is, the value function.Remember that we're approximating the value function,which tells us the value of a given action in a given state.Okay.Stepping back a bit, as I said, we train it on the state transition data,or in other words, the replay buffer.Problem is, we don't really have a ground truth,so we have to come up with a clever way of doing it.Normally when we train a neural network, we have a ground truth,which is we have this labeled data set with outputs and inputs,so it says something like... Let me jump into my editors to show you this.Normally we have, say, a set of inputs and we say, \"Okay,when we give you this input, we want you to do that output.\"Then, what we do in training is we say, \"Okay, we'll give it the input.\"We know what the correct outputis and we look at which output it actually generates,and then we look at the difference between the two and that's our loss.Say, we're training it on these threeand then we knew the three correct things.We compute the loss over the training set and the trick is,the way that the trick with neural networksis that you can then take that loss, the mistakes,the errors that it's made and you can feedthat back into the weights on the network.You can adjust the neural networkso that it comes up with a better solution next time.That's exactly what training is.That's what backpropagation does basically.The error function is dictatingwhat is the loss or the error between the two,what it should do and what it actually did,and then the backpropagation allows us to then trainthat back into the network so we can figureout how to adjust the network suchthat that gives us a better result next time.That's the basics of how neural networks work.What's our input and output on this one?Well, we don't really have a correct output, [chuckles]so we we don't really know what the correct output should be,because the correct output is the correct valuefor each of the moves and each of the states.We don't really have that because the only thing we have in our replaybuffer is the immediate reward that a given action will give us.We don't have the reward going into the future to get that.We would need to somehow analyze it and build a state transition matrix,but it's just too big to do that and analyze it properly.This is the problem.We don't know what the correct answer is.How do we do it?Well, the trick is this thing.Okay, so the loss function,it tells us what is the difference between what we've producedand what we were expecting to produce.If you see an equation like that, there may be some magical peoplein the world who see that equation and go, \"Yes, I know exactly what that means.\"There are not many of them. [Chuckles]What most people have to do,is they have to look at the equation and breakit down and work out what each little bit means.Even if it's computer size, often you would then want to lookat the code implementation of that equation to verify.Especially with research papers,you want to verify that the equation in the paperis actually the thing that's done in the code, and often it isn't actually.There's a little things they missed out or details,so the equation is not the be all and end all.Often it's a work in progress.You look at that, you have to read the code and figure out the details.Anyway, we're going to try and understand this equationbecause this tells us how we know what our thing has done wrong.Remember, with this image here, it's obvious what we've done wrong.That's what we were supposed to get and that's what we got,but we don't have that, okay?The reason we've got this fancy loss functionis because we have a proxy for what the correct answer is.This is the trick, the proxy is the previous version of the networkbefore we did this round of training.What we do is we keep two versions of the network.Now, one, is used to estimate the value of somethingand then when we break this equation down, you see how that works.The other one is the one that we're training over time.There's a slower one that we updateoccasionally and then the training one, we keep updating all the time.That's the idea.You're going to see that when we break the equation downinto its different component parts.I've actually got it over here.Let me bring that up for you.Yes, here we go.I'm going to get out of the way.What does it all mean?Well, the first thing to identify is that these thetas here,I got a few different thetas going on, those thetas,they're the weights in the network.In case you're not super familiar with neuro networks,the idea is that you've got a bunch of nodesconnected together and signals go through them.The weights are the way that the signalis transferred between the units and the network.The training neural network weights is what is all about.You want to get the correct weights in therebecause then when you put an input in, you'll get the correct output.It's all about the weights, that's what we want to do.These theta expressions, the theta expressions here, here, and here,that's referring to the network weights.What we're saying is, and θi is the network now.Let's write that there.Θi, current network, right.Current network.Saying, \"The loss of the current network is this.\"[chuckles]That's the first thing to know.It's basically telling youwhat is wrong with the current network.What we do, let's break down these other terms.What it does is this bit.Let's get rid of this bit next.This bit is simply saying, it looks really complicated,it's simply saying that we take a random sample,so that's what the U means.Uniform random sample from the replay buffer.D is the replay buffer.It's basically saying, \"Uniformly, randomly samplefrom the replay buffer to yield a set,\"our training set, \"which consists of,\" we zoom into that.You can see it consists of states, actions, rewards, and next states,which is exactly what's in the replay buffer. Right?That's just referring to the input to the loss function.It's saying, \"The loss function is calculated and loss over a random\"sample from the current training set,\" if you like.Basically saying, \"Here's a massive replay buffer.\"Sample at random from that,\" and thenwe're going to calculate the loss.How good is the network at guessing the rewards on that?Let's move on.The next bit is, here we go.Now, this is the bit where we're saying,\"This is our estimate of what the network should output.\"Similarly to my previous example, where is it?Down here, where I was saying, here,we definitely know for sure what the correct outputis and we just compare that to the actual output.Well, here we have to estimate the outputbecause we don't have the correct output.The way we estimate it is using this magic thing here.What is that magic thing?That is θ-i.That's the weights of the network.That's an old version of the network.Okay. Old network.It's essentially saying, we take an old version of the networkand we basically ask what the values would be.We ask it to come to run its Q functionon it to calculate the value of this, plus we use the actual known reward.We've got the predicted future value of this particularaction versus added to the actual reward that we know.We know the R because we've got it there.The R is part of the dataset.We've taken our known reward that we got in this exactstate and we're adding that to the gamma modified,so this weighted max of what we think the value function should be.We got our best guess at what the value function is,plus what we know the reward was in this frame.That's basically saying, \"This is the correct answeror the best correct answer that we have at the moment.\"We compare that to the answer that the current network yields,which is our current Q function, which is using the original,the current trainee network,and then we do mean squared error,Euclidean distance if you like.What have we said there?We've gone through and we've said that what we do is because we don't knowwhat the correct answer is for the network,we take an older version of the network, plus the reward signalthat we've definitely that's correct that we knowbecause that's in our Action Replay buffer and we combine those two.That gives us an estimate of what the value function should be,an estimate of the best Q function we can come up with.Then, we compare that to what the current network is actually generating.Bear in mind, your current networkdoesn't use the actual reward that we've got.That's how we do it.Over time, this will improvebecause periodically, we take our training,our current network, and we copy it over to this one.This one gets updated occasionally.It takes on the weights of the best network that we got.We retain an old version and we keep using that, eventually,we update the old version to a new version, and we start using that.We've got this cycle going on.Over time, the value function just gets better and better,it gets better and better, because it's comparing itselfto the previous version, plus a known signal,and then it's improving itself based on that.It's a very elegant and clever solution to the problemof not knowing what the correct output should be.In this video, we've just been going throughthe loss function for the Q network in the DQN.It's a pretty gnarly equation,but I hope that by my enthusiastic explanation,have tried to pull it apart and explain what it all means.How we can go about training a neural networkwhen we don't really know what the correct answer should be,by gathering examples of data and using our best guessto compare what we're doing as we progress through the training.In this video, we've just been learning about the loss function in the DQN system."
}